"""
üîß ADAPTIVE ML TRADING SYSTEM - ENHANCED & FIXED VERSION
Key Improvements Applied:
1. ‚úÖ Fixed data leakage completely
2. ‚úÖ Enhanced feature engineering with validation
3. ‚úÖ Improved model training pipeline integration
4. ‚úÖ Better error handling and logging
5. ‚úÖ Optimized for small accounts
6. ‚úÖ Enhanced configuration management
7. ‚úÖ Production-ready model loading/saving
8. ‚úÖ Comprehensive validation checks
"""

import pandas as pd
import numpy as np
import sqlite3
import pickle
import json
import warnings
import time
import threading
import re
import logging
import joblib
from pathlib import Path
from datetime import datetime, timedelta
from collections import deque
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
try:
    import MetaTrader5 as mt5
except ImportError:
    mt5 = None
    print("‚ö†Ô∏è MetaTrader5 package not available - running in analysis mode")
    print("‚ö†Ô∏è MetaTrader5 package not found. Live trading will be disabled.")
import os
import glob
from pathlib import Path

warnings.filterwarnings('ignore')

# === TRADE EXIT DB UPDATE FUNCTION ===
def update_trade_on_close(trade_id, exit_price, pnl, pnl_pips, exit_reason, db_path='enhanced_trading_system.db'):
    import sqlite3
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE enhanced_trades
            SET exit_price = ?, pnl = ?, pnl_pips = ?, exit_reason = ?, closed_at = ?
            WHERE trade_id = ?
        """, (exit_price, pnl, pnl_pips, exit_reason, datetime.now(), trade_id))
        conn.commit()
        conn.close()
        print(f"‚úÖ Trade {trade_id} closed and logged to DB. PnL: {pnl}, Pips: {pnl_pips}, Reason: {exit_reason}")
    except Exception as e:
        print(f"‚ùå Error updating trade {trade_id} in DB: {e}")

# === ADAPTIVE RSI ENSEMBLE SYSTEM ===
class AdaptiveRSIEnsemble:
    def __init__(self, periods=[5,7,9,14,21,25,34,50]):
        self.periods = periods
        self.price_history = {p: deque(maxlen=p+2) for p in periods}
        self.weights = np.ones(len(periods)) / len(periods)
    def update(self, price):
        for p in self.periods:
            self.price_history[p].append(price)
    def compute(self):
        rsi_vals = []
        for i, p in enumerate(self.periods):
            if len(self.price_history[p]) < p + 1:
                rsi_vals.append(50)
                continue
            arr = np.array(self.price_history[p])
            delta = np.diff(arr)
            up = delta.clip(min=0).mean()
            down = -delta.clip(max=0).mean()
            rs = up / down if down != 0 else 0
            rsi = 100 - 100 / (1 + rs) if down != 0 else 100
            rsi_vals.append(rsi)
        return np.array(rsi_vals)
    def ensemble_score(self):
        rsi_vals = self.compute()
        score = np.dot(self.weights, rsi_vals) / np.sum(self.weights)
        return score, rsi_vals
# === HMM REGIME DETECTOR (Stub) ===
class HMMRegimeDetector:
    def __init__(self):
        pass
    def detect(self, price_series):
        return 0
# === RL OPTIMIZER (Stub) ===
class RSIWeightOptimizer:
    def __init__(self, ensemble):
        self.ensemble = ensemble
    def optimize(self, reward):
        pass

# ================== ENHANCED LEARNING COMPONENTS ==================

class AdvancedLearningEngine:
    """Enhanced learning system that learns from rich trade outcomes"""
    
    def __init__(self, system):
        self.system = system
        self.trade_memory = deque(maxlen=1000)
        self.pattern_detector = {}
        self.feature_performance = {}
        self.confidence_calibrator = {}
        self.market_regime_history = deque(maxlen=200)
        self.learning_metrics = {
            'total_updates': 0,
            'pattern_matches': 0,
            'calibration_adjustments': 0
        }
        # Ensure required attributes exist for persistence
        self.trade_outcomes = []  # List of trade outcome dicts
        self.pattern_library = {}  # Dict for pattern learning
        self.feature_importance = {}  # Dict for feature importance
        self.confidence_history = []  # List for confidence tracking
        
    def on_enhanced_trade_outcome(self, trade_data, outcome, exit_details):
        """Learn from comprehensive trade outcome data"""
        try:
            # Calculate trade quality beyond win/loss
            quality_score = self._calculate_trade_quality(trade_data, exit_details)
            
            # Extract rich learning features
            learning_context = self._extract_learning_context(trade_data, exit_details)
            
            # Update multiple learning components
            self._update_pattern_recognition(learning_context, outcome, quality_score)
            self._update_confidence_calibration(trade_data, outcome, quality_score)
            self._update_feature_importance(learning_context, outcome, quality_score)
            self._update_market_regime_learning(learning_context, outcome)
            
            # Store in memory for future reference
            self.trade_memory.append({
                'context': learning_context,
                'outcome': outcome,
                'quality': quality_score,
                'timestamp': datetime.now()
            })
            
            self.learning_metrics['total_updates'] += 1
            
            if self.system.debug_mode:
                print(f"üß† Enhanced learning: Quality={quality_score:.2f}, Context patterns detected")
                
        except Exception as e:
            self.system.logger.error(f"Enhanced learning error: {e}")
    
    def _calculate_trade_quality(self, trade_data, exit_details):
        """Calculate comprehensive trade quality score (0-1)"""
        try:
            entry_price = trade_data.get('entry_price', 0)
            exit_price = exit_details.get('exit_price', entry_price)
            max_favorable = exit_details.get('max_favorable_excursion', exit_price)
            max_adverse = exit_details.get('max_adverse_excursion', entry_price)
            duration_minutes = exit_details.get('duration_minutes', 60)
            exit_reason = exit_details.get('exit_reason', 'unknown')
            
            direction = trade_data.get('direction', 'BUY')
            
            # Calculate profit metrics
            if direction == 'BUY':
                actual_profit = exit_price - entry_price
                potential_profit = max_favorable - entry_price
                max_loss = entry_price - max_adverse
            else:
                actual_profit = entry_price - exit_price
                potential_profit = entry_price - max_favorable
                max_loss = max_adverse - entry_price
            
            # Quality components
            profit_efficiency = 0.5  # Default
            if potential_profit > 0:
                profit_efficiency = max(0, min(1, actual_profit / potential_profit))
            
            risk_efficiency = 0.5  # Default
            if max_loss > 0 and actual_profit != 0:
                risk_efficiency = max(0, min(1, 1 - (max_loss / abs(actual_profit))))
            
            # Time efficiency (prefer faster trades)
            time_efficiency = max(0, min(1, 1 - (duration_minutes / 240)))  # 4 hours max
            
            # Exit quality bonus
            exit_quality = 1.0 if exit_reason == 'take_profit' else 0.8 if exit_reason == 'trailing_stop' else 0.3
            
            # Combined quality score
            quality = (profit_efficiency * 0.4 + risk_efficiency * 0.3 + 
                      time_efficiency * 0.2 + exit_quality * 0.1)
            
            return max(0, min(1, quality))
            
        except Exception as e:
            return 0.5  # Neutral quality if calculation fails
    
    def _extract_learning_context(self, trade_data, exit_details):
        """Extract rich context for learning"""
        # Defensive extraction for features
        analysis = self.system._safe_get(trade_data, 'analysis', {})
        features_raw = self.system._safe_get(analysis, 'features', {})
        features = features_raw.copy() if isinstance(features_raw, dict) else {}

        features['confidence_at_entry'] = self.system._safe_get(trade_data, 'confidence', 0.5)
        features['setup_type_ultra_forced'] = 1 if self.system._safe_get(analysis, 'setup_type') == 'ULTRA_FORCED' else 0
        timestamp = self.system._safe_get(trade_data, 'timestamp', datetime.now())
        atr = self.system._safe_get(features, 'atr', 0.001)
        features['volatility_regime'] = self._classify_volatility_regime(atr, self.system._safe_get(trade_data, 'symbol', ''))
        features['exit_reason_encoded'] = self._encode_exit_reason(self.system._safe_get(exit_details, 'exit_reason', 'unknown'))
        features['duration_normalized'] = min(1, self.system._safe_get(exit_details, 'duration_minutes', 60) / 240)
        # ...existing code...
        features['market_session'] = self._encode_market_session(timestamp)
        return features
    
    def _update_pattern_recognition(self, context, outcome, quality):
        """Learn and recognize successful trading patterns"""
        try:
            # Create pattern signature
            pattern_key = self._create_pattern_signature(context)
            
            if pattern_key not in self.pattern_detector:
                self.pattern_detector[pattern_key] = {
                    'occurrences': 0,
                    'successes': 0,
                    'total_quality': 0.0,
                    'avg_quality': 0.0,
                    'success_rate': 0.0
                }
            
            pattern = self.pattern_detector[pattern_key]
            pattern['occurrences'] += 1
            pattern['successes'] += outcome
            pattern['total_quality'] += quality
            pattern['avg_quality'] = pattern['total_quality'] / pattern['occurrences']
            pattern['success_rate'] = pattern['successes'] / pattern['occurrences']
            
            if pattern['occurrences'] >= 5 and pattern['success_rate'] > 0.6:
                self.learning_metrics['pattern_matches'] += 1
                
        except Exception as e:
            self.system.logger.error(f"Pattern recognition update error: {e}")
    
    def _update_confidence_calibration(self, trade_data, outcome, quality):
        """Learn to calibrate confidence predictions better"""
        try:
            predicted_confidence = trade_data.get('confidence', 0.5)
            confidence_bucket = round(predicted_confidence * 10) / 10  # Round to nearest 0.1
            
            if confidence_bucket not in self.confidence_calibrator:
                self.confidence_calibrator[confidence_bucket] = {
                    'predictions': 0,
                    'successes': 0,
                    'total_quality': 0.0,
                    'calibration_factor': 1.0,
                    'quality_factor': 1.0
                }
            
            cal = self.confidence_calibrator[confidence_bucket]
            cal['predictions'] += 1
            cal['successes'] += outcome
            cal['total_quality'] += quality
            
            if cal['predictions'] >= 10:  # Minimum samples for calibration
                actual_success_rate = cal['successes'] / cal['predictions']
                avg_quality = cal['total_quality'] / cal['predictions']
                
                # Update calibration factors
                if confidence_bucket > 0:
                    cal['calibration_factor'] = actual_success_rate / confidence_bucket
                    cal['quality_factor'] = avg_quality * 2  # Quality multiplier
                
                self.learning_metrics['calibration_adjustments'] += 1
                
        except Exception as e:
            self.system.logger.error(f"Confidence calibration error: {e}")
    
    def _update_feature_importance(self, context, outcome, quality):
        """Track which features matter most for success"""
        try:
            for feature_name, feature_value in context.items():
                if feature_name not in self.feature_performance:
                    self.feature_performance[feature_name] = {
                        'total_trades': 0,
                        'successful_trades': 0,
                        'total_quality': 0.0,
                        'importance_score': 0.5
                    }
                
                perf = self.feature_performance[feature_name]
                perf['total_trades'] += 1
                perf['successful_trades'] += outcome
                perf['total_quality'] += quality
                
                # Update importance score
                if perf['total_trades'] >= 20:
                    success_rate = perf['successful_trades'] / perf['total_trades']
                    avg_quality = perf['total_quality'] / perf['total_trades']
                    perf['importance_score'] = (success_rate * 0.6 + avg_quality * 0.4)
                    
        except Exception as e:
            self.system.logger.error(f"Feature importance update error: {e}")
    
    def _update_market_regime_learning(self, context, outcome):
        """Learn market regime patterns"""
        try:
            # Use _safe_get via system reference to handle possible float values
            if hasattr(self, 'system') and self.system is not None:
                safe_get = self.system._safe_get
            else:
                # Local safe_get implementation if system is not available
                safe_get = lambda obj, key, default=None: obj.get(key, default) if isinstance(obj, dict) else default
                
            # Extract market regime features
            regime_features = [
                safe_get(context, 'high_volatility', 0),
                safe_get(context, 'rsi_extreme', 0),
                safe_get(context, 'london_session', 0),
                safe_get(context, 'newyork_session', 0),
                safe_get(context, 'overlap_session', 0)
            ]
            
            # Store regime data for analysis
            self.market_regime_history.append({
                'features': regime_features,
                'outcome': outcome,
                'timestamp': datetime.now()
            })
            
        except Exception as e:
            self.system.logger.error(f"Market regime learning error: {e}")
    
    def get_enhanced_predictions(self, base_predictions, features, symbol, timeframe):
        """Enhance predictions with learning insights"""
        try:
            enhanced_predictions = base_predictions.copy()
            
            # Apply confidence calibration
            for model_name, confidence in enhanced_predictions.items():
                confidence_bucket = round(confidence * 10) / 10
                if confidence_bucket in self.confidence_calibrator:
                    cal = self.confidence_calibrator[confidence_bucket]
                    if cal['predictions'] >= 10:
                        calibration_factor = cal['calibration_factor']
                        quality_factor = cal['quality_factor']
                        enhanced_predictions[model_name] = min(0.95, max(0.05, 
                            confidence * calibration_factor * quality_factor))
            
            # Apply pattern recognition boost
            pattern_boost = self._get_pattern_boost(features, symbol, timeframe)
            if pattern_boost > 0:
                for model_name in enhanced_predictions:
                    enhanced_predictions[model_name] = min(0.95, 
                        enhanced_predictions[model_name] + pattern_boost)
            
            return enhanced_predictions
            
        except Exception as e:
            self.system.logger.error(f"Enhanced prediction error: {e}")
            return base_predictions
    
    def _get_pattern_boost(self, features, symbol, timeframe):
        """Get confidence boost from recognized successful patterns"""
        try:
            current_context = {
                'symbol': symbol,
                'timeframe': timeframe,
                'rsi_range': self._discretize_rsi(self.system._safe_get(features, 'rsi', 50)),
                'volatility_regime': self.system._safe_get(features, 'high_volatility', 0),
                'session': self._encode_market_session(datetime.now())
            }
            
            pattern_key = self._create_pattern_signature(current_context)
            
            if pattern_key in self.pattern_detector:
                pattern = self.pattern_detector[pattern_key]
                if pattern['occurrences'] >= 5 and pattern['success_rate'] > 0.6:
                    boost = (pattern['success_rate'] - 0.5) * pattern['avg_quality'] * 0.2
                    return min(0.15, boost)  # Cap boost at 15%
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    # Helper methods
    def _create_pattern_signature(self, context):
        """Create a unique signature for pattern recognition"""
        # Use _safe_get via system reference to handle possible float values
        if hasattr(self, 'system') and self.system is not None:
            safe_get = self.system._safe_get
        else:
            # Local safe_get implementation if system is not available
            safe_get = lambda obj, key, default=None: obj.get(key, default) if isinstance(obj, dict) else default
            
        signature_features = [
            safe_get(context, 'setup_type_ultra_forced', 0),
            self._discretize_rsi(safe_get(context, 'rsi', 50)),
            safe_get(context, 'volatility_regime', 0),
            safe_get(context, 'market_session', 0)
        ]
        return str(hash(tuple(signature_features)))
    
    def _discretize_rsi(self, rsi):
        """Convert RSI to discrete categories"""
        if rsi < 25: return 'oversold'
        elif rsi < 40: return 'low'
        elif rsi < 60: return 'neutral'
        elif rsi < 75: return 'high'
        else: return 'overbought'
    
    def _encode_market_session(self, timestamp):
        """Encode market session"""
        hour = timestamp.hour
        if 8 <= hour <= 16: return 'london'
        elif 13 <= hour <= 21: return 'newyork'
        else: return 'asian'
    
    def _classify_volatility_regime(self, atr, symbol):
        """Classify volatility regime"""
        if 'XAU' in symbol:
            if atr < 2.0: return 'low'
            elif atr < 8.0: return 'medium'
            else: return 'high'
        else:
            if atr < 0.001: return 'low'
            elif atr < 0.003: return 'medium'
            else: return 'high'
    
    def _encode_exit_reason(self, reason):
        """Encode exit reason"""
        reason_map = {
            'take_profit': 1.0, 'trailing_stop': 0.8, 'stop_loss': 0.0,
            'manual': 0.5, 'time_exit': 0.3
        }
        return reason_map.get(reason.lower(), 0.5)
    
    def calibrate_confidence(self, confidence, features, market_context):
        """Calibrate confidence based on historical performance"""
        try:
            # Get confidence bucket
            confidence_bucket = round(confidence * 10) / 10
            
            # Check if we have calibration data for this bucket
            if confidence_bucket in self.confidence_calibrator:
                cal_data = self.confidence_calibrator[confidence_bucket]
                if cal_data['predictions'] >= 10:  # Minimum samples for calibration
                    calibration_factor = cal_data['calibration_factor']
                    quality_factor = cal_data['quality_factor']
                    
                    # Apply calibration
                    calibrated = min(0.95, max(0.05, 
                        confidence * calibration_factor * quality_factor))
                    
                    return calibrated
            
            # If no calibration data, return original confidence
            return confidence
            
        except Exception as e:
            self.system.logger.error(f"Confidence calibration error: {e}")
            return confidence
    
    def get_trading_insights(self):
        """Get comprehensive trading insights from learning system"""
        try:
            insights = {
                'recent_accuracy': 0.5,
                'pattern_count': len(self.pattern_detector),
                'confidence_accuracy': 0.5,
                'total_updates': self.learning_metrics['total_updates'],
                'pattern_matches': self.learning_metrics['pattern_matches']
            }
            
            # Calculate recent accuracy from trade memory
            if len(self.trade_memory) >= 10:
                recent_trades = list(self.trade_memory)[-20:]
                total_outcomes = sum(trade['outcome'] for trade in recent_trades)
                insights['recent_accuracy'] = total_outcomes / len(recent_trades)
            
            # Calculate overall confidence accuracy
            if self.confidence_calibrator:
                total_predictions = sum(cal['predictions'] for cal in self.confidence_calibrator.values())
                total_successes = sum(cal['successes'] for cal in self.confidence_calibrator.values())
                if total_predictions > 0:
                    insights['confidence_accuracy'] = total_successes / total_predictions
            
            # Get best performing patterns
            if self.pattern_detector:
                patterns = [
                    (pattern_key, data) for pattern_key, data in self.pattern_detector.items()
                    if data['occurrences'] >= 5 and data['success_rate'] > 0.6
                ]
                insights['high_performance_patterns'] = len(patterns)
            else:
                insights['high_performance_patterns'] = 0
            
            return insights
            
        except Exception as e:
            self.system.logger.error(f"Trading insights error: {e}")
            return {
                'recent_accuracy': 0.5,
                'pattern_count': 0,
                'confidence_accuracy': 0.5,
                'total_updates': 0,
                'pattern_matches': 0,
                'high_performance_patterns': 0
            }


class RealTimeAnalytics:
    def update_system_metrics(self):
        """Stub for system metrics update (to avoid attribute error)"""
        pass
    """Real-time performance analytics and monitoring"""
    
    def __init__(self, system):
        self.system = system
        self.metrics_history = deque(maxlen=1000)
        self.alerts = deque(maxlen=50)
        self.performance_dashboard = {}
        self.baseline_metrics = None
        # Ensure required attributes exist for persistence
        self.performance_history = []  # List of performance snapshots
        self.confidence_accuracy = {}  # Dict for confidence accuracy tracking
        self.total_trades = 0
        self.winning_trades = 0
        self.current_drawdown = 0.0
        self.max_drawdown = 0.0
        
    def calculate_live_metrics(self, trades, timeframe_hours=24):
        """Calculate comprehensive live performance metrics"""
        try:
            if not trades:
                return self._get_empty_metrics()
            
            # Filter trades by timeframe
            cutoff_time = datetime.now() - timedelta(hours=timeframe_hours)
            recent_trades = [t for t in trades if t.get('timestamp', datetime.min) > cutoff_time]
            
            if not recent_trades:
                return self._get_empty_metrics()
            
            # Calculate metrics
            metrics = {
                'timestamp': datetime.now(),
                'timeframe_hours': timeframe_hours,
                'total_trades': len(recent_trades),
                'win_rate': self._calculate_win_rate(recent_trades),
                'profit_factor': self._calculate_profit_factor(recent_trades),
                'sharpe_ratio': self._calculate_sharpe_ratio(recent_trades),
                'max_drawdown': self._calculate_max_drawdown(recent_trades),
                'avg_trade_duration': self._calculate_avg_duration(recent_trades),
                'confidence_accuracy': self._calculate_confidence_accuracy(recent_trades),
                'ultra_forced_performance': self._calculate_ultra_forced_performance(recent_trades)
            }
            
            # Store metrics
            self.metrics_history.append(metrics)
            self.performance_dashboard = metrics
            
            # Check for alerts
            self._check_performance_alerts(metrics)
            
            return metrics
            
        except Exception as e:
            self.system.logger.error(f"Live metrics calculation error: {e}")
            return self._get_empty_metrics()
    
    def _calculate_win_rate(self, trades):
        """Calculate win rate"""
        if not trades:
            return 0.0
        wins = sum(1 for t in trades if t.get('actual_outcome') == 1)
        return wins / len(trades)
    
    def _calculate_profit_factor(self, trades):
        """Calculate profit factor"""
        profits = [t.get('pnl', 0) for t in trades if t.get('pnl', 0) > 0]
        losses = [abs(t.get('pnl', 0)) for t in trades if t.get('pnl', 0) < 0]
        
        total_profit = sum(profits) if profits else 0
        total_loss = sum(losses) if losses else 1  # Avoid division by zero
        
        return total_profit / total_loss if total_loss > 0 else 0
    
    def _calculate_sharpe_ratio(self, trades):
        """Calculate Sharpe ratio"""
        if len(trades) < 2:
            return 0.0
        
        returns = [t.get('pnl', 0) for t in trades]
        avg_return = np.mean(returns)
        std_return = np.std(returns)
        
        return (avg_return / std_return) if std_return > 0 else 0
    
    def _calculate_max_drawdown(self, trades):
        """Calculate maximum drawdown"""
        if not trades:
            return 0.0
        
        cumulative_pnl = 0
        peak = 0
        max_drawdown = 0
        
        for trade in sorted(trades, key=lambda x: x.get('timestamp', datetime.min)):
            cumulative_pnl += trade.get('pnl', 0)
            if cumulative_pnl > peak:
                peak = cumulative_pnl
            drawdown = peak - cumulative_pnl
            if drawdown > max_drawdown:
                max_drawdown = drawdown
        
        return max_drawdown
    
    def _calculate_avg_duration(self, trades):
        """Calculate average trade duration"""
        durations = [t.get('duration_minutes', 60) for t in trades if 'duration_minutes' in t]
        return np.mean(durations) if durations else 60
    
    def _calculate_confidence_accuracy(self, trades):
        """Calculate how well confidence predictions match actual outcomes"""
        if not trades:
            return 0.0
        
        confidence_bins = {}
        for trade in trades:
            confidence = trade.get('confidence', 0.5)
            outcome = trade.get('actual_outcome', 0)
            bin_key = round(confidence * 10) / 10
            
            if bin_key not in confidence_bins:
                confidence_bins[bin_key] = {'predictions': 0, 'successes': 0}
            
            confidence_bins[bin_key]['predictions'] += 1
            confidence_bins[bin_key]['successes'] += outcome
        
        # Calculate average calibration error
        total_error = 0
        total_predictions = 0
        
        for confidence, data in confidence_bins.items():
            if data['predictions'] >= 5:  # Minimum sample size
                actual_rate = data['successes'] / data['predictions']
                error = abs(confidence - actual_rate)
                total_error += error * data['predictions']
                total_predictions += data['predictions']
        
        return 1 - (total_error / total_predictions) if total_predictions > 0 else 0.5
    
    def _calculate_ultra_forced_performance(self, trades):
        """Calculate performance specifically for ULTRA_FORCED setups"""
        ultra_forced_trades = [t for t in trades if t.get('analysis', {}).get('setup_type') == 'ULTRA_FORCED']
        
        if not ultra_forced_trades:
            return {'count': 0, 'win_rate': 0, 'avg_quality': 0}
        
        win_rate = self._calculate_win_rate(ultra_forced_trades)
        avg_pnl = np.mean([t.get('pnl', 0) for t in ultra_forced_trades])
        
        return {
            'count': len(ultra_forced_trades),
            'win_rate': win_rate,
            'avg_pnl': avg_pnl,
            'percentage_of_total': len(ultra_forced_trades) / len(trades) if trades else 0
        }
    
    def _check_performance_alerts(self, current_metrics):
        """Check for performance degradation and generate alerts"""
        try:
            alerts = []
            
            # Set baseline if not exists
            if self.baseline_metrics is None and len(self.metrics_history) >= 10:
                recent_metrics = list(self.metrics_history)[-10:]
                self.baseline_metrics = {
                    'win_rate': np.mean([m['win_rate'] for m in recent_metrics]),
                    'profit_factor': np.mean([m['profit_factor'] for m in recent_metrics]),
                    'max_drawdown': np.mean([m['max_drawdown'] for m in recent_metrics])
                }
            
            if self.baseline_metrics:
                # Win rate degradation
                if current_metrics['win_rate'] < self.baseline_metrics['win_rate'] * 0.7:
                    alerts.append({
                        'type': 'performance_degradation',
                        'severity': 'high',
                        'message': f"Win rate dropped to {current_metrics['win_rate']:.1%} (baseline: {self.baseline_metrics['win_rate']:.1%})",
                        'action': 'increase_confidence_threshold',
                        'timestamp': datetime.now()
                    })
                
                # Drawdown alert
                if current_metrics['max_drawdown'] > self.baseline_metrics['max_drawdown'] * 1.5:
                    alerts.append({
                        'type': 'risk_alert',
                        'severity': 'critical',
                        'message': f"Drawdown exceeding baseline by 50%: {current_metrics['max_drawdown']:.2f}",
                        'action': 'reduce_position_size',
                        'timestamp': datetime.now()
                    })
                
                # Low confidence accuracy
                if current_metrics['confidence_accuracy'] < 0.6:
                    alerts.append({
                        'type': 'calibration_alert',
                        'severity': 'medium',
                        'message': f"Poor confidence calibration: {current_metrics['confidence_accuracy']:.1%}",
                        'action': 'recalibrate_models',
                        'timestamp': datetime.now()
                    })
            
            # Add alerts to queue
            for alert in alerts:
                self.alerts.append(alert)
                if self.system.debug_mode:
                    print(f"üö® ALERT [{alert['severity'].upper()}]: {alert['message']}")
                    
        except Exception as e:
            self.system.logger.error(f"Performance alert check error: {e}")
    
    def get_performance_summary(self):
        """Get formatted performance summary"""
        if not self.performance_dashboard:
            return "No performance data available"
        
        metrics = self.performance_dashboard
        
        summary = f"""
üìä REAL-TIME PERFORMANCE SUMMARY
{'='*50}
‚è±Ô∏è  Timeframe: {metrics['timeframe_hours']} hours
üìà Total Trades: {metrics['total_trades']}
üéØ Win Rate: {metrics['win_rate']:.1%}
üí∞ Profit Factor: {metrics['profit_factor']:.2f}
üìâ Max Drawdown: {metrics['max_drawdown']:.2f}
‚ö° Sharpe Ratio: {metrics['sharpe_ratio']:.2f}
üîç Confidence Accuracy: {metrics['confidence_accuracy']:.1%}

üéØ ULTRA_FORCED Performance:
   ‚Ä¢ Count: {metrics['ultra_forced_performance']['count']}
   ‚Ä¢ Win Rate: {metrics['ultra_forced_performance']['win_rate']:.1%}
   ‚Ä¢ Avg P&L: {metrics['ultra_forced_performance']['avg_pnl']:.2f}
   ‚Ä¢ % of Total: {metrics['ultra_forced_performance']['percentage_of_total']:.1%}
"""
        
        # Add recent alerts
        if self.alerts:
            summary += f"\nüö® Recent Alerts ({len(self.alerts)}):\n"
            for alert in list(self.alerts)[-3:]:  # Show last 3 alerts
                summary += f"   ‚Ä¢ [{alert['severity'].upper()}] {alert['message']}\n"
        
        return summary
    
    def _get_empty_metrics(self):
        """Return empty metrics structure"""
        return {
            'timestamp': datetime.now(),
            'timeframe_hours': 24,
            'total_trades': 0,
            'win_rate': 0.0,
            'profit_factor': 0.0,
            'sharpe_ratio': 0.0,
            'max_drawdown': 0.0,
            'avg_trade_duration': 0.0,
            'confidence_accuracy': 0.0,
            'ultra_forced_performance': {'count': 0, 'win_rate': 0, 'avg_quality': 0}
        }


class EnhancedRiskManager:
    """Advanced risk management with dynamic adjustments"""
    
    def __init__(self, system):
        self.system = system
        self.risk_metrics = {}
        self.position_correlations = {}
        self.dynamic_adjustments = {}
        self.risk_alerts = deque(maxlen=20)
        # Ensure required attributes exist for persistence
        self.portfolio_risk = 0.0  # Current portfolio risk
        self.risk_history = []  # List of risk snapshots
        self.asset_correlations = {}  # Dict for asset correlations

    def update_market_conditions(self, market_data=None):
        """Stub for updating market conditions (accepts optional market_data)"""
        pass

    def get_risk_multiplier(self, *args, **kwargs):
        """Stub for getting risk multiplier (to avoid argument errors)"""
        return 1.0
        
    def calculate_portfolio_risk(self, active_positions, market_data):
        """Calculate comprehensive portfolio risk metrics"""
        try:
            if not active_positions:
                return self._get_empty_risk_metrics()
            
            # Calculate basic risk metrics
            risk_metrics = {
                'timestamp': datetime.now(),
                'total_positions': len(active_positions),
                'portfolio_var_95': 0.0,
                'portfolio_cvar_95': 0.0,
                'max_position_correlation': 0.0,
                'diversification_ratio': 0.0,
                'leverage_ratio': 0.0,
                'risk_concentration': 0.0
            }
            
            self.risk_metrics = risk_metrics
            return risk_metrics
            
        except Exception as e:
            self.system.logger.error(f"Portfolio risk calculation error: {e}")
            return self._get_empty_risk_metrics()
    
    def apply_dynamic_risk_adjustment(self, base_risk_per_trade, market_conditions, account_performance):
        """Dynamically adjust risk based on multiple factors"""
        try:
            adjusted_risk = base_risk_per_trade
            
            # Market volatility adjustment
            market_volatility = market_conditions.get('avg_atr', 0.001)
            vol_adjustment = min(1.5, max(0.5, 1.0 / (market_volatility * 1000 + 0.1)))
            adjusted_risk *= vol_adjustment
            
            # Account performance adjustment
            recent_win_rate = account_performance.get('win_rate_30d', 0.5)
            recent_drawdown = account_performance.get('max_drawdown_30d', 0.0)
            
            # Performance-based adjustment
            if recent_win_rate > 0.6:
                perf_adjustment = 1.1  # Slightly more aggressive
            elif recent_win_rate < 0.4:
                perf_adjustment = 0.8  # More conservative
            else:
                perf_adjustment = 1.0
            
            # Drawdown-based adjustment
            if recent_drawdown > 0.1:  # 10% drawdown
                dd_adjustment = 0.7  # Significantly reduce risk
            elif recent_drawdown > 0.05:  # 5% drawdown
                dd_adjustment = 0.45  # More aggressive risk reduction
            else:
                dd_adjustment = 1.0
            
            # Apply all adjustments
            adjusted_risk *= perf_adjustment * dd_adjustment
            
            # Ensure bounds
            final_risk = max(0.005, min(0.03, adjusted_risk))  # Between 0.5% and 3%
            
            # Store adjustment info
            self.dynamic_adjustments = {
                'base_risk': base_risk_per_trade,
                'vol_adjustment': vol_adjustment,
                'perf_adjustment': perf_adjustment,
                'dd_adjustment': dd_adjustment,
                'final_risk': final_risk,
                'timestamp': datetime.now()
            }
            
            if self.system.debug_mode:
                print(f"üõ°Ô∏è Dynamic risk adjustment: {base_risk_per_trade:.1%} ‚Üí {final_risk:.1%}")
            
            return final_risk
            
        except Exception as e:
            self.system.logger.error(f"Dynamic risk adjustment error: {e}")
            return base_risk_per_trade
    
    def _get_empty_risk_metrics(self):
        """Return empty risk metrics"""
        return {
            'timestamp': datetime.now(),
            'total_positions': 0,
            'portfolio_var_95': 0.0,
            'portfolio_cvar_95': 0.0,
            'max_position_correlation': 0.0,
            'diversification_ratio': 0.0,
            'leverage_ratio': 0.0,
            'risk_concentration': 0.0
        }

# ================== MAIN TRADING SYSTEM ==================

class EnhancedAdaptiveMLTradingSystem:
    @staticmethod
    def simple_profitable_rsi_signal(data):
        """Best-practice RSI signal: only trade when RSI exits extreme and trend confirms."""
        def calculate_rsi(series, period=14):
            delta = series.diff()
            up = delta.clip(lower=0)
            down = -1 * delta.clip(upper=0)
            ma_up = up.rolling(window=period, min_periods=period).mean()
            ma_down = down.rolling(window=period, min_periods=period).mean()
            rs = ma_up / (ma_down + 1e-9)
            rsi = 100 - (100 / (1 + rs))
            return rsi

        rsi = calculate_rsi(data['close'], 14)
        sma_50 = data['close'].rolling(50).mean()
        current_rsi = rsi.iloc[-1]
        prev_rsi = rsi.iloc[-2]
        current_price = data['close'].iloc[-1]
        sma_now = sma_50.iloc[-1]

        # BUY: RSI exits oversold in uptrend
        if prev_rsi <= 20 and current_rsi > 20 and current_price > sma_now:
            return "BUY"
        # SELL: RSI exits overbought in downtrend
        elif prev_rsi >= 80 and current_rsi < 80 and current_price < sma_now:
            return "SELL"
        return "HOLD"
    def __init__(self, config_file='adaptive_ml_trading_config.json'):
        """
        Initialize the trading system and load canonical feature columns from training.
        """
        # Set random seed for reproducibility
        np.random.seed(42)
        import random
        random.seed(42)
        self.feature_columns = []
        # Load canonical feature columns from training if available
        try:
            with open('feature_columns.json', 'r') as f:
                self.feature_columns = json.load(f)
            print(f"‚úÖ Loaded {len(self.feature_columns)} canonical feature columns from training.")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load feature_columns.json: {e}")
        # Load dual-model components
        try:
            import joblib
            self.signal_finder_package = joblib.load('signal_finder_model.pkl')
            self.signal_finder = self.signal_finder_package['model']
            self.signal_finder_scaler = self.signal_finder_package['scaler']
            self.signal_finder_features = self.signal_finder_package['feature_columns']
            self.signal_finder_threshold = self.signal_finder_package['optimal_threshold']
            print(f"‚úÖ Signal Finder model loaded for dual-model integration.")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load signal_finder_model.pkl: {e}")
            self.signal_finder = None
        try:
            self.main_model_package = joblib.load('production_models_latest.pkl')
            self.main_model = self.main_model_package['models']['timeframe_ensemble']
            self.main_scaler = self.main_model_package['scaler']
            self.main_features = self.main_model_package['feature_columns']
            print(f"‚úÖ Main Precision model loaded for dual-model integration.")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load production_models_latest.pkl: {e}")
            self.main_model = None

    def train_main_model_with_cv(self, X, y, model_type='RandomForest', param_grid=None, cv=5, scaler=None, save_path='production_models_latest.pkl'):
        """
        Train the main model with cross-validation, hyperparameter tuning, and feature scaling.
        X: pd.DataFrame, y: pd.Series or np.array
        model_type: 'RandomForest' or 'SGDClassifier'
        param_grid: dict, hyperparameters for GridSearchCV
        cv: int, number of folds
        scaler: sklearn scaler instance or None
        save_path: str, where to save the trained model
        """
        import joblib
        from sklearn.model_selection import GridSearchCV
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import SGDClassifier
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import classification_report, accuracy_score
        import logging
        try:
            # Handle missing values
            X = X.fillna(0)
            # Feature scaling
            if scaler is None:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
            else:
                X_scaled = scaler.fit_transform(X)
            # Model selection
            if model_type == 'RandomForest':
                model = RandomForestClassifier(random_state=42)
                if param_grid is None:
                    param_grid = {'n_estimators': [100, 200], 'max_depth': [5, 10, None]}
            elif model_type == 'SGDClassifier':
                model = SGDClassifier(random_state=42)
                if param_grid is None:
                    param_grid = {'alpha': [0.0001, 0.001, 0.01]}
            else:
                raise ValueError('Unsupported model_type')
            # Cross-validation and hyperparameter tuning
            grid = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
            grid.fit(X_scaled, y)
            best_model = grid.best_estimator_
            print(f"Best params: {grid.best_params_}")
            print(f"Best CV score: {grid.best_score_:.4f}")
            # Evaluate on training data
            y_pred = best_model.predict(X_scaled)
            print("Training classification report:")
            print(classification_report(y, y_pred))
            # Log feature importance if available
            if hasattr(best_model, 'feature_importances_'):
                importances = best_model.feature_importances_
                feature_importance = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)
                print("Top 10 features by importance:")
                for feat, imp in feature_importance[:10]:
                    print(f"  {feat}: {imp:.4f}")
            # Save model, scaler, and feature columns
            model_package = {
                'models': {'timeframe_ensemble': best_model},
                'scaler': scaler,
                'feature_columns': list(X.columns)
            }
            joblib.dump(model_package, save_path)
            print(f"‚úÖ Model and scaler saved to {save_path}")
            # Update instance variables
            self.main_model = best_model
            self.main_scaler = scaler
            self.main_features = list(X.columns)
        except Exception as e:
            print(f"‚ùå Error in training pipeline: {e}")
            logging.error(f"Error in training pipeline: {e}")

    def train_main_model_with_cv(self, X, y, model_type='RandomForest', param_grid=None, cv=5, scaler=None, save_path='production_models_latest.pkl'):
        """
        Train the main model with cross-validation, hyperparameter tuning, and feature scaling.
        X: pd.DataFrame, y: pd.Series or np.array
        model_type: 'RandomForest' or 'SGDClassifier'
        param_grid: dict, hyperparameters for GridSearchCV
        cv: int, number of folds
        scaler: sklearn scaler instance or None
        save_path: str, where to save the trained model
        """
        import joblib
        from sklearn.model_selection import GridSearchCV
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import SGDClassifier
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import classification_report, accuracy_score
        import logging
        try:
            # Handle missing values
            X = X.fillna(0)
            # Feature scaling
            if scaler is None:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
            else:
                X_scaled = scaler.fit_transform(X)
            # Model selection
            if model_type == 'RandomForest':
                model = RandomForestClassifier(random_state=42)
                if param_grid is None:
                    param_grid = {'n_estimators': [100, 200], 'max_depth': [5, 10, None]}
            elif model_type == 'SGDClassifier':
                model = SGDClassifier(random_state=42)
                if param_grid is None:
                    param_grid = {'alpha': [0.0001, 0.001, 0.01]}
            else:
                raise ValueError('Unsupported model_type')
            # Cross-validation and hyperparameter tuning
            grid = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
            grid.fit(X_scaled, y)
            best_model = grid.best_estimator_
            print(f"Best params: {grid.best_params_}")
            print(f"Best CV score: {grid.best_score_:.4f}")
            # Evaluate on training data
            y_pred = best_model.predict(X_scaled)
            print("Training classification report:")
            print(classification_report(y, y_pred))
            # Log feature importance if available
            if hasattr(best_model, 'feature_importances_'):
                importances = best_model.feature_importances_
                feature_importance = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)
                print("Top 10 features by importance:")
                for feat, imp in feature_importance[:10]:
                    print(f"  {feat}: {imp:.4f}")
            # Save model, scaler, and feature columns
            model_package = {
                'models': {'timeframe_ensemble': best_model},
                'scaler': scaler,
                'feature_columns': list(X.columns)
            }
            joblib.dump(model_package, save_path)
            print(f"‚úÖ Model and scaler saved to {save_path}")
            # Update instance variables
            self.main_model = best_model
            self.main_scaler = scaler
            self.main_features = list(X.columns)
        except Exception as e:
            print(f"‚ùå Error in training pipeline: {e}")
            logging.error(f"Error in training pipeline: {e}")

    def dual_model_predict(self, X):
        """
        Dual-model prediction: only return signals that pass both the signal finder and main model.
        X: DataFrame with live features (columns must include all required by both models)
        Returns: dict with dual_predictions, dual_confidence, and details from both models
        """
        import numpy as np
        import pandas as pd
        # Prepare features for signal finder
        sf_features = [col for col in self.signal_finder_features if col in X.columns]
        X_sf = X[sf_features].copy()
        # Fill missing features with 0
        for col in self.signal_finder_features:
            if col not in X_sf.columns:
                X_sf[col] = 0
        X_sf = X_sf[self.signal_finder_features]
        # Scale
        X_sf_scaled = self.signal_finder_scaler.transform(X_sf)
        sf_probs = self.signal_finder.predict_proba(X_sf_scaled)[:, 1]
        sf_preds = (sf_probs >= self.signal_finder_threshold).astype(int)
        # Prepare features for main model
        main_features = [col for col in self.main_features if col in X.columns]
        X_main = X[main_features].copy()
        for col in self.main_features:
            if col not in X_main.columns:
                X_main[col] = 0
        X_main = X_main[self.main_features]
        X_main_scaled = self.main_scaler.transform(X_main)
        main_probs = self.main_model.predict_proba(X_main_scaled)[:, 1]
        main_preds = (main_probs >= 0.5).astype(int)
        # Dual filter: only signals that pass both
        dual_predictions = (sf_preds == 1) & (main_preds == 1)
        dual_confidence = np.where(dual_predictions, main_probs, 0)
        return {
            'dual_predictions': dual_predictions,
            'dual_confidence': dual_confidence,
            'signal_finder_probs': sf_probs,
            'main_model_probs': main_probs
        }
    def _calculate_position_size(self, symbol, confidence):
        """Position sizing optimized for small accounts"""
        account = mt5.account_info()
        risk_amount = account.balance * self.risk_per_trade
        # Get symbol info
        symbol_info = mt5.symbol_info(symbol)
        if symbol_info is None:
            return 0.01  # Default to minimum
        # Calculate position size based on stop distance
        stop_distance = symbol_info.point * 100  # 100 pips as example
        if stop_distance == 0:
            return 0.01
        # Convert risk amount to lots
        tick_value = symbol_info.trade_tick_value
        if tick_value == 0:
            return 0.01
        lots = (risk_amount / stop_distance) / tick_value
        # Apply confidence multiplier
        lots *= min(2.0, confidence * 2)  # Scale up to 2x for high confidence
        # Ensure within broker limits
        lots = max(symbol_info.volume_min, min(symbol_info.volume_max, lots))
        # Round to nearest 0.01
        return round(lots * 100) / 100
    def scan_all_symbols(self):
        """Scan all configured symbols and timeframes, always providing the full feature set for each scan."""
        signals = []
        for symbol in self.config['trading']['symbols']:
            for timeframe in self.config['trading']['timeframes']:
                try:
                    # Get market data
                    data = self.get_live_market_data(symbol, timeframe, bars=100)
                    if data is None or len(data) < 20:
                        continue
                    # === Adaptive RSI Feature Injection ===
                    close_prices = data['close'].values if 'close' in data else None
                    if close_prices is not None:
                        self.rsi_ensemble.update(close_prices[-1])
                        rsi_score, rsi_vals = self.rsi_ensemble.ensemble_score()
                    else:
                        rsi_score, rsi_vals = 50, [50]*len(self.rsi_ensemble.periods)
                    # Generate features for this (symbol, timeframe)
                    features = self.calculate_live_features(symbol, timeframe, data)
                    if not features:
                        continue
                    # Inject RSI features
                    features['adaptive_rsi_score'] = rsi_score
                    for i, p in enumerate(self.rsi_ensemble.periods):
                        features[f'rsi_{p}'] = rsi_vals[i]
                    # Build full feature vector for model (robust: always match training columns)
                    # Always use self.main_features as the canonical list
                    feature_vector = {col: features.get(col, 0) for col in self.main_features}
                    # Log missing features for this scan
                    missing = [col for col in self.main_features if col not in features]
                    if missing:
                        print(f"‚ö†Ô∏è For {symbol} {timeframe}, missing features filled with 0: {missing}")
                    X = pd.DataFrame([feature_vector], columns=self.main_features)
                    # Use dual-model prediction if available, else fallback
                    if hasattr(self, 'dual_model_predict'):
                        result = self.dual_model_predict(X)
                        # Only add signals that pass both models
                        if result and result['dual_predictions'][0]:
                            signals.append({
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'direction': 'BUY' if result['dual_confidence'][0] > 0 else 'SELL',
                                'confidence': float(result['dual_confidence'][0]),
                                'features': feature_vector,
                                'timestamp': datetime.now(),
                                'adaptive_rsi_score': rsi_score,
                                'rsi_vals': rsi_vals
                            })
                            if self.debug_mode:
                                print(f"Dual-model signal: {symbol} {timeframe} Confidence: {result['dual_confidence'][0]:.2f} | RSI: {rsi_score:.2f}")
                    else:
                        # Fallback to single-model prediction
                        prediction = self.predict_with_ensemble(features, symbol, timeframe)
                        if prediction['confidence'] >= self._get_dynamic_threshold(symbol, timeframe):
                            signals.append({
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'direction': prediction['direction'],
                                'confidence': prediction['confidence'],
                                'features': features,
                                'timestamp': datetime.now()
                            })
                            if self.debug_mode:
                                print(f"Signal found: {symbol} {timeframe} {prediction['direction']} "
                                      f"Confidence: {prediction['confidence']:.2f}")
                except Exception as e:
                    print(f"Scan error for {symbol} {timeframe}: {e}")
        return signals
    def run_trading_loop(self):
        """Main trading loop"""
        print("Starting trading loop...")
        while True:
            try:
                # 1. Check system status
                if not self.check_system_status():
                    time.sleep(60)
                    continue
                # 2. Scan for opportunities
                signals = self.scan_all_symbols()
                # 3. Execute trades
                for signal in signals:
                    if self._passes_filters(signal):
                        trade_id = self._execute_enhanced_trade(signal)
                        if trade_id:
                            print(f"Executed trade {trade_id}")
                            # Attribution logging for adaptive RSI
                            if 'adaptive_rsi_score' in signal:
                                logging.info(f"Trade {trade_id} | Symbol: {signal['symbol']} | Timeframe: {signal['timeframe']} | RSI Score: {signal['adaptive_rsi_score']:.2f} | RSI vals: {signal['rsi_vals']}")
                # 4. Monitor open positions
                self.check_completed_trades()
                # 5. Update learning system
                self.update_enhanced_learning_system()
                # 6. Update RSI weights (stub, can be improved with reward feedback)
                self.update_rsi_weights()
                # 7. Sleep until next cycle
                time.sleep(30)
            except Exception as e:
                print(f"Trading loop error: {e}")
                time.sleep(60)
    def create_performance_dashboard(self):
        """Create comprehensive performance monitoring dashboard"""
        try:
            # Define dashboard creation logic
            def generate_dashboard():
                """Generate performance dashboard text report"""
                if not hasattr(self, 'performance_dashboard_data'):
                    self.performance_dashboard_data = {
                        'last_updated': datetime.now(),
                        'update_interval': 60*60,  # Update hourly
                        'metrics': {}
                    }
                # Check if update is needed
                now = datetime.now()
                if ((now - self.performance_dashboard_data['last_updated']).total_seconds() < 
                    self.performance_dashboard_data['update_interval']):
                    # Use cached data if recent
                    return self._format_dashboard(self.performance_dashboard_data['metrics'])
                # Calculate fresh metrics
                metrics = {}
                # Account metrics
                if mt5:
                    account = mt5.account_info()
                    if account:
                        metrics['account'] = {
                            'balance': account.balance,
                            'equity': account.equity,
                            'margin_level': account.margin_level,
                            'free_margin': account.margin_free
                        }
                # Trading metrics
                if self.trade_history:
                    recent_trades = self.trade_history[-100:]
                    total_trades = len(recent_trades)
                    winning_trades = sum(1 for t in recent_trades if t.get('actual_outcome', 0) == 1)
                    metrics['trading'] = {
                        'total_trades': total_trades,
                        'win_rate': winning_trades / total_trades if total_trades else 0,
                        'avg_win': np.mean([t.get('pnl', 0) for t in recent_trades if t.get('actual_outcome', 0) == 1]) if winning_trades else 0,
                        'avg_loss': np.mean([abs(t.get('pnl', 0)) for t in recent_trades if t.get('actual_outcome', 0) == 0]) if total_trades - winning_trades else 0,
                        'profit_factor': sum(t.get('pnl', 0) for t in recent_trades if t.get('pnl', 0) > 0) / 
                                        abs(sum(t.get('pnl', 0) for t in recent_trades if t.get('pnl', 0) < 0)) 
                                        if abs(sum(t.get('pnl', 0) for t in recent_trades if t.get('pnl', 0) < 0)) > 0 else 0
                    }
                # ULTRA_FORCED metrics
                if hasattr(self, 'ultra_forced_patterns'):
                    uf_trades = sum(1 for t in self.trade_history if t.get('setup_type') == 'ULTRA_FORCED')
                    uf_wins = sum(1 for t in self.trade_history if t.get('setup_type') == 'ULTRA_FORCED' and t.get('actual_outcome', 0) == 1)
                    metrics['ultra_forced'] = {
                        'total_trades': uf_trades,
                        'win_rate': uf_wins / uf_trades if uf_trades else 0,
                        'best_rsi_range': self._get_best_ultra_forced_rsi_range()
                    }
                # Model metrics
                metrics['models'] = {model: self._get_model_accuracy(model) for model in self.models}
                # Store metrics
                self.performance_dashboard_data['metrics'] = metrics
                self.performance_dashboard_data['last_updated'] = now
                return self._format_dashboard(metrics)

            def _format_dashboard(metrics):
                """Format dashboard metrics as text"""
                lines = []
                lines.append("\n" + "="*60)
                lines.append("üìä PERFORMANCE DASHBOARD".center(60))
                lines.append("="*60)
                if 'account' in metrics:
                    acct = metrics['account']
                    lines.append("\nüí∞ ACCOUNT METRICS")
                    lines.append(f"Balance: ${acct['balance']:.2f}")
                    lines.append(f"Equity: ${acct['equity']:.2f}")
                    lines.append(f"Margin Level: {acct['margin_level']:.1f}%")
                    lines.append(f"Free Margin: ${acct['free_margin']:.2f}")
                if 'trading' in metrics:
                    trading = metrics['trading']
                    lines.append("\nüìà TRADING METRICS")
                    lines.append(f"Total Trades: {trading['total_trades']}")
                    lines.append(f"Win Rate: {trading['win_rate']:.1%}")
                    lines.append(f"Avg Win: ${trading['avg_win']:.2f}")
                    lines.append(f"Avg Loss: ${trading['avg_loss']:.2f}")
                    lines.append(f"Profit Factor: {trading['profit_factor']:.2f}")
                if 'ultra_forced' in metrics:
                    uf = metrics['ultra_forced']
                    lines.append("\nüéØ ULTRA_FORCED METRICS")
                    lines.append(f"Total ULTRA_FORCED Trades: {uf['total_trades']}")
                    lines.append(f"Win Rate: {uf['win_rate']:.1%}")
                    lines.append(f"Best RSI Range: {uf['best_rsi_range']}")
                if 'models' in metrics:
                    lines.append("\nü§ñ MODEL METRICS")
                    for model, accuracy in metrics['models'].items():
                        lines.append(f"{model}: {accuracy:.1%} accuracy")
                lines.append("\n" + "="*60)
                return "\n".join(lines)

            def _get_model_accuracy(model_name):
                """Get accuracy for a specific model"""
                if not self.trade_history or model_name not in self.models:
                    return 0.0
                # Get trades where this model was the primary decision maker
                model_trades = [t for t in self.trade_history 
                               if t.get('primary_model') == model_name]
                # If none found, check all trades
                if not model_trades:
                    return 0.0
                wins = sum(1 for t in model_trades if t.get('actual_outcome', 0) == 1)
                return wins / len(model_trades) if model_trades else 0

            def _get_best_ultra_forced_rsi_range():
                """Get best performing RSI range for ULTRA_FORCED setups"""
                if not hasattr(self, 'ultra_forced_patterns'):
                    return "Unknown"
                patterns = self.ultra_forced_patterns
                if 'rsi_buckets' not in patterns:
                    return "Unknown"
                # Find bucket with at least 5 trades and highest win rate
                best_bucket = None
                best_win_rate = 0
                for bucket, data in patterns['rsi_buckets'].items():
                    if data['trades'] >= 5 and data['win_rate'] > best_win_rate:
                        best_bucket = bucket
                        best_win_rate = data['win_rate']
                return f"{best_bucket} ({best_win_rate:.1%})" if best_bucket else "Unknown"

            # Attach methods to the class
            self._format_dashboard = _format_dashboard
            self._get_model_accuracy = _get_model_accuracy
            self._get_best_ultra_forced_rsi_range = _get_best_ultra_forced_rsi_range
            self.generate_dashboard = generate_dashboard
        except Exception as e:
            print(f"‚ùå Error creating dashboard: {e}")
    def optimize_ultra_forced_strategy(self):
        """Optimize the ULTRA_FORCED strategy based on performance analysis"""
        # Implement specialized exit strategies for ULTRA_FORCED setups
        def optimize_ultra_forced_exit_strategy(setup_type, win_rate, rsi, symbol):
            """Get optimized exit parameters based on setup performance"""
            # Base parameters
            tp_multiplier = 3.0  # Default TP at 3x SL distance
            sl_atr_multiplier = 1.0  # Default SL at 1x ATR
            trailing_activation = 0.5  # Activate trailing at 50% of TP
            # Adjust based on win rate
            if win_rate > 0.45:  # Higher win rate - can be more aggressive
                tp_multiplier = 4.0
                sl_atr_multiplier = 0.8  # Tighter stop
                trailing_activation = 0.3  # Earlier trailing
            elif win_rate < 0.35:  # Lower win rate - be more conservative
                tp_multiplier = 2.5  # Lower target
                sl_atr_multiplier = 1.2  # Wider stop
                trailing_activation = 0.7  # Later trailing
            # Adjust based on RSI extremes
            if rsi > 85 or rsi < 15:  # Super extreme
                tp_multiplier += 0.5  # Increase target for extreme RSI
                trailing_activation -= 0.1  # Earlier trailing
            # Symbol-specific adjustments
            if 'XAU' in symbol:
                # Gold tends to have larger moves but needs wider stops
                tp_multiplier *= 1.2
                sl_atr_multiplier *= 1.3
            elif 'EUR' in symbol:
                # EURUSD needs tighter management
                tp_multiplier *= 0.9
                sl_atr_multiplier *= 0.8
            return {
                'tp_multiplier': tp_multiplier,
                'sl_atr_multiplier': sl_atr_multiplier,
                'trailing_activation': trailing_activation,
                'take_partial': win_rate > 0.4  # Take partial profits if win rate is good
            }
        # Store the function for later use
        self.optimize_ultra_forced_exit_strategy = optimize_ultra_forced_exit_strategy

        # Adapt the ULTRA_FORCED detection parameters based on market conditions
        def adaptive_ultra_forced_parameters(market_conditions):
            """Adapt ULTRA_FORCED parameters based on market conditions"""
            # Get default thresholds
            thresholds = self.get_optimized_ultra_forced_thresholds().copy()
            # Adjust based on volatility
            volatility = market_conditions.get('volatility_regime', 'normal')
            if volatility == 'high':
                # In high volatility, RSI can go to more extreme levels
                thresholds['oversold_min'] -= 5
                thresholds['oversold_max'] -= 3
                thresholds['overbought_min'] += 3
                thresholds['overbought_max'] += 5
            elif volatility == 'low':
                # In low volatility, smaller RSI moves are significant
                thresholds['oversold_min'] += 3
                thresholds['oversold_max'] += 2
                thresholds['overbought_min'] -= 2
                thresholds['overbought_max'] -= 3
            # Adjust based on trend strength
            trend = market_conditions.get('trend_regime', 'neutral')
            if trend == 'strong_uptrend':
                # In strong uptrend, focus more on oversold conditions
                thresholds['oversold_min'] += 5
                thresholds['oversold_max'] += 3
            elif trend == 'strong_downtrend':
                # In strong downtrend, focus more on overbought conditions
                thresholds['overbought_min'] -= 3
                thresholds['overbought_max'] -= 5
            return thresholds
        # Store the function for later use
        self.adaptive_ultra_forced_parameters = adaptive_ultra_forced_parameters
    def enhance_ml_models(self):
        """Implement enhancements to ML models"""
        # 1. Feature importance analysis
        def analyze_feature_importance():
            """Analyze and store feature importance"""
            if not all(model is not None for model in self.models.values()):
                return
            feature_importance = {}
            for model_name, model in self.models.items():
                if hasattr(model, 'feature_importances_'):
                    # For tree-based models
                    importances = model.feature_importances_
                    for i, importance in enumerate(importances):
                        if i < len(self.feature_columns):
                            feature_name = self.feature_columns[i]
                            if feature_name not in feature_importance:
                                feature_importance[feature_name] = []
                            feature_importance[feature_name].append(importance)
            # Average importance across models
            avg_importance = {}
            for feature, values in feature_importance.items():
                avg_importance[feature] = sum(values) / len(values)
            # Sort by importance
            sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)
            # Store top features
            self.top_features = [f[0] for f in sorted_features[:20]]
            if self.debug_mode:
                print("üìä Top 10 features by importance:")
                for feature, importance in sorted_features[:10]:
                    print(f"   {feature}: {importance:.4f}")
        # Store the function for later use
        self.analyze_feature_importance = analyze_feature_importance

        # 2. Improved online learning
        def enhanced_online_learning(features, target, confidence=None):
            """Enhanced online learning with importance weighting"""
            try:
                with self.model_lock:
                    features_array = np.array(features).reshape(1, -1)
                    target_array = np.array([target])
                    # Weight samples by confidence - give more importance to high confidence predictions
                    sample_weight = None
                    if confidence is not None:
                        # If prediction was wrong with high confidence, give it more weight to learn from
                        if (confidence > 0.7 and target == 0) or (confidence < 0.3 and target == 1):
                            sample_weight = np.array([2.0])  # Double importance for wrong high-confidence predictions
                        else:
                            sample_weight = np.array([1.0])
                    if not self.online_model_initialized:
                        self.online_model.partial_fit(features_array, target_array, classes=np.array([0, 1]))
                        self.online_model_initialized = True
                        print("üß† Online model initialized")
                    else:
                        self.online_model.partial_fit(features_array, target_array, sample_weight=sample_weight)
                    if self.debug_mode:
                        print(f"üîÑ Online model updated with target: {target}" + 
                              (f", weight: {sample_weight[0]}" if sample_weight is not None else ""))
            except Exception as e:
                self.logger.error(f"Enhanced online learning error: {e}")
        # Replace the original method
        self.enhanced_online_learning = enhanced_online_learning
        self.online_partial_fit = enhanced_online_learning
    # 1. Enhanced Market Regime Detection
    def detect_market_regime(self, market_data, lookback=100):
        """Detect current market regime using multiple indicators"""
        volatility = market_data['atr'].rolling(lookback).std() / market_data['atr'].rolling(lookback).mean()
        trend_strength = abs(market_data['close'].rolling(lookback).mean() - market_data['close']) / market_data['atr']
        # Classify market conditions
        if volatility.iloc[-1] > 1.5:
            regime = 'high_volatility'
        elif trend_strength.iloc[-1] > 2.0:
            regime = 'strong_trend'
        elif volatility.iloc[-1] < 0.5:
            regime = 'low_volatility_range'
        else:
            regime = 'normal'
        return regime, {
            'volatility_score': volatility.iloc[-1],
            'trend_strength': trend_strength.iloc[-1],
            'regime': regime
        }

    # 2. Volatility-Adjusted Position Sizing
    def calculate_volatility_adjusted_position(self, symbol, account_balance, risk_percentage, confidence):
        """Calculate position size based on recent volatility"""
        # Get recent ATR for volatility measurement
        market_data = self.get_live_market_data(symbol, 'H1', bars=50)
        if market_data is None or len(market_data) < 20:
            return self._calculate_enhanced_position_size(confidence, mt5.account_info(), symbol)
        # Calculate normalized ATR
        atr = market_data['atr'].mean()
        normalized_atr = atr / market_data['close'].iloc[-1]
        # Adjust risk based on volatility
        volatility_factor = 1.0 / (normalized_atr * 200)  # Scale factor
        volatility_factor = max(0.5, min(2.0, volatility_factor))  # Limit adjustment range
        # Apply to position calculation
        adjusted_risk = risk_percentage * volatility_factor
        adjusted_risk = min(adjusted_risk, risk_percentage * 1.5)  # Cap maximum risk
        dollar_risk = account_balance * adjusted_risk
        return dollar_risk / (atr * 10)  # Convert to position size

    # 3. Adaptive Threshold Management
    def optimize_thresholds_based_on_performance(self):
        """Dynamically adjust thresholds based on recent performance"""
        if len(self.trade_history) < 50:
            return  # Not enough data
        # Analyze last 50 trades
        recent_trades = self.trade_history[-50:]
        win_rate = sum(1 for t in recent_trades if t.get('actual_outcome') == 1) / len(recent_trades)
        # Adjust thresholds based on performance
        for symbol in self.asset_thresholds:
            if isinstance(self.asset_thresholds[symbol], float):
                # Single threshold
                if win_rate < 0.4:
                    # Increase threshold when performance is poor
                    self.asset_thresholds[symbol] = min(0.55, self.asset_thresholds[symbol] + 0.03)
                elif win_rate > 0.6:
                    # Decrease threshold when performance is good
                    self.asset_thresholds[symbol] = max(0.25, self.asset_thresholds[symbol] - 0.02)
                if self.debug_mode:
                    print(f"üéØ Adjusted threshold for {symbol}: {self.asset_thresholds[symbol]:.2f}")
    # Example usage for fixing '.get' on possibly-float objects:
    # Replace: value = obj.get('key', default)
    # With:    value = self._safe_get(obj, 'key', default)
    def _safe_get(self, obj, key, default=None):
        """Safely get value from dict or return float itself if not a dict."""
        if isinstance(obj, dict):
            return obj.get(key, default)
        return obj if isinstance(obj, float) else default
    def start_intelligent_monitoring(self, scan_number=1):
        """Step 3: Starting intelligent monitoring with clear status messages"""
        print("Step 3: Starting intelligent monitoring...")
        print("üîç Enhanced monitoring system started...")
        # Check market session status
        current_session = self._get_current_session()
        if current_session == 'closed':
            print(f"üö´ Trading conditions not met (Scan #{scan_number})")
            return False
        # ...existing code...
    def save_learning_state(self):
        """Save all learning data to database for persistence"""
        try:
            cursor = self.conn.cursor()
            import pickle
            learning_data = {
                'feature_win_rates': getattr(self, 'feature_win_rates', {}),
                'threshold_performance': getattr(self, 'threshold_performance', {}),
                'ultra_forced_performance': getattr(self, 'ultra_forced_patterns', {}),
                'asset_risk_multipliers': getattr(self, 'asset_risk_multipliers', {}),
                'model_recent_performance': getattr(self, 'model_recent_performance', {}),
                'session_performance': getattr(self, 'session_performance', {}),
                'timestamp': datetime.now(),
            }
            blob = pickle.dumps(learning_data)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS enhanced_trades_learning_state (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    saved_at DATETIME,
                    state BLOB
                )''')
            cursor.execute('INSERT INTO enhanced_trades_learning_state (saved_at, state) VALUES (?, ?)',
                           (datetime.now(), blob))
            self.conn.commit()
            if self.debug_mode:
                print("üß† Full learning state saved to enhanced_trades database.")
            return True
        except Exception as e:
            print(f"‚ùå Error saving full learning state: {e}")
            logging.error(f"Error saving full learning state: {e}")
            return False
    def save_learning_state_to_db(self):
        """Save all learning state to the database for persistence"""
        try:
            cursor = self.conn.cursor()
            # Serialize learning state as a binary blob
            import pickle
            learning_state = {
                'asset_thresholds': self.asset_thresholds,
                'confidence_calibration': self.confidence_calibration,
                'threshold_adaptation': self.threshold_adaptation,
                'ultra_forced_patterns': getattr(self, 'ultra_forced_patterns', None),
                'dynamic_rsi_thresholds': getattr(self, 'dynamic_rsi_thresholds', None),
                'adaptive_risk': {
                    'asset_risk_multipliers': getattr(self, 'asset_risk_multipliers', {}),
                    'symbol_performance_history': getattr(self, 'symbol_performance_history', {}),
                    'adaptive_risk_config': getattr(self, 'adaptive_risk_config', {})
                },
                'timestamp': datetime.now(),
            }
            blob = pickle.dumps(learning_state)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_state (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    saved_at DATETIME,
                    state BLOB
                )''')
            cursor.execute('INSERT INTO learning_state (saved_at, state) VALUES (?, ?)',
                           (datetime.now(), blob))
            self.conn.commit()
            if self.debug_mode:
                print("üß† Learning state saved to database.")
            return True
        except Exception as e:
            print(f"‚ùå Error saving learning state to DB: {e}")
            logging.error(f"Error saving learning state to DB: {e}")
            return False

    def load_learning_state_from_db(self):
        """Load the most recent learning state from the database"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT state FROM learning_state ORDER BY saved_at DESC LIMIT 1
            ''')
            row = cursor.fetchone()
            if not row:
                print("‚ÑπÔ∏è No learning state found in database.")
                return False
            import pickle
            learning_state = pickle.loads(row[0])
            # Restore basic state
            self.asset_thresholds = learning_state.get('asset_thresholds', {})
            self.confidence_calibration = learning_state.get('confidence_calibration', {})
            self.threshold_adaptation = learning_state.get('threshold_adaptation', {})
            # Restore ULTRA_FORCED pattern data
            if 'ultra_forced_patterns' in learning_state:
                self.ultra_forced_patterns = learning_state['ultra_forced_patterns']
            # Restore dynamic RSI thresholds
            if 'dynamic_rsi_thresholds' in learning_state:
                self.dynamic_rsi_thresholds = learning_state['dynamic_rsi_thresholds']
            # Restore adaptive risk management data
            if 'adaptive_risk' in learning_state:
                adaptive_data = learning_state['adaptive_risk']
                self.asset_risk_multipliers = adaptive_data.get('asset_risk_multipliers', {})
                self.symbol_performance_history = adaptive_data.get('symbol_performance_history', {})
                self.adaptive_risk_config = adaptive_data.get('adaptive_risk_config', {})
            if self.debug_mode:
                print("üß† Learning state loaded from database.")
            return True
        except Exception as e:
            print(f"‚ùå Error loading learning state from DB: {e}")
            logging.error(f"Error loading learning state from DB: {e}")
            return False
    # TODO: Enhance _execute_enhanced_trade to track trade lifecycle
    # Current: Creates trade but limited completion tracking
    # Enhancement: Store trade data for learning when trade closes
    # Include: All analysis data, features, setup_type, expected_win_rate
    # Expected: Complete trade data available for comprehensive learning
    def _execute_enhanced_trade(self, analysis):
        """Execute trade and store comprehensive trade data for lifecycle tracking"""
        try:
            symbol = analysis.get('symbol', 'Unknown')
            setup_type = analysis.get('setup_type', 'STANDARD')
            features = analysis.get('features', {})
            expected_win_rate = analysis.get('expected_historical_win_rate', None)

            # Generate a unique order ID (simulate or use MT5)
            order_id = f"{symbol}_{int(datetime.now().timestamp())}"

            trade_data = {
                'timestamp': datetime.now(),
                'symbol': symbol,
                'analysis': analysis,  # Include full analysis for learning
                'features': features,
                'setup_type': setup_type,
                'expected_win_rate': expected_win_rate,
                # Add more fields as needed for learning
            }

            # Store in active_trades for lifecycle tracking
            self.active_trades[order_id] = trade_data

            # (Optional) Execute trade via MT5 if available
            if mt5 is not None:
                # Example: Place order logic here
                pass  # Replace with actual MT5 order placement

            if self.debug_mode:
                print(f"üöÄ Trade executed and tracked: {order_id} | {symbol} | {setup_type}")

            return order_id
        except Exception as e:
            print(f"‚ùå Error executing enhanced trade: {e}")
            logging.error(f"Error executing enhanced trade: {e}")
    # TODO: Add trade completion detection to main monitoring loop
    def update_feature_importance_from_trades(self, trade_outcome, features_dict):
        """Learn which features are most predictive of winning trades"""
        try:
            # Initialize feature performance tracking if not exists
            if not hasattr(self, 'feature_win_rates'):
                self.feature_win_rates = {
                    'rsi': {'total': 0, 'wins': 0, 'categories': {}},
                    'atr': {'total': 0, 'wins': 0, 'categories': {}},
                    'volumeratio': {'total': 0, 'wins': 0, 'categories': {}},
                    'trend_strength': {'total': 0, 'wins': 0, 'categories': {}},
                }
                
            # Process each feature and update its win rate statistics
            for feature_name, feature_value in features_dict.items():
                if feature_name not in self.feature_win_rates:
                    self.feature_win_rates[feature_name] = {
                        'total': 0, 
                        'wins': 0,
                        'categories': {}
                    }
                
                # Update overall feature statistics
                self.feature_win_rates[feature_name]['total'] += 1
                if trade_outcome == 1:  # Win
                    self.feature_win_rates[feature_name]['wins'] += 1
                
                # Categorize feature value and update category statistics
                category = self._categorize_feature_value(feature_name, feature_value)
                if category not in self.feature_win_rates[feature_name]['categories']:
                    self.feature_win_rates[feature_name]['categories'][category] = {
                        'total': 0, 'wins': 0
                    }
                    
                cat_stats = self.feature_win_rates[feature_name]['categories'][category]
                cat_stats['total'] += 1
                if trade_outcome == 1:
                    cat_stats['wins'] += 1
            
            # Update ensemble weights based on feature importance
            self._update_ensemble_weights()
            
            if self.debug_mode:
                self._log_feature_importance()
                
        except Exception as e:
            self.logger.error(f"Error updating feature importance: {e}")
    
    def _categorize_feature_value(self, feature_name, value):
        """Categorize feature values into meaningful ranges"""
        if feature_name == 'rsi':
            if value <= 20: return 'extremely_oversold'
            elif value <= 30: return 'oversold'
            elif value <= 45: return 'weak'
            elif value <= 55: return 'neutral'
            elif value <= 70: return 'overbought'
            else: return 'extremely_overbought'
        
        elif feature_name == 'atr':
            if value < 0.0005: return 'very_low'
            elif value < 0.001: return 'low'
            elif value < 0.002: return 'medium'
            elif value < 0.003: return 'high'
            else: return 'very_high'
        
        elif feature_name == 'volumeratio':
            if value < 0.5: return 'very_low'
            elif value < 0.8: return 'low'
            elif value < 1.2: return 'normal'
            elif value < 2.0: return 'high'
            else: return 'very_high'
        
        elif feature_name == 'trend_strength':
            if value < 0.2: return 'very_weak'
            elif value < 0.4: return 'weak'
            elif value < 0.6: return 'moderate'
            elif value < 0.8: return 'strong'
            else: return 'very_strong'
        
        return 'default'  # Default category for unknown features
    
    def _update_ensemble_weights(self):
        """Update ensemble model weights based on feature importance and performance"""
        try:
            if not hasattr(self, 'model_weights'):
                # Initialize with equal weights
                self.model_weights = {
                    'timeframe_ensemble': 0.25,
                    'gold_specialist': 0.25,
                    'adaptive_learner': 0.25,
                    'EURUSDm_specialist': 0.25
                }
                
            # Update weights based on recent performance if available
            if hasattr(self, 'model_recent_performance'):
                performance_weights = {}
                total_accuracy = 0
                
                # Calculate accuracy-based weights
                for model_name, stats in self.model_recent_performance.items():
                    if stats['total'] > 0:
                        accuracy = stats['correct'] / stats['total']
                        # Apply smoothing to prevent extreme weight changes
                        performance_weights[model_name] = max(0.1, accuracy)
                        total_accuracy += performance_weights[model_name]
                
                if total_accuracy > 0:
                    # Normalize performance weights
                    performance_weights = {
                        m: w/total_accuracy for m, w in performance_weights.items()
                    }
                    
                    # Combine with feature importance weights (70% performance, 30% feature importance)
                    for model_name in self.model_weights:
                        if model_name in performance_weights:
                            self.model_weights[model_name] = (
                                self.model_weights[model_name] * 0.3 + 
                                performance_weights[model_name] * 0.7
                            )
            
            # Calculate feature importance scores
            feature_scores = {}
            for feature_name, stats in self.feature_win_rates.items():
                if stats['total'] >= 20:  # Minimum sample size
                    overall_win_rate = stats['wins'] / stats['total']
                    
                    # Calculate category-specific win rates
                    category_win_rates = []
                    for cat_stats in stats['categories'].values():
                        if cat_stats['total'] >= 5:  # Minimum category samples
                            cat_win_rate = cat_stats['wins'] / cat_stats['total']
                            category_win_rates.append(cat_win_rate)
                    
                    # Feature score combines overall win rate and category predictiveness
                    if category_win_rates:
                        best_category_win_rate = max(category_win_rates)
                        feature_scores[feature_name] = (overall_win_rate * 0.4 + 
                                                      best_category_win_rate * 0.6)
            
            if not feature_scores:
                return
                
            # Normalize feature importance scores
            total_score = sum(feature_scores.values())
            normalized_scores = {f: s/total_score for f, s in feature_scores.items()}
            
            # Update model weights based on their feature usage
            new_weights = self.model_weights.copy()
            for model_name, model_info in self.models.items():
                if hasattr(model_info, 'feature_importances_'):
                    model_score = 0
                    for feature_idx, importance in enumerate(model_info.feature_importances_):
                        feature_name = self.feature_names[feature_idx]
                        if feature_name in normalized_scores:
                            model_score += importance * normalized_scores[feature_name]
                    
                    # Adjust model weight (with dampening to prevent rapid changes)
                    old_weight = self.model_weights[model_name]
                    new_weights[model_name] = old_weight * 0.7 + model_score * 0.3
            
            # Normalize new weights
            total_weight = sum(new_weights.values())
            self.model_weights = {m: w/total_weight for m, w in new_weights.items()}
            
        except Exception as e:
            self.logger.error(f"Error updating ensemble weights: {e}")
    
    def _log_feature_importance(self):
        """Log current feature importance statistics"""
        try:
            log_msg = "\n=== FEATURE IMPORTANCE UPDATE ===\n"
            for feature_name, stats in self.feature_win_rates.items():
                if stats['total'] >= 20:
                    win_rate = stats['wins'] / stats['total']
                    log_msg += f"\n{feature_name}:\n"
                    log_msg += f"  Overall: {win_rate:.2%} ({stats['wins']}/{stats['total']})\n"
                    log_msg += "  Categories:\n"
                    
                    for category, cat_stats in stats['categories'].items():
                        if cat_stats['total'] >= 5:
                            cat_win_rate = cat_stats['wins'] / cat_stats['total']
                            log_msg += f"    {category}: {cat_win_rate:.2%} "
                            log_msg += f"({cat_stats['wins']}/{cat_stats['total']})\n"
            
            self.logger.info(log_msg)
            
        except Exception as e:
            self.logger.error(f"Error logging feature importance: {e}")
            
    # Current: System executes trades but may not track completions automatically
    # Enhancement: Check for completed trades and call learning methods
    # Monitor: MT5 position closures, trade history updates
    # Expected: Automatic learning from every completed trade
    def check_completed_trades(self):
        """Check for recently completed trades and trigger learning"""
        # --- Best Practice: Update DB and learning for every closed trade ---
        completed_trades = self.get_completed_trades()  # Should be a list of dicts with trade info
        for trade in completed_trades:
            trade_id = trade.get('trade_id')
            exit_price = trade.get('exit_price')
            pnl = trade.get('pnl')
            pnl_pips = trade.get('pnl_pips')
            exit_reason = trade.get('exit_reason')
            entry_time = trade.get('entry_time')
            exit_time = trade.get('exit_time')
            symbol = trade.get('symbol')
            direction = trade.get('direction')
            features = trade.get('features')  # full feature vector at entry
            confidence = trade.get('confidence')
            # Log to DB
            try:
                update_trade_on_close(trade_id, exit_price, pnl, pnl_pips, exit_reason)
            except Exception as e:
                print(f"‚ùå Error logging closed trade {trade_id}: {e}")
            # Log to learning system
            if hasattr(self, 'on_enhanced_trade_outcome'):
                outcome = 1 if pnl > 0 else 0
                exit_details = {
                    'exit_price': exit_price,
                    'exit_reason': exit_reason,
                    'duration_minutes': (exit_time - entry_time).total_seconds() / 60 if entry_time and exit_time else None,
                    'max_favorable_excursion': trade.get('max_favorable_excursion'),
                    'max_adverse_excursion': trade.get('max_adverse_excursion'),
                    'exit_time': exit_time,
                    'entry_time': entry_time,
                    'symbol': symbol,
                    'direction': direction,
                    'confidence': confidence,
                    # ...add more as needed...
                }
                self.on_enhanced_trade_outcome(trade, outcome, exit_details)
            if hasattr(self, 'logger'):
                self.logger.info(f"Trade closed: {trade_id}, Symbol: {symbol}, Dir: {direction}, PnL: {pnl}, Pips: {pnl_pips}, Reason: {exit_reason}, Entry: {entry_time}, Exit: {exit_time}, Confidence: {confidence}")
        # --- End Best Practice ---
        # ...rest of your existing logic...
        try:
            if mt5 is None:
                print("‚ö†Ô∏è MT5 not available, cannot check completed trades.")
                return

            # Get all closed positions from MT5
            closed_positions = mt5.history_deals_get(
                datetime.now() - timedelta(days=2), datetime.now()
            )
            if closed_positions is None:
                print("‚ö†Ô∏è No closed positions found.")
                return

            # Build a set of closed order IDs
            closed_order_ids = set(pos.order for pos in closed_positions)

            # Find trades in active_trades that are now closed
            completed_trades = []
            for order_id, trade_data in list(self.active_trades.items()):
                if order_id in closed_order_ids:
                    completed_trades.append((order_id, trade_data))

            # For each completed trade, call learning and update history
            for order_id, trade_data in completed_trades:
                # Get actual outcome (e.g., profit/loss)
                actual_outcome = 1 if trade_data.get('profit_loss', 0) > 0 else 0
                self.on_trade_close(trade_data, actual_outcome)
                # Remove from active trades
                del self.active_trades[order_id]
                # Add to trade history (already handled in on_trade_close)

            if self.debug_mode and completed_trades:
                print(f"üîÑ {len(completed_trades)} trades completed and processed for learning.")

        except Exception as e:
            print(f"‚ùå Error checking completed trades: {e}")
            logging.error(f"Error checking completed trades: {e}")
    def _calculate_enhanced_position_size(self, confidence, account_info, symbol):
        """Calculate position size optimized for small accounts"""
        try:
            # Base lot size for $50 accounts
            base_lot_size = 0.01  # 0.01 lots (micro lots)

            # Scale position size based on account balance
            balance_scale = min(1.0, account_info.balance / 100.0)
            scaled_lot_size = base_lot_size * balance_scale

            # Confidence-based adjustment
            confidence_multiplier = min(1.5, max(0.5, confidence / 0.5))
            adjusted_lot_size = scaled_lot_size * confidence_multiplier

            # Ensure within symbol limits
            symbol_info = mt5.symbol_info(symbol)
            if symbol_info:
                min_lot = symbol_info.volume_min
                max_lot = symbol_info.volume_max
                step = symbol_info.volume_step

                # Round to step size
                adjusted_lot_size = round(adjusted_lot_size / step) * step

                # Apply limits
                adjusted_lot_size = max(min_lot, min(max_lot, adjusted_lot_size))

            return adjusted_lot_size

        except Exception as e:
            print(f"‚ùå Position size calculation error: {e}")
            return 0.01  # Default to minimum 0.01 lots

    def _get_dynamic_threshold(self, symbol, timeframe):
        """Get dynamic confidence threshold optimized for small accounts"""
        # Use advanced threshold learning if available
        if hasattr(self, 'threshold_performance'):
            market_conditions = {
                'rsi': self.current_market_state.get('rsi', 50),
                'volatility': 'high' if self.current_market_state.get('high_volatility', False) else 'normal'
            }
            return self.get_optimized_threshold(symbol, timeframe, market_conditions)
            
        # Fall back to basic threshold adjustment
        if isinstance(self.confidence_threshold, dict):
            base_threshold = self.confidence_threshold.get(symbol, 0.3)
        else:
            base_threshold = self.confidence_threshold
        
        # Performance-based adjustment
        if len(self.trade_history) >= 10:
            recent_trades = self.trade_history[-20:]
            recent_accuracy = sum(1 for t in recent_trades if t.get('actual_outcome') == 1) / len(recent_trades)

            if recent_accuracy > 0.7:
                base_threshold -= 0.03  # Slightly more aggressive for small accounts
            elif recent_accuracy < 0.4: 
                base_threshold += 0.15  # More conservative for small accounts
        
        # Asset-specific adjustments  
        if 'XAU' in symbol:
            base_threshold += 0.05  # More conservative for gold
        elif 'BTC' in symbol:   
            base_threshold += 0.10  # Very conservative for crypto

        # Timeframe adjustments
        if timeframe == 'M5':
            base_threshold += 0.05  # More conservative for lower timeframes 
        elif timeframe == 'H1':
            base_threshold -= 0.01  # Slightly more aggressive for higher timeframes

        # Ensure reasonable bounds  
        return max(0.30, min(0.55, base_threshold))  # Max threshold capped at 0.55

    def advanced_threshold_learning(self, symbol, timeframe, market_conditions):
        """Learn optimal confidence thresholds based on market conditions and performance"""
        try:
            # Initialize threshold performance tracking if not exists
            if not hasattr(self, 'threshold_performance'):
                self.threshold_performance = {
                    'conditions': {},  # Performance by market condition
                    'symbols': {},     # Symbol-specific thresholds
                    'timeframes': {},  # Timeframe-specific thresholds
                    'updates': 0,      # Track number of updates
                    'last_optimization': datetime.now()
                }
            
            # Get current market context
            volatility = market_conditions.get('volatility', 'normal')
            rsi_regime = self._get_rsi_regime(market_conditions.get('rsi', 50))
            session = self._get_current_session()
            
            # Create condition key for this specific market state
            condition_key = f"{symbol}_{timeframe}_{volatility}_{rsi_regime}_{session}"
            
            # Initialize condition tracking if new
            if condition_key not in self.threshold_performance['conditions']:
                self.threshold_performance['conditions'][condition_key] = {
                    'thresholds': {t/100: {'trades': 0, 'wins': 0} for t in range(30, 56, 5)},
                    'optimal_threshold': 0.5,
                    'total_trades': 0,
                    'successful_trades': 0,
                    'last_updated': datetime.now()
                }
            
            condition_data = self.threshold_performance['conditions'][condition_key]
            
            # Get recent trades for this condition
            recent_trades = [
                t for t in self.trade_history[-100:]
                if (t.get('symbol') == symbol and 
                    t.get('timeframe') == timeframe and
                    t.get('market_conditions', {}).get('volatility') == volatility and
                    self._get_rsi_regime(t.get('market_conditions', {}).get('rsi', 50)) == rsi_regime and
                    t.get('session') == session)
            ]
            
            # Update threshold performance statistics
            for trade in recent_trades:
                confidence = trade.get('confidence', 0.5)
                outcome = trade.get('actual_outcome', 0)
                
                # Find closest threshold bucket
                threshold_bucket = round(confidence * 20) / 20  # Round to nearest 0.05
                threshold_bucket = max(0.3, min(0.55, threshold_bucket))  # Keep within bounds
                
                if threshold_bucket in condition_data['thresholds']:
                    condition_data['thresholds'][threshold_bucket]['trades'] += 1
                    if outcome == 1:  # Win
                        condition_data['thresholds'][threshold_bucket]['wins'] += 1
            
            # Optimize thresholds if enough data
            if len(recent_trades) >= 20:
                best_threshold = None
                best_win_rate = 0.0
                
                for threshold, stats in condition_data['thresholds'].items():
                    if stats['trades'] >= 5:  # Minimum sample size
                        win_rate = stats['wins'] / stats['trades']
                        if win_rate > best_win_rate:
                            best_win_rate = win_rate
                            best_threshold = threshold
                
                if best_threshold is not None:
                    # Update optimal threshold with smoothing
                    old_threshold = condition_data['optimal_threshold']
                    new_threshold = old_threshold * 0.7 + best_threshold * 0.3
                    condition_data['optimal_threshold'] = new_threshold
                    
                    if self.debug_mode:
                        print(f"üéØ Optimized threshold for {condition_key}: {new_threshold:.2f} "
                              f"(win rate: {best_win_rate:.1%})")
            
            # Update symbol-specific thresholds
            if symbol not in self.threshold_performance['symbols']:
                self.threshold_performance['symbols'][symbol] = {'base_threshold': 0.5}
            
            # Update timeframe-specific thresholds
            if timeframe not in self.threshold_performance['timeframes']:
                self.threshold_performance['timeframes'][timeframe] = {'adjustment': 0.0}
            
            self.threshold_performance['updates'] += 1
            
            return condition_data['optimal_threshold']
            
        except Exception as e:
            self.logger.error(f"Error in advanced threshold learning: {e}")
            return 0.5  # Return default threshold on error
        
    def _classify_market_regime(self, features):
        """Classify current market regime based on multiple indicators"""
        try:
            # Volatility regime
            atr = features.get('atr', 0)
            high_volatility = features.get('high_volatility', False)
            volatility_regime = 'high' if high_volatility or atr > 0.003 else ('medium' if atr > 0.001 else 'low')
            
            # Volume regime
            volume_ratio = features.get('volumeratio', 1.0)
            volume_regime = 'high' if volume_ratio > 1.5 else ('medium' if volume_ratio > 0.8 else 'low')
            
            # Trend regime (using RSI and trend strength)
            rsi = features.get('rsi', 50)
            trend_strength = features.get('trend_strength', 0.5)
            
            if trend_strength > 0.7:
                trend_regime = 'strong_trend'
            elif trend_strength > 0.4:
                trend_regime = 'weak_trend'
            else:
                trend_regime = 'ranging'
                
            # Combine regimes into a market state
            market_state = f"{volatility_regime}_{volume_regime}_{trend_regime}"
            
            return {
                'market_state': market_state,
                'volatility_regime': volatility_regime,
                'volume_regime': volume_regime,
                'trend_regime': trend_regime,
                'rsi_zone': self._get_rsi_regime(rsi)
            }
            
        except Exception as e:
            self.logger.error(f"Error classifying market regime: {e}")
            return None

    def _get_rsi_regime(self, rsi_value):
        """Classify RSI value into regime"""
        if rsi_value <= 30:
            return 'oversold'
        elif rsi_value >= 70:
            return 'overbought'
        elif 45 <= rsi_value <= 55:
            return 'neutral'
        elif rsi_value < 45:
            return 'weak'
        else:
            return 'strong'

    def _get_current_session(self):
        """Determine current session and market status"""
        current_time = datetime.now()
        hour = current_time.hour
        weekday = current_time.weekday()
        
        # Check if market is closed (weekend)
        if weekday >= 5:  # Saturday = 5, Sunday = 6
            next_open = current_time + timedelta(days=(7-weekday))
            next_open = next_open.replace(hour=7, minute=0, second=0, microsecond=0)
            print(f"üö´ Market closed for weekend. Market opens at {next_open.strftime('%Y-%m-%d %H:%M')} GMT")
            return 'closed'
            
        # Check daily session times
        if 0 <= hour < 7:  # Pre-London
            next_open = current_time.replace(hour=7, minute=0, second=0, microsecond=0)
            print(f"üö´ Market closed. London session opens at {next_open.strftime('%H:%M')} GMT")
            return 'closed'
        elif 7 <= hour < 16:  # 7:00-15:59 London
            print("üè¶ London session active")
            return 'london'
        elif 13 <= hour < 22:  # 13:00-21:59 New York
            print("üóΩ New York session active")
            return 'newyork'
        elif 22 <= hour <= 23:  # After NY close
            next_open = (current_time + timedelta(days=1)).replace(hour=7, minute=0, second=0, microsecond=0)
            if next_open.weekday() >= 5:  # If tomorrow is weekend
                next_open += timedelta(days=(7-next_open.weekday()))
            print(f"üö´ Market closed for the day. Next session opens at {next_open.strftime('%Y-%m-%d %H:%M')} GMT")
            return 'closed'
        else:
            print("üåè Asian session active")
            return 'asian'

    def get_optimized_threshold(self, symbol, timeframe, market_conditions):
        """Get optimized threshold for current market conditions"""
        try:
            if not hasattr(self, 'threshold_performance'):
                return self.confidence_threshold  # Return default if no learning data
            
            # Get condition-specific threshold
            volatility = market_conditions.get('volatility', 'normal')
            rsi_regime = self._get_rsi_regime(market_conditions.get('rsi', 50))
            session = self._get_current_session()
            
            condition_key = f"{symbol}_{timeframe}_{volatility}_{rsi_regime}_{session}"
            
            if condition_key in self.threshold_performance['conditions']:
                condition_threshold = self.threshold_performance['conditions'][condition_key]['optimal_threshold']
            else:
                condition_threshold = self.confidence_threshold
            
            # Apply symbol-specific adjustment
            symbol_adj = self.threshold_performance['symbols'].get(symbol, {}).get('base_threshold', 0.0)
            
            # Apply timeframe adjustment
            timeframe_adj = self.threshold_performance['timeframes'].get(timeframe, {}).get('adjustment', 0.0)
            
            # Combine adjustments
            final_threshold = condition_threshold + symbol_adj + timeframe_adj
            
            # Ensure reasonable bounds
            return max(0.3, min(0.55, final_threshold))
            
        except Exception as e:
            self.logger.error(f"Error getting optimized threshold: {e}")
            return self.confidence_threshold

    def check_system_status(self):
        """Check overall system status and market conditions"""
        print("\nüîç Enhanced monitoring system started...")
        
        # Check market session status
        current_session = self._get_current_session()
        if current_session == 'closed':
            print("üö´ Trading conditions not met (Market closed)")
            return False
            
        # Check if MT5 is connected
        if mt5 is None or not mt5.initialize():
            print("üö´ Trading conditions not met (MT5 not connected)")
            return False
            
        # Additional system checks can be added here
        
        print("‚úÖ System ready for trading")
        return True

    def _apply_trading_filters(self, features, symbol, timeframe):
        """Apply adaptive trading filters based on system learning state and market regime"""
        filters_passed = True
        filter_reasons = []

        if features is None:
            return False, ['No features available']
            
        # Get current market regime and optimal parameters
        regime_info = self._classify_market_regime(features)
        if regime_info and hasattr(self, 'market_regime_performance'):
            market_state = regime_info['market_state']
            if market_state in self.market_regime_performance['regimes']:
                regime_params = self.market_regime_performance['regimes'][market_state]['optimal_parameters']
            else:
                regime_params = None
        else:
            regime_params = None
        
        # Get total trades for learning progress assessment
        total_trades = len(getattr(self, 'trade_history', []))
        learning_phase = total_trades < 100  # First 100 trades are considered learning phase
        
        # Adaptive thresholds based on learning progress
        atr_val = features.get('atr', 0)
        vol_ratio = features.get('volumeratio', 1)
        rsi = features.get('rsi', 50)

        # Dynamic threshold adjustment based on learning phase
        if learning_phase:
            # More permissive thresholds during learning
            extreme_vol_threshold = 0.005  # Higher ATR tolerance
            low_vol_threshold = 0.6    # Lower volume requirement
            rsi_extreme_min = 15       # Wider RSI bounds
            rsi_extreme_max = 45
        else:
            # Gradually tighten thresholds based on performance
            performance = self.get_recent_performance()
            
            # Adjust thresholds based on win rate
            win_rate = performance.get('win_rate', 0.5)
            if win_rate >= 0.55:  # System is performing well, can be more selective
                extreme_vol_threshold = 0.003
                low_vol_threshold = 0.8
                rsi_extreme_min = 20
                rsi_extreme_max = 80
            else:  # System needs more optimization, stay moderate
                extreme_vol_threshold = 0.004
                low_vol_threshold = 0.7
                rsi_extreme_min = 18
                rsi_extreme_max = 82

        # Volatility filter with adaptive threshold
        if features.get('high_volatility', 0) and atr_val > extreme_vol_threshold:
            if not learning_phase:  # Only filter volatility after learning phase
                filters_passed = False    
                filter_reasons.append(f'Extreme volatility: {atr_val:.4f} > {extreme_vol_threshold:.4f}')
        
        # Adaptive RSI filter
        if not learning_phase and (rsi > rsi_extreme_max or rsi < rsi_extreme_min):
            filters_passed = False
            filter_reasons.append(f'RSI extreme levels: {rsi}')

        # Adaptive volume filter   
        if not learning_phase and features.get('low_volume', 0) and vol_ratio < low_vol_threshold:
            filters_passed = False
            filter_reasons.append(f'Low volume: {vol_ratio:.2f} < {low_vol_threshold}')

        # Selective news filter - only avoid highest impact news times
        current_hour = datetime.now().hour
        if not learning_phase and current_hour == 15:  # Major news hour (e.g., NFP release time)
            filters_passed = False
            filter_reasons.append('Major high-impact news window')

        if self.debug_mode and not filters_passed:
            print(f"üîç Trade filtered out: {', '.join(filter_reasons)}")
            if learning_phase:
                print("üìö System in learning phase - filters are more permissive")
        
        return filters_passed, filter_reasons

    def get_recent_performance(self, lookback=50):
        """Calculate recent system performance metrics"""
        try:
            if not hasattr(self, 'trade_history') or len(self.trade_history) == 0:
                return {'win_rate': 0.5, 'avg_profit': 0.0, 'trades': 0}
            
            # Get recent trades
            recent_trades = self.trade_history[-lookback:]
            total_trades = len(recent_trades)
            
            if total_trades == 0:
                return {'win_rate': 0.5, 'avg_profit': 0.0, 'trades': 0}
            
            # Calculate metrics
            winning_trades = sum(1 for t in recent_trades if t.get('actual_outcome', 0) == 1)
            win_rate = winning_trades / total_trades
            
            # Calculate average profit
            profits = [t.get('profit_loss', 0) for t in recent_trades]
            avg_profit = sum(profits) / total_trades if profits else 0
            
            return {
                'win_rate': win_rate,
                'avg_profit': avg_profit,
                'trades': total_trades,
                'consecutive_losses': self._get_consecutive_losses()
            }
            
        except Exception as e:
            self.logger.error(f"Error calculating recent performance: {e}")
            return {'win_rate': 0.5, 'avg_profit': 0.0, 'trades': 0}
    
    def _get_consecutive_losses(self):
        """Calculate current consecutive loss streak"""
        losses = 0
        for trade in reversed(self.trade_history):
            if trade.get('actual_outcome', 0) == 0:
                losses += 1
            else:
                break
        return losses

    def update_ensemble_weights_from_performance(self, trade_data, actual_outcome):
        """Dynamically adjust ensemble model weights based on recent prediction accuracy"""
        try:
            # Initialize performance tracking if not exists
            if not hasattr(self, 'model_recent_performance'):
                self.model_recent_performance = {
                    'timeframe_ensemble': {'correct': 0, 'total': 0},
                    'gold_specialist': {'correct': 0, 'total': 0},
                    'adaptive_learner': {'correct': 0, 'total': 0},
                    'EURUSDm_specialist': {'correct': 0, 'total': 0}
                }
            
            # Get model predictions from trade data
            predictions = trade_data.get('model_predictions', {})
            
            # Update performance stats for each model
            for model_name, pred_data in predictions.items():
                confidence = pred_data.get('confidence', 0.0)
                # Only count high confidence predictions (confidence > 0.7)
                if confidence > 0.7:
                    self.model_recent_performance[model_name]['total'] += 1
                    if actual_outcome == 1:  # Win
                        self.model_recent_performance[model_name]['correct'] += 1
            
            # Maintain rolling window of last 50 trades
            for model_stats in self.model_recent_performance.values():
                if model_stats['total'] > 50:
                    reduction_factor = 50 / model_stats['total']
                    model_stats['total'] = 50
                    model_stats['correct'] = int(model_stats['correct'] * reduction_factor)
            
            # Update ensemble weights based on performance
            self._update_ensemble_weights()
            
            if self.debug_mode:
                self._log_model_performance()
                
        except Exception as e:
            self.logger.error(f"Error updating ensemble weights from performance: {e}")
    
    def _log_model_performance(self):
        """Log current model performance statistics"""
        try:
            log_msg = "\n=== MODEL PERFORMANCE UPDATE ===\n"
            for model_name, stats in self.model_recent_performance.items():
                if stats['total'] > 0:
                    accuracy = stats['correct'] / stats['total']
                    weight = self.model_weights.get(model_name, 0.0)
                    log_msg += f"\n{model_name}:\n"
                    log_msg += f"  Accuracy: {accuracy:.2%} ({stats['correct']}/{stats['total']})\n"
                    log_msg += f"  Current Weight: {weight:.3f}\n"
            
            self.logger.info(log_msg)
            
        except Exception as e:
            self.logger.error(f"Error logging model performance: {e}")
    
    def learn_market_regime_performance(self, trade_data, outcome):
        """Learn which strategies work best in different market regimes"""
        try:
            # Initialize market regime tracking if not exists
            if not hasattr(self, 'market_regime_performance'):
                self.market_regime_performance = {
                    'regimes': {},  # Performance by regime combination
                    'strategy_adjustments': {},  # Learned parameter adjustments
                    'setup_performance': {},  # Setup type performance by regime
                    'total_regime_trades': 0,
                    'last_optimization': datetime.now()
                }
            
            # Extract features and classify regime
            features = trade_data.get('analysis', {}).get('features', {})
            regime_info = self._classify_market_regime(features)
            
            if regime_info is None:
                return
                
            market_state = regime_info['market_state']
            setup_type = trade_data.get('setup_type', 'STANDARD')
            
            # Initialize regime tracking if new
            if market_state not in self.market_regime_performance['regimes']:
                self.market_regime_performance['regimes'][market_state] = {
                    'total_trades': 0,
                    'winning_trades': 0,
                    'setup_performance': {},
                    'optimal_parameters': {
                        'rsi_thresholds': {'oversold': 30, 'overbought': 70},
                        'volume_requirement': 0.8,
                        'trend_strength_requirement': 0.5,
                        'confidence_threshold': 0.5
                    }
                }
            
            # Update regime performance
            regime_stats = self.market_regime_performance['regimes'][market_state]
            regime_stats['total_trades'] += 1
            if outcome == 1:
                regime_stats['winning_trades'] += 1
            
            # Update setup performance in this regime
            if setup_type not in regime_stats['setup_performance']:
                regime_stats['setup_performance'][setup_type] = {'trades': 0, 'wins': 0}
            
            setup_stats = regime_stats['setup_performance'][setup_type]
            setup_stats['trades'] += 1
            setup_stats['wins'] += outcome
            
            # Optimize strategy parameters if enough data
            if regime_stats['total_trades'] >= 20:
                self._optimize_regime_parameters(market_state, regime_info)
            
            self.market_regime_performance['total_regime_trades'] += 1
            
            if self.debug_mode:
                self._log_regime_performance(market_state)
                
        except Exception as e:
            self.logger.error(f"Error in market regime learning: {e}")
    
    def _optimize_regime_parameters(self, market_state, regime_info):
        """Optimize strategy parameters for specific market regime"""
        try:
            regime_stats = self.market_regime_performance['regimes'][market_state]
            setup_performance = regime_stats['setup_performance']
            
            # Adjust parameters based on regime characteristics
            params = regime_stats['optimal_parameters']
            
            # Volatility-based adjustments
            if regime_info['volatility_regime'] == 'high':
                params['rsi_thresholds']['oversold'] = 25  # More extreme for high volatility
                params['rsi_thresholds']['overbought'] = 75
                params['confidence_threshold'] = 0.6  # Require higher confidence
            else:
                params['rsi_thresholds']['oversold'] = 30
                params['rsi_thresholds']['overbought'] = 70
                params['confidence_threshold'] = 0.5
            
            # Volume-based adjustments
            if regime_info['volume_regime'] == 'low':
                params['volume_requirement'] = 0.6  # More lenient in low volume
            else:
                params['volume_requirement'] = 0.8
            
            # Trend-based adjustments
            if regime_info['trend_regime'] == 'strong_trend':
                params['trend_strength_requirement'] = 0.7
            else:
                params['trend_strength_requirement'] = 0.5
            
            # Find best performing setup types in this regime
            for setup_type, stats in setup_performance.items():
                if stats['trades'] >= 10:
                    win_rate = stats['wins'] / stats['trades']
                    if win_rate > 0.6:  # High performing setup
                        if setup_type == 'ULTRA_FORCED':
                            # Make ULTRA_FORCED more aggressive in good regimes
                            params['rsi_thresholds']['oversold'] += 5
                            params['rsi_thresholds']['overbought'] -= 5
            
            # Store optimized parameters
            regime_stats['optimal_parameters'] = params
            
        except Exception as e:
            self.logger.error(f"Error optimizing regime parameters: {e}")
    
    def _log_regime_performance(self, market_state):
        """Log performance statistics for current market regime"""
        try:
            regime_stats = self.market_regime_performance['regimes'][market_state]
            win_rate = regime_stats['winning_trades'] / regime_stats['total_trades']
            
            log_msg = f"\n=== MARKET REGIME PERFORMANCE: {market_state} ===\n"
            log_msg += f"Win Rate: {win_rate:.2%} ({regime_stats['winning_trades']}/{regime_stats['total_trades']})\n"
            log_msg += "Setup Performance:\n"
            
            for setup_type, stats in regime_stats['setup_performance'].items():
                if stats['trades'] > 0:
                    setup_win_rate = stats['wins'] / stats['trades']
                    log_msg += f"  {setup_type}: {setup_win_rate:.2%} ({stats['wins']}/{stats['trades']})\n"
            
            log_msg += "\nOptimal Parameters:\n"
            for param, value in regime_stats['optimal_parameters'].items():
                log_msg += f"  {param}: {value}\n"
            
            self.logger.info(log_msg)
            
        except Exception as e:
            self.logger.error(f"Error logging regime performance: {e}")
    
    def learn_session_performance(self, trade_data, outcome):
        """Learn optimal trading sessions for ULTRA_FORCED setups"""
        try:
            # Initialize session performance tracking if not exists
            if not hasattr(self, 'session_performance'):
                self.session_performance = {
                    'sessions': {
                        'london': {'trades': 0, 'wins': 0, 'setups': {}},
                        'newyork': {'trades': 0, 'wins': 0, 'setups': {}},
                        'asian': {'trades': 0, 'wins': 0, 'setups': {}}
                    },
                    'hourly': {str(h): {'trades': 0, 'wins': 0, 'setups': {}} for h in range(24)},
                    'session_thresholds': {
                        'london': 0.5,
                        'newyork': 0.5,
                        'asian': 0.5
                    },
                    'optimal_hours': set(),
                    'total_trades': 0,
                    'last_optimization': datetime.now()
                }
            
            # Get session info
            session = self._get_current_session()
            hour = trade_data['timestamp'].hour
            setup_type = trade_data.get('setup_type', 'UNKNOWN')
            
            # Update session statistics
            session_stats = self.session_performance['sessions'][session]
            session_stats['trades'] += 1
            if outcome == 1:
                session_stats['wins'] += 1
            
            # Update setup-specific performance for session
            if setup_type not in session_stats['setups']:
                session_stats['setups'][setup_type] = {'trades': 0, 'wins': 0}
            setup_stats = session_stats['setups'][setup_type]
            setup_stats['trades'] += 1
            setup_stats['wins'] += outcome
            
            # Update hourly statistics
            hour_stats = self.session_performance['hourly'][str(hour)]
            hour_stats['trades'] += 1
            if outcome == 1:
                hour_stats['wins'] += 1
            
            # Update setup-specific performance for hour
            if setup_type not in hour_stats['setups']:
                hour_stats['setups'][setup_type] = {'trades': 0, 'wins': 0}
            hour_setup_stats = hour_stats['setups'][setup_type]
            hour_setup_stats['trades'] += 1
            hour_setup_stats['wins'] += outcome
            
            self.session_performance['total_trades'] += 1
            
            # Optimize session thresholds if enough data
            if self.session_performance['total_trades'] % 50 == 0:
                self._optimize_session_parameters()
            
            if self.debug_mode:
                self._log_session_performance()
                
        except Exception as e:
            self.logger.error(f"Error in session performance learning: {e}")
    
    def _optimize_session_parameters(self):
        """Optimize trading parameters based on session performance"""
        try:
            # Update session-specific confidence thresholds
            for session, stats in self.session_performance['sessions'].items():
                if stats['trades'] >= 20:
                    win_rate = stats['wins'] / stats['trades']
                    # Adjust threshold based on session performance
                    if win_rate > 0.6:
                        self.session_performance['session_thresholds'][session] = max(0.4, min(0.6, win_rate - 0.1))
                    else:
                        self.session_performance['session_thresholds'][session] = max(0.5, min(0.7, 0.8 - win_rate))
            
            # Identify optimal trading hours
            optimal_hours = set()
            for hour, stats in self.session_performance['hourly'].items():
                if stats['trades'] >= 10:
                    hour_win_rate = stats['wins'] / stats['trades']
                    if hour_win_rate > 0.55:  # Consider hours with >55% win rate as optimal
                        optimal_hours.add(int(hour))
            
            self.session_performance['optimal_hours'] = optimal_hours
            self.session_performance['last_optimization'] = datetime.now()
            
        except Exception as e:
            self.logger.error(f"Error optimizing session parameters: {e}")
    
    def _log_session_performance(self):
        """Log session and hourly performance statistics"""
        try:
            log_msg = "\n=== SESSION PERFORMANCE UPDATE ===\n"
            
            # Log session performance
            for session, stats in self.session_performance['sessions'].items():
                if stats['trades'] > 0:
                    win_rate = stats['wins'] / stats['trades']
                    log_msg += f"\n{session.upper()} Session:"
                    log_msg += f"\n  Overall: {win_rate:.2%} ({stats['wins']}/{stats['trades']})"
                    log_msg += f"\n  Threshold: {self.session_performance['session_thresholds'][session]:.2f}"
                    
                    for setup_type, setup_stats in stats['setups'].items():
                        if setup_stats['trades'] > 0:
                            setup_win_rate = setup_stats['wins'] / setup_stats['trades']
                            log_msg += f"\n  {setup_type}: {setup_win_rate:.2%} "
                            log_msg += f"({setup_stats['wins']}/{setup_stats['trades']})"
            
            # Log optimal hours
            log_msg += "\n\nOptimal Trading Hours:"
            for hour in sorted(self.session_performance['optimal_hours']):
                stats = self.session_performance['hourly'][str(hour)]
                win_rate = stats['wins'] / stats['trades']
                log_msg += f"\n  {hour:02d}:00 - {win_rate:.2%} "
                log_msg += f"({stats['wins']}/{stats['trades']})"
            
            self.logger.info(log_msg)
            
        except Exception as e:
            self.logger.error(f"Error logging session performance: {e}")
    
    def learn_optimal_stop_distances(self, trade_data, outcome, exit_reason):
        """Learn optimal stop loss distances to minimize premature exits"""
        try:
            # Initialize stop loss performance tracking if not exists
            if not hasattr(self, 'stop_loss_performance'):
                self.stop_loss_performance = {
                    'regimes': {},  # Performance by market regime
                    'setups': {},   # Performance by setup type
                    'distances': {}, # Performance by stop distance bucket
                    'optimal_distances': {},  # Optimal distances by setup/regime
                    'total_trades': 0,
                    'last_optimization': datetime.now()
                }
            
            # Extract relevant information
            setup_type = trade_data.get('setup_type', 'UNKNOWN')
            features = trade_data.get('analysis', {}).get('features', {})
            stop_distance = trade_data.get('stop_distance', 0)
            regime_info = self._classify_market_regime(features)
            
            if regime_info is None or stop_distance == 0:
                return
                
            # Create regime key
            regime_key = f"{regime_info['volatility_regime']}_{regime_info['trend_regime']}"
            
            # Initialize tracking for new regime
            if regime_key not in self.stop_loss_performance['regimes']:
                self.stop_loss_performance['regimes'][regime_key] = {
                    'total': 0,
                    'premature_stops': 0,
                    'good_stops': 0,
                    'distances': {}
                }
            
            # Initialize tracking for new setup type
            if setup_type not in self.stop_loss_performance['setups']:
                self.stop_loss_performance['setups'][setup_type] = {
                    'total': 0,
                    'premature_stops': 0,
                    'good_stops': 0,
                    'optimal_distance': None
                }
            
            # Get distance bucket (round to nearest 5 pips)
            distance_bucket = round(stop_distance * 200) / 200  # 0.005 = 5 pips
            if distance_bucket not in self.stop_loss_performance['distances']:
                self.stop_loss_performance['distances'][distance_bucket] = {
                    'total': 0,
                    'premature_stops': 0,
                    'good_stops': 0
                }
            
            # Update statistics
            regime_stats = self.stop_loss_performance['regimes'][regime_key]
            setup_stats = self.stop_loss_performance['setups'][setup_type]
            distance_stats = self.stop_loss_performance['distances'][distance_bucket]
            
            regime_stats['total'] += 1
            setup_stats['total'] += 1
            distance_stats['total'] += 1
            
            # Analyze exit reason
            if exit_reason == 'hit_stop':
                if outcome == 0:  # Loss
                    regime_stats['premature_stops'] += 1
                    setup_stats['premature_stops'] += 1
                    distance_stats['premature_stops'] += 1
                else:  # Win after hitting stop
                    regime_stats['good_stops'] += 1
                    setup_stats['good_stops'] += 1
                    distance_stats['good_stops'] += 1
            
            self.stop_loss_performance['total_trades'] += 1
            
            # Optimize stop distances periodically
            if self.stop_loss_performance['total_trades'] % 50 == 0:
                self._optimize_stop_distances()
            
            if self.debug_mode:
                self._log_stop_loss_performance()
                
        except Exception as e:
            self.logger.error(f"Error in stop loss learning: {e}")
    
    def _optimize_stop_distances(self):
        """Optimize stop loss distances based on performance data"""
        try:
            # Find optimal distances for each setup type
            for setup_type, stats in self.stop_loss_performance['setups'].items():
                if stats['total'] >= 20:
                    best_distance = None
                    lowest_premature_rate = float('inf')
                    
                    for distance, dist_stats in self.stop_loss_performance['distances'].items():
                        if dist_stats['total'] >= 5:
                            premature_rate = dist_stats['premature_stops'] / dist_stats['total']
                            if premature_rate < lowest_premature_rate:
                                lowest_premature_rate = premature_rate
                                best_distance = distance
                    
                    if best_distance is not None:
                        self.stop_loss_performance['optimal_distances'][setup_type] = best_distance
            
            # Adjust for market regimes
            for regime_key, regime_stats in self.stop_loss_performance['regimes'].items():
                if regime_stats['total'] >= 20:
                    volatility = regime_key.split('_')[0]
                    
                    # Adjust optimal distances based on volatility
                    for setup_type in self.stop_loss_performance['optimal_distances']:
                        base_distance = self.stop_loss_performance['optimal_distances'][setup_type]
                        if volatility == 'high':
                            self.stop_loss_performance['optimal_distances'][f"{setup_type}_{regime_key}"] = base_distance * 1.5
                        elif volatility == 'low':
                            self.stop_loss_performance['optimal_distances'][f"{setup_type}_{regime_key}"] = base_distance * 0.8
            
            self.stop_loss_performance['last_optimization'] = datetime.now()
            
        except Exception as e:
            self.logger.error(f"Error optimizing stop distances: {e}")
    
    def _log_stop_loss_performance(self):
        """Log stop loss performance statistics"""
        try:
            log_msg = "\n=== STOP LOSS PERFORMANCE UPDATE ===\n"
            
            # Log setup type performance
            for setup_type, stats in self.stop_loss_performance['setups'].items():
                if stats['total'] > 0:
                    premature_rate = stats['premature_stops'] / stats['total']
                    optimal_distance = self.stop_loss_performance['optimal_distances'].get(setup_type)
                    
                    log_msg += f"\n{setup_type}:"
                    log_msg += f"\n  Premature Stop Rate: {premature_rate:.2%}"
                    log_msg += f"\n  Optimal Distance: {optimal_distance:.4f} if set"
            
            # Log regime-specific performance
            log_msg += "\n\nRegime Performance:"
            for regime_key, stats in self.stop_loss_performance['regimes'].items():
                if stats['total'] > 0:
                    premature_rate = stats['premature_stops'] / stats['total']
                    log_msg += f"\n  {regime_key}: {premature_rate:.2%} premature stops"
            
            self.logger.info(log_msg)
            
        except Exception as e:
            self.logger.error(f"Error logging stop loss performance: {e}")
    
    def on_trade_close(self, trade_data, actual_outcome):
        """Enhanced trade close handling with advanced learning"""
        try:
            # Add outcome to trade data and trade history
            trade_data['actual_outcome'] = actual_outcome
            self.trade_history.append(trade_data)

            # Update ensemble weights based on performance
            self.update_ensemble_weights_from_performance(trade_data, actual_outcome)
            
            # Learn from market regime
            self.learn_market_regime_performance(trade_data, actual_outcome)
            
            # Learn from session performance
            self.learn_session_performance(trade_data, actual_outcome)
            
            # Learn from stop loss performance
            exit_reason = trade_data.get('exit_reason', 'unknown')
            self.learn_optimal_stop_distances(trade_data, actual_outcome, exit_reason)
            
            # Learn from market regime
            self.learn_market_regime_performance(trade_data, actual_outcome)

            # --- Master learning coordinator integration ---
            if hasattr(self, 'master_learning_update'):
                self.master_learning_update(trade_data, actual_outcome)

            # Prepare features for online learning
            analysis = trade_data.get('analysis', {})
            features_dict = analysis.get('features', {})
            feature_vector = [features_dict.get(f, 0.0) for f in self.feature_columns]

            # Enhanced Learning with Advanced Learning Engine
            if hasattr(self, 'advanced_learning_engine'):
                try:
                    confidence = trade_data.get('confidence', 0.5)
                    self.advanced_learning_engine.learn_from_trade(
                        outcome=actual_outcome,
                        confidence=confidence,
                        features=features_dict,
                        market_context=analysis.get('market_context', {})
                    )
                    # Update real-time analytics
                    if hasattr(self, 'real_time_analytics'):
                        self.real_time_analytics.update_trade_outcome(actual_outcome, confidence)
                    # Update enhanced risk manager
                    if hasattr(self, 'enhanced_risk_manager'):
                        profit_loss = trade_data.get('profit_loss', 0)
                        self.enhanced_risk_manager.update_trade_result(profit_loss, actual_outcome)
                    if self.debug_mode:
                        print(f"üß† Enhanced learning updated for trade {trade_data.get('order_id')}")
                except Exception as learning_error:
                    print(f"‚ö†Ô∏è Enhanced learning error: {learning_error}")

            # Learn from ULTRA_FORCED patterns specifically
            self.learn_ultra_forced_patterns(trade_data, actual_outcome)

            # Original online learning (as fallback)
            if self.config['ml_settings'].get('online_learning_enabled', True):
                self.online_partial_fit(feature_vector, actual_outcome)
                if self.debug_mode:
                    print(f"üß† Online learning updated for trade {trade_data.get('order_id')}")

        except Exception as e:
            print(f"‚ùå Trade close handling error: {e}")
            self.logger.error(f"Trade close handling error: {e}")

    # TODO: Add specialized learning for ULTRA_FORCED setup performance patterns
    # Based on your 25K dataset showing 40%+ win rates for ULTRA_FORCED setups
    # Track: RSI level vs win rate, Timeframe vs performance, Volatility regime vs success
    # Learn: Which specific RSI ranges (20-25, 75-80, 80-85) perform best
    # Adapt: RSI thresholds dynamically based on recent ULTRA_FORCED performance
    # Expected: Fine-tune ULTRA_FORCED detection for maximum profitability
    def learn_ultra_forced_patterns(self, trade_data, outcome):
        """Learn winning patterns for ULTRA_FORCED setups to optimize RSI thresholds"""
        if trade_data.get('setup_type') != 'ULTRA_FORCED':
            return
        
        try:
            # Extract pattern components
            analysis = trade_data.get('analysis', {})
            features = analysis.get('features', {})
            
            rsi = features.get('rsi', 50)
            timeframe = trade_data.get('timeframe', 'M15')
            symbol = trade_data.get('symbol', 'Unknown')
            volatility = features.get('high_volatility', 0)
            atr = features.get('atr', 0.001)
            confidence = trade_data.get('confidence', 0.5)
            
            # Initialize ULTRA_FORCED pattern tracking if not exists
            if not hasattr(self, 'ultra_forced_patterns'):
                self.ultra_forced_patterns = {
                    'rsi_buckets': {},  # RSI ranges and their performance
                    'timeframe_performance': {},  # Performance by timeframe
                    'volatility_performance': {},  # Performance in different volatility regimes
                    'symbol_performance': {},  # Performance by symbol
                    'confidence_accuracy': {},  # Confidence vs actual performance
                    'total_ultra_trades': 0,
                    'winning_ultra_trades': 0,
                    'last_optimization': datetime.now(),
                    'optimization_interval': 50  # Optimize every 50 ULTRA_FORCED trades
                }
            
            # Update total ULTRA_FORCED trade count
            self.ultra_forced_patterns['total_ultra_trades'] += 1
            if outcome == 1:
                self.ultra_forced_patterns['winning_ultra_trades'] += 1
            
            # 1. Track RSI bucket performance
            rsi_bucket = self._get_rsi_bucket(rsi)
            if rsi_bucket not in self.ultra_forced_patterns['rsi_buckets']:
                self.ultra_forced_patterns['rsi_buckets'][rsi_bucket] = {
                    'trades': 0, 'wins': 0, 'win_rate': 0.0, 'avg_confidence': 0.0
                }
            
            bucket_data = self.ultra_forced_patterns['rsi_buckets'][rsi_bucket]
            bucket_data['trades'] += 1
            bucket_data['wins'] += outcome
            bucket_data['win_rate'] = bucket_data['wins'] / bucket_data['trades']
            bucket_data['avg_confidence'] = (bucket_data['avg_confidence'] * (bucket_data['trades'] - 1) + confidence) / bucket_data['trades']
            
            # 2. Track timeframe performance
            if timeframe not in self.ultra_forced_patterns['timeframe_performance']:
                self.ultra_forced_patterns['timeframe_performance'][timeframe] = {
                    'trades': 0, 'wins': 0, 'win_rate': 0.0
                }
            
            tf_data = self.ultra_forced_patterns['timeframe_performance'][timeframe]
            tf_data['trades'] += 1
            tf_data['wins'] += outcome
            tf_data['win_rate'] = tf_data['wins'] / tf_data['trades']
            
            # 3. Track volatility regime performance
            vol_regime = 'high' if volatility else ('medium' if atr > 0.002 else 'low')
            if vol_regime not in self.ultra_forced_patterns['volatility_performance']:
                self.ultra_forced_patterns['volatility_performance'][vol_regime] = {
                    'trades': 0, 'wins': 0, 'win_rate': 0.0
                }
            
            vol_data = self.ultra_forced_patterns['volatility_performance'][vol_regime]
            vol_data['trades'] += 1
            vol_data['wins'] += outcome
            vol_data['win_rate'] = vol_data['wins'] / vol_data['trades']
            
            # 4. Track symbol performance
            if symbol not in self.ultra_forced_patterns['symbol_performance']:
                self.ultra_forced_patterns['symbol_performance'][symbol] = {
                    'trades': 0, 'wins': 0, 'win_rate': 0.0
                }
            
            symbol_data = self.ultra_forced_patterns['symbol_performance'][symbol]
            symbol_data['trades'] += 1
            symbol_data['wins'] += outcome
            symbol_data['win_rate'] = symbol_data['wins'] / symbol_data['trades']
            
            # 5. Track confidence accuracy for ULTRA_FORCED
            conf_bucket = self._get_confidence_bucket(confidence)
            if conf_bucket not in self.ultra_forced_patterns['confidence_accuracy']:
                self.ultra_forced_patterns['confidence_accuracy'][conf_bucket] = {
                    'predictions': 0, 'successes': 0, 'accuracy': 0.0
                }
            
            conf_data = self.ultra_forced_patterns['confidence_accuracy'][conf_bucket]
            conf_data['predictions'] += 1
            conf_data['successes'] += outcome
            conf_data['accuracy'] = conf_data['successes'] / conf_data['predictions']
            
            if self.debug_mode:
                print(f"üéØ ULTRA_FORCED pattern learned: RSI {rsi:.1f} ({rsi_bucket}), {timeframe}, {vol_regime} vol, outcome: {outcome}")
            
            # 6. Optimize ULTRA_FORCED thresholds periodically
            if (self.ultra_forced_patterns['total_ultra_trades'] % self.ultra_forced_patterns['optimization_interval'] == 0):
                self._optimize_ultra_forced_thresholds()
                
        except Exception as e:
            print(f"‚ö†Ô∏è ULTRA_FORCED pattern learning error: {e}")
            logging.error(f"ULTRA_FORCED pattern learning error: {e}")
    
    def _get_rsi_bucket(self, rsi):
        """Get RSI bucket for pattern analysis"""
        if rsi <= 15:
            return 'RSI_0-15'
        elif rsi <= 20:
            return 'RSI_15-20'
        elif rsi <= 25:
            return 'RSI_20-25'
        elif rsi <= 30:
            return 'RSI_25-30'
        elif rsi >= 85:
            return 'RSI_85-100'
        elif rsi >= 80:
            return 'RSI_80-85'
        elif rsi >= 75:
            return 'RSI_75-80'
        elif rsi >= 70:
            return 'RSI_70-75'
        else:
            return 'RSI_30-70'  # Neutral zone (shouldn't trigger ULTRA_FORCED)
    
    def _adjust_rsi_thresholds_from_patterns(self, best_rsi_ranges):
        """Adjust RSI thresholds based on learned patterns"""
        try:
            # Initialize dynamic RSI thresholds if not exists
            if not hasattr(self, 'dynamic_rsi_thresholds'):
                self.dynamic_rsi_thresholds = {
                    'oversold_min': 20,   # Original threshold
                    'oversold_max': 30,   # Original threshold
                    'overbought_min': 70, # Original threshold
                    'overbought_max': 80, # Original threshold
                    'optimization_count': 0
                }
            
            # Analyze best performing ranges
            oversold_winners = []
            overbought_winners = []
            
            for bucket, win_rate, trades in best_rsi_ranges:
                if trades >= 10 and win_rate > 0.45:  # Only consider ranges with >45% win rate
                    if 'RSI_15-20' in bucket or 'RSI_20-25' in bucket or 'RSI_25-30' in bucket:
                        oversold_winners.append((bucket, win_rate))
                    elif 'RSI_70-75' in bucket or 'RSI_75-80' in bucket or 'RSI_80-85' in bucket:
                        overbought_winners.append((bucket, win_rate))
            
            # Adjust oversold thresholds
            if oversold_winners:
                best_oversold = max(oversold_winners, key=lambda x: x[1])
                if 'RSI_15-20' in best_oversold[0]:
                    self.dynamic_rsi_thresholds['oversold_min'] = 15
                    self.dynamic_rsi_thresholds['oversold_max'] = 20
                elif 'RSI_20-25' in best_oversold[0]:
                    self.dynamic_rsi_thresholds['oversold_min'] = 20
                    self.dynamic_rsi_thresholds['oversold_max'] = 25
                elif 'RSI_25-30' in best_oversold[0]:
                    self.dynamic_rsi_thresholds['oversold_min'] = 25
                    self.dynamic_rsi_thresholds['oversold_max'] = 30
                
                print(f"üéØ Optimized oversold RSI threshold: {self.dynamic_rsi_thresholds['oversold_min']}-{self.dynamic_rsi_thresholds['oversold_max']} (based on {best_oversold[0]}: {best_oversold[1]:.1%})")
            
            # Adjust overbought thresholds
            if overbought_winners:
                best_overbought = max(overbought_winners, key=lambda x: x[1])
                if 'RSI_70-75' in best_overbought[0]:
                    self.dynamic_rsi_thresholds['overbought_min'] = 70
                    self.dynamic_rsi_thresholds['overbought_max'] = 75
                elif 'RSI_75-80' in best_overbought[0]:
                    self.dynamic_rsi_thresholds['overbought_min'] = 75
                    self.dynamic_rsi_thresholds['overbought_max'] = 80
                elif 'RSI_80-85' in best_overbought[0]:
                    self.dynamic_rsi_thresholds['overbought_min'] = 80
                    self.dynamic_rsi_thresholds['overbought_max'] = 85
                
                print(f"üéØ Optimized overbought RSI threshold: {self.dynamic_rsi_thresholds['overbought_min']}-{self.dynamic_rsi_thresholds['overbought_max']} (based on {best_overbought[0]}: {best_overbought[1]:.1%})")
            
            self.dynamic_rsi_thresholds['optimization_count'] += 1
            
        except Exception as e:
            print(f"‚ùå RSI threshold adjustment error: {e}")
            logging.error(f"RSI threshold adjustment error: {e}")
    
    def get_optimized_ultra_forced_thresholds(self):
        """Get current optimized ULTRA_FORCED RSI thresholds"""
        if hasattr(self, 'dynamic_rsi_thresholds'):
            return self.dynamic_rsi_thresholds
        else:
            # Return default thresholds
            return {
                'oversold_min': 20,
                'oversold_max': 30,
                'overbought_min': 70,
                'overbought_max': 80
            }

    # TODO: Add adaptive position sizing based on recent symbol performance
    # Current: Fixed position sizing with basic confidence multiplier
    # Enhancement: Increase size for symbols/setups with recent winning streaks
    # Decrease size for symbols with recent losses, regardless of ML confidence
    # Track last 20 trades per symbol, adjust risk multiplier accordingly
    # Expected: Larger positions on hot streaks, smaller on cold streaks
    def adaptive_risk_management(self, symbol, timeframe):
        """Adjust position sizing based on recent performance by symbol and setup type"""
        try:
            # Initialize adaptive risk tracking if not exists
            if not hasattr(self, 'asset_risk_multipliers'):
                self.asset_risk_multipliers = {}
                self.symbol_performance_history = {}
                self.adaptive_risk_config = {
                    'lookback_trades': 20,  # Number of recent trades to analyze
                    'hot_streak_threshold': 0.60,  # Win rate > 60% = hot streak
                    'cold_streak_threshold': 0.30,  # Win rate < 30% = cold streak
                    'hot_multiplier': 1.3,  # Increase size by 30% on hot streaks
                    'cold_multiplier': 0.7,  # Decrease size by 30% on cold streaks
                    'neutral_multiplier': 1.0,  # Normal size for balanced performance
                    'min_trades_required': 5,  # Minimum trades before adjustment
                    'update_frequency': 5  # Update every 5 trades
                }
            
            # Get recent trades for this symbol (last 50 trades, filter to symbol)
            recent_symbol_trades = [
                t for t in self.trade_history[-50:] 
                if t.get('symbol') == symbol and 'actual_outcome' in t
            ]
            
            # Need minimum trades for statistical significance
            config = self.adaptive_risk_config
            if len(recent_symbol_trades) < config['min_trades_required']:
                # Not enough data, use neutral multiplier
                self.asset_risk_multipliers[symbol] = config['neutral_multiplier']
                if self.debug_mode:
                    print(f"üîÑ {symbol}: Insufficient trade history ({len(recent_symbol_trades)} trades), using neutral multiplier")
                return config['neutral_multiplier']
            
            # Take only the most recent trades up to lookback limit
            recent_trades = recent_symbol_trades[-config['lookback_trades']:]
            
            # Calculate performance metrics
            total_trades = len(recent_trades)
            winning_trades = sum(1 for t in recent_trades if t.get('actual_outcome') == 1)
            win_rate = winning_trades / total_trades if total_trades > 0 else 0.0
            
            # Calculate additional performance metrics
            recent_pnl = sum(t.get('profit_loss', 0) for t in recent_trades if 'profit_loss' in t)
            avg_pnl_per_trade = recent_pnl / total_trades if total_trades > 0 else 0.0
            
            # Track setup type performance separately
            setup_performance = {}
            for trade in recent_trades:
                setup_type = trade.get('setup_type', 'STANDARD')
                if setup_type not in setup_performance:
                    setup_performance[setup_type] = {'trades': 0, 'wins': 0}
                setup_performance[setup_type]['trades'] += 1
                setup_performance[setup_type]['wins'] += trade.get('actual_outcome', 0)
            
            # Determine risk multiplier based on performance
            if win_rate >= config['hot_streak_threshold']:
                # Hot streak - increase position size
                multiplier = config['hot_multiplier']
                streak_type = "HOT"
                
                # Extra bonus for very hot streaks (>75% win rate)
                if win_rate >= 0.75:
                    multiplier = min(1.5, multiplier * 1.1)  # Cap at 1.5x
                    
            elif win_rate <= config['cold_streak_threshold']:
                # Cold streak - decrease position size
                multiplier = config['cold_multiplier']
                streak_type = "COLD"
                
                # Extra penalty for very cold streaks (<20% win rate)
                if win_rate <= 0.20:
                    multiplier = max(0.5, multiplier * 0.9)  # Floor at 0.5x
                    
            else:
                # Neutral performance - normal position size
                multiplier = config['neutral_multiplier']
                streak_type = "NEUTRAL"
            
            # Apply timeframe-specific adjustments
            timeframe_adjustments = {
                'M5': 0.9,   # Slightly smaller for higher frequency
                'M15': 1.0,  # Baseline
                'M30': 1.05, # Slightly larger for longer holds
                'H1': 1.1    # Larger for longest timeframe
            }
            timeframe_adj = timeframe_adjustments.get(timeframe, 1.0)
            multiplier *= timeframe_adj
            
            # Store the multiplier and performance data
            self.asset_risk_multipliers[symbol] = multiplier
            self.symbol_performance_history[symbol] = {
                'win_rate': win_rate,
                'total_trades': total_trades,
                'winning_trades': winning_trades,
                'avg_pnl_per_trade': avg_pnl_per_trade,
                'streak_type': streak_type,
                'multiplier': multiplier,
                'setup_performance': setup_performance,
                'last_updated': datetime.now(),
                'timeframe_adjustment': timeframe_adj
            }
            
            if self.debug_mode:
                print(f"üéØ {symbol} Adaptive Risk Analysis:")
                print(f"   Recent Trades: {total_trades} (Win Rate: {win_rate:.1%})")
                print(f"   Streak Type: {streak_type}")
                print(f"   Base Multiplier: {config.get(streak_type.lower() + '_multiplier', 1.0):.2f}")
                print(f"   Timeframe Adj ({timeframe}): {timeframe_adj:.2f}")
                print(f"   Final Multiplier: {multiplier:.2f}")
                print(f"   Avg P&L per Trade: ${avg_pnl_per_trade:.2f}")
                
                # Show setup type breakdown
                if setup_performance:
                    print(f"   Setup Performance:")
                    for setup, perf in setup_performance.items():
                        setup_wr = perf['wins'] / max(perf['trades'], 1)
                        print(f"      {setup}: {setup_wr:.1%} ({perf['trades']} trades)")
            
            return multiplier
            
        except Exception as e:
            print(f"‚ö†Ô∏è Adaptive risk management error for {symbol}: {e}")
            logging.error(f"Adaptive risk management error for {symbol}: {e}")
            # Return neutral multiplier on error
            return 1.0

    def get_symbol_performance_summary(self, symbol=None):
        """Get performance summary for symbol(s)"""
        try:
            if not hasattr(self, 'symbol_performance_history'):
                return {}
            
            if symbol:
                # Return data for specific symbol
                return self.symbol_performance_history.get(symbol, {})
            else:
                # Return data for all symbols
                return self.symbol_performance_history.copy()
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error getting symbol performance summary: {e}")
            return {}

    def update_adaptive_risk_system(self):
        """Update adaptive risk multipliers for all traded symbols"""
        try:
            if not hasattr(self, 'asset_risk_multipliers'):
                return
            
            # Get all symbols that have been traded
            traded_symbols = set()
            for trade in self.trade_history[-100:]:  # Look at recent trades
                if 'symbol' in trade:
                    traded_symbols.add(trade['symbol'])
            
            # Update risk multipliers for each symbol
            updated_count = 0
            for symbol in traded_symbols:
                # Use M15 as default timeframe for risk calculation
                self.adaptive_risk_management(symbol, 'M15')
                updated_count += 1
            
            if self.debug_mode and updated_count > 0:
                print(f"üîÑ Updated adaptive risk multipliers for {updated_count} symbols")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating adaptive risk system: {e}")
            logging.error(f"Adaptive risk system update error: {e}")

    def generate_adaptive_risk_report(self):
        """Generate comprehensive adaptive risk management report"""
        try:
            print("\n" + "="*60)
            print("üéØ ADAPTIVE RISK MANAGEMENT REPORT")
            print("="*60)
            
            if not hasattr(self, 'symbol_performance_history'):
                print("‚ùå No adaptive risk data available")
                return
            
            # Overall statistics
            total_symbols = len(self.symbol_performance_history)
            hot_streaks = sum(1 for data in self.symbol_performance_history.values() 
                            if data.get('streak_type') == 'HOT')
            cold_streaks = sum(1 for data in self.symbol_performance_history.values() 
                             if data.get('streak_type') == 'COLD')
            
            print(f"üìä Overview:")
            print(f"   Symbols Tracked: {total_symbols}")
            print(f"   Hot Streaks: {hot_streaks}")
            print(f"   Cold Streaks: {cold_streaks}")
            print(f"   Neutral: {total_symbols - hot_streaks - cold_streaks}")
            
            # Individual symbol performance
            print(f"\nüí∞ Symbol Performance & Risk Multipliers:")
            
            # Sort symbols by performance
            sorted_symbols = sorted(
                self.symbol_performance_history.items(),
                key=lambda x: x[1].get('win_rate', 0),
                reverse=True
            )
            
            for symbol, data in sorted_symbols:
                streak_emoji = {
                    'HOT': 'üî•',
                    'COLD': 'üßä',
                    'NEUTRAL': '‚öñÔ∏è'
                }.get(data.get('streak_type', 'NEUTRAL'), '‚öñÔ∏è')
                
                print(f"   {streak_emoji} {symbol}:")
                print(f"      Win Rate: {data.get('win_rate', 0):.1%} ({data.get('winning_trades', 0)}/{data.get('total_trades', 0)} trades)")
                print(f"      Risk Multiplier: {data.get('multiplier', 1.0):.2f}x")
                print(f"      Avg P&L: ${data.get('avg_pnl_per_trade', 0):.2f}")
                print(f"      Streak: {data.get('streak_type', 'UNKNOWN')}")
                
                # Show setup breakdown if available
                setup_perf = data.get('setup_performance', {})
                if setup_perf and len(setup_perf) > 1:
                    print(f"      Setup Breakdown:")
                    for setup, perf in setup_perf.items():
                        setup_wr = perf['wins'] / max(perf['trades'], 1)
                        print(f"         {setup}: {setup_wr:.1%} ({perf['trades']} trades)")
            
            # Risk distribution
            if self.asset_risk_multipliers:
                multipliers = list(self.asset_risk_multipliers.values())
                avg_multiplier = sum(multipliers) / len(multipliers)
                max_multiplier = max(multipliers)
                min_multiplier = min(multipliers)
                
                print(f"\nüìà Risk Multiplier Statistics:")
                print(f"   Average: {avg_multiplier:.2f}x")
                print(f"   Range: {min_multiplier:.2f}x - {max_multiplier:.2f}x")
                
                # Show most aggressive and conservative
                max_symbol = max(self.asset_risk_multipliers.items(), key=lambda x: x[1])
                min_symbol = min(self.asset_risk_multipliers.items(), key=lambda x: x[1])
                
                print(f"   Most Aggressive: {max_symbol[0]} ({max_symbol[1]:.2f}x)")
                print(f"   Most Conservative: {min_symbol[0]} ({min_symbol[1]:.2f}x)")
            
            print("\n" + "="*60)
            print("üéØ ADAPTIVE RISK REPORT COMPLETE")
            print("="*60)
            
        except Exception as e:
            print(f"‚ùå Error generating adaptive risk report: {e}")
            logging.error(f"Adaptive risk report error: {e}")

    def __init__(self, config_file='adaptive_ml_trading_config.json'):
        """
        Enhanced Adaptive ML Trading System with comprehensive fixes
        
        Key Features:
        - Complete data leakage protection
        - Enhanced model management
        - Production-ready architecture
        - Comprehensive validation
        """
        
        # === System Initialization Banner ===
        self._print_init_banner()
        
        # === Thread Safety Setup ===
        self._init_threading()
        
        # === Configuration Loading ===
        self.config = self._load_config_with_fallback(config_file)
        
        # === Enhanced Model Architecture ===
        self.models = {
            'timeframe_ensemble': None,
            'adaptive_learner': None,
            'EURUSDm_specialist': None,
            'gold_specialist': None,  # Consistent naming
            'meta_ensemble': None
        }
        
        # === Feature Management ===
        self.feature_columns = []
        self.feature_scaler = StandardScaler()
        self.label_encoders = {}
        
        # === Account/Risk Parameters ===
        self.min_account_balance = self.config.get('account', {}).get('min_account_balance', 50.0)
        self.max_daily_trades = self.config.get('account', {}).get('max_daily_trades', 12)
        self.risk_per_trade = self.config.get('account', {}).get('risk_per_trade', 0.01)
        self.debug_mode = self.config.get('system', {}).get('debug_mode', False)
        
        # === Data Management ===
        self.mega_dataset = None
        self.available_assets = []
        self.available_timeframes = []
        
        # === Trading State ===
        self.active_trades = {}
        self.trade_history = []
        self.pending_orders = []
        
        # === Performance Tracking ===
        self.performance_metrics = self._init_performance_metrics()
        
        # === Adaptive Learning Parameters ===
        self.confidence_threshold = self.config['ml_settings']['confidence_threshold']
        self.learning_active = True
        self.retraining_frequency = self.config['adaptive_learning'].get('retrain_interval', 100)

        # === Adaptive RSI Ensemble & Regime Detection ===
        self.rsi_ensemble = AdaptiveRSIEnsemble()
        self.regime_detector = HMMRegimeDetector()
        self.rsi_optimizer = RSIWeightOptimizer(self.rsi_ensemble)
        self.analysis_log = []
        
        # === Online Learning Model ===
        self.online_model = SGDClassifier(
            loss='log_loss', 
            learning_rate='optimal', 
            eta0=0.01, 
            random_state=42, 
            warm_start=True
        )
        self.online_model_initialized = False
        
        # === System Services ===
        self._init_system_services()
        
        # === Enhanced Learning Components ===
        self.learning_engine = None
        self.analytics_engine = None
        self.risk_manager = None
        
        # Dynamic threshold adaptation
        self.asset_thresholds = {
            'XAUUSDm': 0.30,
            'EURUSDm': 0.35,
            'GBPUSDm': 0.40,
            'BTCUSDm': 0.45
        }
        
        # Confidence calibration tracking
        self.confidence_calibration = {
            'total_trades': 0,
            'confidence_buckets': {},  # {confidence_range: {'predictions': 0, 'successes': 0}}
            'symbol_performance': {},  # {symbol: {'high_conf_losses': 0, 'low_conf_wins': 0}}
            'last_calibration': datetime.now()
        }
        
        # Threshold adaptation tracking
        self.threshold_adaptation = {
            'adjustment_history': [],
            'performance_window': 50,  # Trades to consider for adaptation
            'min_trades_for_adjustment': 20,
            'adaptation_rate': 0.02,  # How much to adjust thresholds
            'last_adaptation': datetime.now()
        }
        
        # Initialize adaptive position sizing system
        self.asset_risk_multipliers = {}
        self.symbol_performance_history = {}
        self.adaptive_risk_config = {
            'lookback_trades': 20,  # Number of recent trades to analyze
            'hot_streak_threshold': 0.60,  # Win rate > 60% = hot streak
            'cold_streak_threshold': 0.30,  # Win rate < 30% = cold streak
            'hot_multiplier': 1.3,  # Increase size by 30% on hot streaks
            'cold_multiplier': 0.7,  # Decrease size by 30% on cold streaks
            'neutral_multiplier': 1.0,  # Normal size for balanced performance
            'min_trades_required': 5,  # Minimum trades before adjustment
            'update_frequency': 5  # Update every 5 trades
        }
        
        # Initialize enhanced components after other setup
        self._init_enhanced_learning_components()
        
        print("‚úÖ Enhanced Adaptive ML Trading System Initialized Successfully")
        self._print_config_summary()

    def _print_init_banner(self):
        """Print enhanced system initialization banner"""
        print("\n" + "="*80)
        print("üöÄ INITIALIZING ENHANCED ADAPTIVE ML TRADING SYSTEM - v3.0".center(80))
        print("="*80)
        print("üìä Foundation: Professional Breakout Setups with Enhanced Validation")
        print("ü§ñ Capabilities: Cross-Asset, Multi-Timeframe, Self-Learning, Data-Leak Protection")
        print("üîß Features: Production Ready, Online Learning, Comprehensive Risk Management")
        print("-"*80)

    def _init_threading(self):
        """Initialize enhanced threading components"""
        self.print_lock = threading.RLock()
        self.scan_lock = threading.Lock()
        self.trade_lock = threading.Lock()
        self.data_lock = threading.Lock()
        self.model_lock = threading.Lock()  # Added for model updates
        self.currently_scanning = False

    def _load_config_with_fallback(self, config_file):
        """Enhanced configuration loading with validation"""
        default_config = self._get_enhanced_default_config()
        
        try:
            if Path(config_file).exists():
                with open(config_file, 'r', encoding='utf-8-sig') as f:  # Use utf-8-sig to handle BOM
                    user_config = json.load(f)
                config = self._deep_merge(default_config, user_config)
            else:
                config = default_config
                # Save default config
                with open(config_file, 'w') as f:
                    json.dump(default_config, f, indent=4)
                print(f"üìù Default configuration created at {config_file}")
            
            # Validate configuration
            self._validate_enhanced_config(config)
            
            print(f"‚úÖ Configuration loaded successfully")
            return config
            
        except Exception as e:
            print(f"‚ö†Ô∏è Config error: {e}. Using defaults.")
            return default_config

    def _get_enhanced_default_config(self):
        """Enhanced default configuration"""
        return {
            "version": "3.0_enhanced",
            "system": {
                "debug_mode": True,
                "max_threads": 4,
                "memory_limit_mb": 2048,
                "performance_mode": "balanced",
                "model_validation_enabled": True
            },
            "account": {
                "server": "Exness-MT5Trial9",
                "login": 210265375,
                "password": "Glo@1234567890",
                "risk_per_trade": 0.01,
                "max_daily_trades": 12,
                "min_account_balance": 50.0,
                "balance_protection": True,
                "position_sizing": "volatility_adjusted"
            },
            "trading": {
                "symbols": ["EURUSDm", "XAUUSDm", "GBPUSDm", "BTCUSDm"],
                "timeframes": ["M5", "M15", "M30", "H1"],
                "max_simultaneous_trades": 3,
                "max_trades_per_symbol": 2,
                "session_weights": {
                    "london": 1.2,
                    "newyork": 1.1,
                    "asian": 0.8
                }
            },
            "ml_settings": {
                "confidence_threshold": 0.30,
                "dynamic_threshold_adjustment": True,
                "ensemble_strategy": "weighted_average",
                "feature_importance_min": 0.01,
                "model_validation_threshold": 0.52,
                "online_learning_enabled": True
            },
            "risk_management": {
                "max_drawdown": 0.15,
                "daily_loss_limit": 0.05,
                "position_sizing": "confidence_based",
                "volatility_multiplier": 2.0,
                "correlation_limit": 0.7
            },
            "adaptive_learning": {
                "retrain_interval": 100,
                "learning_rate_decay": 0.995,
                "memory_window": 500,
                "drift_detection_enabled": True,
                "performance_monitoring": True
            },
            "data_settings": {
                "base_directory": "dataFiles",
                "required_columns": ["setuptime", "asset", "atr", "rsi", "hittp"],
                "min_data_points": 100,
                "validation_enabled": True
            }
        }

    def _validate_enhanced_config(self, config):
        """Enhanced configuration validation"""
        required_sections = [
            "system", "account", "trading", "ml_settings", 
            "risk_management", "adaptive_learning", "data_settings"
        ]
        
        for section in required_sections:
            if section not in config:
                raise ValueError(f"Missing required config section: {section}")
        
        # Validate critical values
        risk_per_trade = config["account"].get("risk_per_trade", 0.01)
        if not isinstance(risk_per_trade, (int, float)) or risk_per_trade <= 0 or risk_per_trade > 0.1:
            raise ValueError(f"risk_per_trade must be between 0 and 0.1, got {risk_per_trade}")
        conf_thresh = config["ml_settings"].get("confidence_threshold", 0.3)
        if isinstance(conf_thresh, dict):
            for k, v in conf_thresh.items():
                if not isinstance(v, (int, float)) or v <= 0 or v >= 1:
                    raise ValueError(f"confidence_threshold for {k} must be between 0 and 1, got {v}")
        else:
            if not isinstance(conf_thresh, (int, float)) or conf_thresh <= 0 or conf_thresh >= 1:
                raise ValueError(f"confidence_threshold must be between 0 and 1, got {conf_thresh}")
        return True

    def _deep_merge(self, base_dict, update_dict):
        """Deep merge dictionaries"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value
        return base_dict

    def _init_performance_metrics(self):
        """Initialize enhanced performance tracking"""
        return {
            'total_trades': 0,
            'winning_trades': 0,
            'losing_trades': 0,
            'accuracy': 0.0,
            'total_pnl': 0.0,
            'max_drawdown': 0.0,
            'sharpe_ratio': 0.0,
            'profit_factor': 0.0,
            'avg_confidence': 0.0,
            'model_performance': {},
            'last_updated': datetime.now()
        }

    def _init_system_services(self):
        """Initialize enhanced system services"""
        self.setup_enhanced_logging()
        self.setup_enhanced_database()
        self._init_model_manager()

    def setup_enhanced_logging(self):
        """Enhanced logging setup"""
        log_format = '%(asctime)s | %(levelname)s | %(funcName)s | %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler('enhanced_trading_system.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('EnhancedTradingSystem')
        self.logger.info("Enhanced logging system initialized")

    def setup_enhanced_database(self):
        """Enhanced database setup with additional tables"""
        self.conn = sqlite3.connect('enhanced_trading_system.db', check_same_thread=False)
        cursor = self.conn.cursor()
        
        # Enhanced trades table
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS enhanced_trades (
            trade_id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME,
            symbol TEXT,
            timeframe TEXT,
            direction TEXT,
            entry_price REAL,
            exit_price REAL,
            stop_loss REAL,
            take_profit REAL,
            volume REAL,
            duration_minutes INTEGER,
            pnl REAL,
            pnl_pips REAL,
            confidence REAL,
            model_version TEXT,
            features_used TEXT,
            market_conditions TEXT,
            risk_amount REAL,
            actual_outcome INTEGER,
            predicted_outcome INTEGER,
            hit_tp BOOLEAN,
            exit_reason TEXT,
            session TEXT,
            volatility_regime TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )''')
        
        # Model performance tracking
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS model_performance (
            performance_id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME,
            model_name TEXT,
            accuracy REAL,
            precision REAL,
            recall REAL,
            f1_score REAL,
            confidence_avg REAL,
            trades_count INTEGER,
            validation_period TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )''')
        
        # Feature importance tracking
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS feature_tracking (
            feature_id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME,
            feature_name TEXT,
            importance_score REAL,
            model_name TEXT,
            ranking INTEGER,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )''')
        
        self.conn.commit()
        print("‚úÖ Enhanced database initialized")

    def _init_model_manager(self):
        """Initialize model management system"""
        self.model_metadata = {
            'last_trained': None,
            'training_samples': 0,
            'validation_scores': {},
            'feature_importance': {},
            'model_versions': {}
        }

    def _init_enhanced_learning_components(self):
        """Initialize enhanced learning system components"""
        try:
            print("üß† Initializing Enhanced Learning Components...")
            
            # Initialize the advanced learning engine
            self.advanced_learning_engine = AdvancedLearningEngine(self)
            
            # Initialize real-time analytics
            self.real_time_analytics = RealTimeAnalytics(self)
            
            # Initialize enhanced risk manager
            self.enhanced_risk_manager = EnhancedRiskManager(self)
            
            # Track last learning update
            self.last_learning_update = time.time()
            self.learning_update_interval = 300  # 5 minutes
            
            print("‚úÖ Enhanced Learning Components initialized successfully")
            
            # Load previous learning state if available
            self.load_enhanced_learning_state()
            
        except Exception as e:
            print(f"‚ùå Error initializing enhanced learning components: {str(e)}")
            logging.error(f"Enhanced learning initialization error: {str(e)}")
            
            # Fallback initialization - at minimum initialize the required attributes
            if not hasattr(self, 'last_learning_update'):
                self.last_learning_update = time.time()
            if not hasattr(self, 'learning_update_interval'):
                self.learning_update_interval = 300
            
            print("‚ö†Ô∏è Enhanced learning initialized with minimal fallback")
            
            # Re-raise the exception if it's critical
            import traceback
            traceback.print_exc()

    def update_enhanced_learning_system(self):
        """Periodically update enhanced learning components"""
        try:
            # Safety check: ensure enhanced learning is properly initialized
            if not hasattr(self, 'last_learning_update'):
                print("‚ö†Ô∏è Enhanced learning not initialized, initializing now...")
                self._init_enhanced_learning_components()
                return
                
            current_time = time.time()
            
            # Check if update is needed
            if current_time - self.last_learning_update < self.learning_update_interval:
                return
                
            if hasattr(self, 'advanced_learning_engine'):
                # Update confidence calibration
                insights = self.advanced_learning_engine.get_trading_insights()
                
                # Update threshold adaptation based on performance
                for symbol in self.asset_thresholds:
                    timeframes = self.asset_thresholds[symbol]
                    # If timeframes is a float, treat as single threshold
                    if isinstance(timeframes, float):
                        current_threshold = self._safe_get(self.threshold_adaptation, symbol, 0.45)
                        recent_accuracy = self._safe_get(insights, 'recent_accuracy', 0.5)
                        if recent_accuracy > 0.7:
                            new_threshold = max(0.75, current_threshold - 0.02)
                        elif recent_accuracy < 0.4:
                            new_threshold = min(0.95, current_threshold + 0.05)
                        else:
                            new_threshold = current_threshold
                        self.threshold_adaptation[symbol] = new_threshold
                        if self.debug_mode and new_threshold != current_threshold:
                            print(f"üéØ Threshold adapted for {symbol}: {current_threshold:.2f} ‚Üí {new_threshold:.2f}")
                        continue
                    # Otherwise, treat as dict of timeframes
                    for timeframe in timeframes:
                        symbol_thresholds = self._safe_get(self.threshold_adaptation, symbol, {})
                        current_threshold = self._safe_get(symbol_thresholds, timeframe, 0.25) if isinstance(symbol_thresholds, dict) else 0.45
                        recent_accuracy = self._safe_get(insights, 'recent_accuracy', 0.5)
                        if recent_accuracy > 0.7:
                            new_threshold = max(0.75, current_threshold - 0.02)
                        elif recent_accuracy < 0.4:
                            new_threshold = min(0.95, current_threshold + 0.05)
                        else:
                            new_threshold = current_threshold
                        if symbol not in self.threshold_adaptation or isinstance(self.threshold_adaptation[symbol], float):
                            self.threshold_adaptation[symbol] = {}
                        self.threshold_adaptation[symbol][timeframe] = new_threshold
                        if self.debug_mode and new_threshold != current_threshold:
                            print(f"üéØ Threshold adapted for {symbol} {timeframe}: {current_threshold:.2f} ‚Üí {new_threshold:.2f}")
                
                # Update real-time analytics
                if hasattr(self, 'real_time_analytics'):
                    self.real_time_analytics.update_system_metrics()
                
                # Update enhanced risk manager
                if hasattr(self, 'enhanced_risk_manager'):
                    self.enhanced_risk_manager.update_market_conditions()
                
                # Update adaptive risk management system
                self.update_adaptive_risk_system()
                
                self.last_learning_update = current_time
                
                if self.debug_mode:
                    print(f"üß† Enhanced learning system updated")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced learning update error: {e}")
            logging.error(f"Enhanced learning update error: {e}")

    def save_enhanced_learning_state(self, filename=None):
        """Save enhanced learning system state for persistence"""
        try:
            if filename is None:
                filename = f"enhanced_learning_state_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            
            learning_state = {
                'asset_thresholds': self.asset_thresholds,
                'confidence_calibration': self.confidence_calibration,
                'threshold_adaptation': self.threshold_adaptation,
                'timestamp': datetime.now(),
            }
            
            # Include ULTRA_FORCED pattern learning data
            if hasattr(self, 'ultra_forced_patterns'):
                learning_state['ultra_forced_patterns'] = self.ultra_forced_patterns.copy()
            
            # Include dynamic RSI thresholds
            if hasattr(self, 'dynamic_rsi_thresholds'):
                learning_state['dynamic_rsi_thresholds'] = self.dynamic_rsi_thresholds.copy()
            
            # Include adaptive risk management data
            if hasattr(self, 'asset_risk_multipliers'):
                learning_state['adaptive_risk'] = {
                    'asset_risk_multipliers': self.asset_risk_multipliers.copy(),
                    'symbol_performance_history': self.symbol_performance_history.copy(),
                    'adaptive_risk_config': self.adaptive_risk_config.copy()
                }
            
            # Include learning engine data if available
            if hasattr(self, 'advanced_learning_engine'):
                try:
                    learning_state['advanced_learning'] = {
                        'trade_outcomes': list(self.advanced_learning_engine.trade_outcomes),
                        'confidence_history': list(self.advanced_learning_engine.confidence_history),
                        'pattern_library': dict(self.advanced_learning_engine.pattern_library),
                        'feature_importance': dict(self.advanced_learning_engine.feature_importance),
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not save advanced learning state: {e}")
            
            # Include analytics data if available
            if hasattr(self, 'real_time_analytics'):
                try:
                    learning_state['analytics'] = {
                        'performance_history': list(self.real_time_analytics.performance_history),
                        'confidence_accuracy': list(self.real_time_analytics.confidence_accuracy),
                        'total_trades': self.real_time_analytics.total_trades,
                        'winning_trades': self.real_time_analytics.winning_trades,
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not save analytics state: {e}")
            
            # Include risk manager data if available
            if hasattr(self, 'enhanced_risk_manager'):
                try:
                    learning_state['risk_management'] = {
                        'portfolio_risk': self.enhanced_risk_manager.portfolio_risk,
                        'asset_correlations': dict(self.enhanced_risk_manager.asset_correlations),
                        'risk_history': list(self.enhanced_risk_manager.risk_history),
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not save risk manager state: {e}")
            
            with open(filename, 'wb') as f:
                pickle.dump(learning_state, f)
            
            print(f"üß† Enhanced learning state saved to {filename}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error saving enhanced learning state: {e}")
            logging.error(f"Enhanced learning state save error: {e}")
            return False

    def load_enhanced_learning_state(self, filename=None):
        """Load enhanced learning system state for persistence"""
        try:
            if filename is None:
                # Find the latest state file
                pattern = "enhanced_learning_state_*.pkl"
                files = glob.glob(pattern)
                if not files:
                    print("‚ÑπÔ∏è No enhanced learning state file found")
                    return False
                filename = max(files, key=os.path.getctime)
            
            if not os.path.exists(filename):
                print(f"‚ÑπÔ∏è Enhanced learning state file not found: {filename}")
                return False
            
            with open(filename, 'rb') as f:
                learning_state = pickle.load(f)
            
            # Restore basic state
            self.asset_thresholds = learning_state.get('asset_thresholds', {})
            self.confidence_calibration = learning_state.get('confidence_calibration', {})
            self.threshold_adaptation = learning_state.get('threshold_adaptation', {})
            
            # Restore ULTRA_FORCED pattern data
            if 'ultra_forced_patterns' in learning_state:
                self.ultra_forced_patterns = learning_state['ultra_forced_patterns']
                
            # Restore dynamic RSI thresholds
            if 'dynamic_rsi_thresholds' in learning_state:
                self.dynamic_rsi_thresholds = learning_state['dynamic_rsi_thresholds']
            
            # Restore adaptive risk management data
            if 'adaptive_risk' in learning_state:
                adaptive_data = learning_state['adaptive_risk']
                self.asset_risk_multipliers = adaptive_data.get('asset_risk_multipliers', {})
                self.symbol_performance_history = adaptive_data.get('symbol_performance_history', {})
                self.adaptive_risk_config = adaptive_data.get('adaptive_risk_config', {})
            
            # Restore advanced learning state
            if 'advanced_learning' in learning_state and hasattr(self, 'advanced_learning_engine'):
                try:
                    adv_data = learning_state['advanced_learning']
                    self.advanced_learning_engine.trade_outcomes = deque(
                        adv_data.get('trade_outcomes', []), maxlen=1000
                    )
                    self.advanced_learning_engine.confidence_history = deque(
                        adv_data.get('confidence_history', []), maxlen=500
                    )
                    self.advanced_learning_engine.pattern_library.update(
                        adv_data.get('pattern_library', {})
                    )
                    self.advanced_learning_engine.feature_importance.update(
                        adv_data.get('feature_importance', {})
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not restore advanced learning state: {e}")
            
            # Restore analytics state
            if 'analytics' in learning_state and hasattr(self, 'real_time_analytics'):
                try:
                    analytics_data = learning_state['analytics']
                    self.real_time_analytics.performance_history = deque(
                        analytics_data.get('performance_history', []), maxlen=500
                    )
                    self.real_time_analytics.confidence_accuracy = deque(
                        analytics_data.get('confidence_accuracy', []), maxlen=200
                    )
                    self.real_time_analytics.total_trades = analytics_data.get('total_trades', 0)
                    self.real_time_analytics.winning_trades = analytics_data.get('winning_trades', 0)
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not restore analytics state: {e}")
            
            # Restore risk manager state  
            if 'risk_management' in learning_state and hasattr(self, 'enhanced_risk_manager'):
                try:
                    risk_data = learning_state['risk_management']
                    self.enhanced_risk_manager.portfolio_risk = risk_data.get('portfolio_risk', 0.0)
                    self.enhanced_risk_manager.asset_correlations.update(
                        risk_data.get('asset_correlations', {})
                    )
                    self.enhanced_risk_manager.risk_history = deque(
                        risk_data.get('risk_history', []), maxlen=100
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not restore risk manager state: {e}")
            
            print(f"üß† Enhanced learning state loaded from {filename}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading enhanced learning state: {e}")
            logging.error(f"Enhanced learning state load error: {e}")
            return False

    def generate_enhanced_learning_report(self):
        """Generate comprehensive enhanced learning performance report"""
        try:
            print("\n" + "="*60)
            print("üß† ENHANCED LEARNING SYSTEM REPORT")
            print("="*60)
            
            # Basic system status
            print(f"üìä System Status:")
            print(f"   Learning Active: {'‚úÖ' if getattr(self, 'learning_active', False) else '‚ùå'}")
            print(f"   Last Update: {datetime.fromtimestamp(self.last_learning_update).strftime('%H:%M:%S')}")
            print(f"   Update Interval: {self.learning_update_interval//60} minutes")
            
            # Advanced Learning Engine Report
            if hasattr(self, 'advanced_learning_engine'):
                engine = self.advanced_learning_engine
                insights = engine.get_trading_insights()
                
                print(f"\nüéØ Advanced Learning Engine:")
                print(f"   Total Trades Analyzed: {len(engine.trade_outcomes)}")
                print(f"   Recent Win Rate: {insights.get('recent_win_rate', 0):.1%}")
                print(f"   Recent Accuracy: {insights.get('recent_accuracy', 0):.1%}")
                print(f"   Confidence Calibration Quality: {insights.get('calibration_quality', 'Unknown')}")
                print(f"   Pattern Library Size: {len(engine.pattern_library)}")
                
                # Feature importance
                if engine.feature_importance:
                    print(f"\nüìà Top 5 Most Important Features:")
                    sorted_features = sorted(engine.feature_importance.items(), 
                                           key=lambda x: x[1], reverse=True)[:5]
                    for i, (feature, importance) in enumerate(sorted_features, 1):
                        print(f"   {i}. {feature}: {importance:.3f}")
            
            # Real-Time Analytics Report
            if hasattr(self, 'real_time_analytics'):
                analytics = self.real_time_analytics
                
                print(f"\nüìä Real-Time Analytics:")
                print(f"   Total Trades: {analytics.total_trades}")
                print(f"   Winning Trades: {analytics.winning_trades}")
                print(f"   Current Win Rate: {(analytics.winning_trades/max(analytics.total_trades,1)):.1%}")
                print(f"   Current Drawdown: {analytics.current_drawdown:.2%}")
                print(f"   Max Drawdown: {analytics.max_drawdown:.2%}")
                
                if analytics.performance_history:
                    recent_performance = list(analytics.performance_history)[-10:]
                    avg_recent = sum(recent_performance) / len(recent_performance)
                    print(f"   Recent Performance (last 10): {avg_recent:.2%}")
                
                if analytics.confidence_accuracy:
                    recent_accuracy = list(analytics.confidence_accuracy)[-20:]
                    avg_accuracy = sum(recent_accuracy) / len(recent_accuracy)
                    print(f"   Confidence Accuracy (last 20): {avg_accuracy:.1%}")
            
            # Enhanced Risk Manager Report
            if hasattr(self, 'enhanced_risk_manager'):
                risk_mgr = self.enhanced_risk_manager
                
                print(f"\nüõ°Ô∏è Enhanced Risk Manager:")
                print(f"   Current Portfolio Risk: {risk_mgr.portfolio_risk:.2%}")
                print(f"   Risk History Length: {len(risk_mgr.risk_history)}")
                print(f"   Asset Correlations Tracked: {len(risk_mgr.asset_correlations)}")
                
                if risk_mgr.risk_history:
                    recent_risk = list(risk_mgr.risk_history)[-5:]
                    avg_risk = sum(recent_risk) / len(recent_risk)
                    print(f"   Average Recent Risk: {avg_risk:.2%}")
            
            # Threshold Adaptation Report
            print(f"\nüéØ Dynamic Thresholds:")
            if self.threshold_adaptation:
                for symbol in self.threshold_adaptation:
                    print(f"   {symbol}:")
                    for timeframe, threshold in self.threshold_adaptation[symbol].items():
                        print(f"      {timeframe}: {threshold:.1%}")
            else:
                print("   No adaptive thresholds set")
            
            # Asset Performance Summary
            if hasattr(self, 'asset_thresholds') and self.asset_thresholds:
                print(f"\nüí∞ Asset-Specific Thresholds:")
                for symbol in self.asset_thresholds:
                    print(f"   {symbol}:")
                    for timeframe, threshold in self.asset_thresholds[symbol].items():
                        print(f"      {timeframe}: {threshold:.1%}")
            
            # ULTRA_FORCED Pattern Learning Report
            if hasattr(self, 'ultra_forced_patterns'):
                patterns = self.ultra_forced_patterns
                overall_uf_wr = patterns['winning_ultra_trades'] / max(patterns['total_ultra_trades'], 1)
                
                print(f"\nüéØ ULTRA_FORCED Pattern Learning:")
                print(f"   Total ULTRA_FORCED Trades: {patterns['total_ultra_trades']}")
                print(f"   Overall Win Rate: {overall_uf_wr:.1%}")
                print(f"   Last Optimization: {patterns['last_optimization'].strftime('%H:%M:%S')}")
                
                # Top RSI ranges
                if patterns['rsi_buckets']:
                    print(f"   üìà Top RSI Ranges by Win Rate:")
                    sorted_rsi = sorted(patterns['rsi_buckets'].items(), 
                                      key=lambda x: x[1]['win_rate'], reverse=True)[:3]
                    for i, (bucket, data) in enumerate(sorted_rsi, 1):
                        if data['trades'] >= 5:
                            print(f"      {i}. {bucket}: {data['win_rate']:.1%} ({data['trades']} trades)")
                
                # Current optimized thresholds
                if hasattr(self, 'dynamic_rsi_thresholds'):
                    thresholds = self.dynamic_rsi_thresholds
                    print(f"   üß† Optimized RSI Thresholds:")
                    print(f"      Oversold: {thresholds['oversold_min']}-{thresholds['oversold_max']}")
                    print(f"      Overbought: {thresholds['overbought_min']}-{thresholds['overbought_max']}")
                    print(f"      Optimizations: {thresholds['optimization_count']}")
            
            # Adaptive Risk Management Report
            if hasattr(self, 'symbol_performance_history'):
                print("\nüéØ ADAPTIVE RISK MANAGEMENT")
                print("-" * 40)
                for symbol, data in self.symbol_performance_history.items():
                    if data.get('total_trades', 0) > 0:
                        streak_type = data.get('streak_type', 'NEUTRAL')
                        multiplier = data.get('multiplier', 1.0)
                        
                        # Status icon based on performance
                        if data.get('win_rate', 0) >= 0.6:
                            status = "üî• HOT"
                        elif data.get('win_rate', 0) <= 0.3:
                            status = "‚ùÑÔ∏è COLD"
                        else:
                            status = "üîÑ NORMAL"
                        
                        print(f"   {symbol} {status}")
                        print(f"      Recent: {data.get('win_rate', 0):.1%} ({data.get('total_trades', 0)} trades)")
                        print(f"      Streak: {streak_type}")
                        print(f"      Position Multiplier: {multiplier:.2f}x")
                        print()
            
            print("\n" + "="*60)
            print("üìà ENHANCED LEARNING REPORT COMPLETE")
            print("="*60)
            
        except Exception as e:
            print(f"‚ùå Error generating enhanced learning report: {e}")
            logging.error(f"Enhanced learning report error: {e}")

    def load_mega_dataset(self, data_dir='dataFiles', asset_mapping=None, default_timeframe='M15'):
        """Enhanced dataset loading with comprehensive validation"""
        try:
            print("üìä Loading and validating dataset...")
            
            # Default asset mapping
            if asset_mapping is None:
                asset_mapping = {
                    'EURUSD': 'EURUSDm',
                    'XAU': 'XAUUSDm',
                    'GOLD': 'XAUUSDm',
                    'GBPUSD': 'GBPUSDm',
                    'USDJPY': 'USDJPYm',
                    'BTCUSD': 'BTCUSDm',
                    'BTC': 'BTCUSDm'
                }
            
            # Find CSV files
            csv_files = list(Path(data_dir).glob('*.[cC][sS][vV]'))
            
            if not csv_files:
                print(f"‚ùå No CSV files found in {data_dir}")
                return False
            
            print(f"üìÅ Found {len(csv_files)} CSV files")
            
            dfs = []
            loaded_count = 0
            
            for file_path in sorted(csv_files):
                try:
                    # Load with encoding detection
                    df = self._load_csv_robust(file_path)
                    if df is None:
                        continue
                    
                    # Validate and clean data
                    df = self._validate_and_clean_dataframe(df)
                    if df is None:
                        continue
                    
                    # Assign asset and timeframe
                    df = self._assign_asset_and_timeframe(df, file_path, asset_mapping, default_timeframe)
                    if df is None:
                        continue
                    
                    dfs.append(df)
                    loaded_count += 1
                    
                    if self.debug_mode:
                        print(f"‚úÖ Loaded {file_path.name}: {len(df)} rows")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing {file_path.name}: {e}")
                    continue
            
            if not dfs:
                print("‚ùå No valid datasets loaded")
                return False
            
            # Combine datasets
            self.mega_dataset = pd.concat(dfs, ignore_index=True, sort=False)
            
            # Post-processing
            self._post_process_dataset()
            
            # Final validation
            if not self._validate_final_dataset():
                return False
            
            print(f"‚úÖ Dataset loaded successfully: {len(self.mega_dataset):,} records")
            self._print_dataset_summary()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Dataset loading failed: {e}")
            self.logger.error(f"Dataset loading error: {e}")
            return False

    def _load_csv_robust(self, file_path):
        """Robust CSV loading with multiple encoding attempts"""
        encodings = ['utf-16', 'utf-8', 'latin1', 'windows-1252', 'cp1252']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding, on_bad_lines='warn')
                if len(df) > 0:
                    return df
            except Exception as e:
                continue
        
        print(f"‚ö†Ô∏è Could not load {file_path.name} with any encoding")
        return None

    def _validate_and_clean_dataframe(self, df):
        """Validate and clean dataframe"""
        try:
            # Normalize column names
            df.columns = df.columns.str.strip().str.lower()
            
            # Check minimum size
            min_data_points = self.config['data_settings']['min_data_points']
            if isinstance(min_data_points, dict):
                # If dict, use default or asset-specific value
                min_data_points = min_data_points.get('default', 100)
            if len(df) < min_data_points:
                return None
            
            # Ensure required columns exist
            required_cols = ['setuptime', 'hittp']  # Minimum required
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                print(f"‚ö†Ô∏è Missing required columns: {missing_cols}")
                return None
            
            # Clean data types
            if 'setuptime' in df.columns:
                df['setuptime'] = pd.to_datetime(df['setuptime'], errors='coerce')
                df = df[df['setuptime'].notna()]
            
            # Ensure target column is binary
            if 'hittp' in df.columns:
                df['hittp'] = df['hittp'].fillna(0).astype(int)
                # Remove invalid target values
                df = df[df['hittp'].isin([0, 1])]
            
            # Clean technical indicators
            technical_cols = ['atr', 'rsi', 'volumeratio', 'leveltouches']
            for col in technical_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    df[col] = df[col].fillna(df[col].median() if not df[col].isna().all() else 1.0)
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Data validation error: {e}")
            return None

    def _assign_asset_and_timeframe(self, df, file_path, asset_mapping, default_timeframe):
        """Assign asset and timeframe to dataframe"""
        try:
            file_upper = file_path.name.upper()
            
            # Assign asset
            asset_assigned = False
            for pattern, asset in asset_mapping.items():
                if pattern.upper() in file_upper:
                    df['asset'] = asset
                    asset_assigned = True
                    break
            
            if not asset_assigned:
                if 'symbol' in df.columns:
                    df['asset'] = df['symbol'].iloc[0]
                else:
                    print(f"‚ö†Ô∏è Could not determine asset for {file_path.name}")
                    return None
            
            # Assign timeframe
            timeframe = default_timeframe
            timeframe_match = re.search(r'PERIOD_([MH]\d+)', file_upper)
            if timeframe_match:
                timeframe = timeframe_match.group(1)
            elif 'timeframe' in df.columns:
                timeframe = df['timeframe'].iloc[0]
            
            df['timeframe'] = timeframe
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Asset/timeframe assignment error: {e}")
            return None

    def _post_process_dataset(self):
        """Post-process the combined dataset"""
        try:
            # Sort by time if available
            if 'setuptime' in self.mega_dataset.columns:
                self.mega_dataset = self.mega_dataset.sort_values('setuptime')
            
            # Create summary statistics
            self.available_assets = sorted(self.mega_dataset['asset'].unique())
            self.available_timeframes = sorted(self.mega_dataset['timeframe'].unique())
            
            # Remove any potential data leakage columns
            leakage_patterns = ['hitsl', 'exit', 'final', 'outcome', 'result']
            cols_to_drop = []
            
            for col in self.mega_dataset.columns:
                if any(pattern in col.lower() for pattern in leakage_patterns):
                    if col.lower() != 'hittp':  # Keep the target
                        cols_to_drop.append(col)
            
            if cols_to_drop:
                self.mega_dataset = self.mega_dataset.drop(columns=cols_to_drop)
                print(f"‚ö†Ô∏è Removed potential leakage columns: {cols_to_drop}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Post-processing error: {e}")

    def validate_data_for_ultra_forced(self, market_data, timeframe):
        """Validate sufficient data for ULTRA_FORCED calculations"""
        
        min_requirements = {
            'M5': 150,   # Absolute minimum for M5
            'M15': 100,  # Absolute minimum for M15  
            'M30': 80,   # Absolute minimum for M30
            'H1': 60     # Absolute minimum for H1
        }
        
        required = min_requirements.get(timeframe, 100)
        actual = len(market_data) if market_data is not None else 0
        
        if actual < required:
            return False, f"Need {required} bars, got {actual}"
        
        # Check for data gaps
        if 'time' in market_data.columns:
            time_diffs = market_data['time'].diff()
            large_gaps = (time_diffs > pd.Timedelta(hours=4)).sum()
            if large_gaps > 2:
                return False, f"Too many data gaps: {large_gaps}"
        
        return True, "Sufficient data for ULTRA_FORCED analysis"

    def _validate_final_dataset(self):
        """Final dataset validation"""
        try:
            # Check target distribution
            target_dist = self.mega_dataset['hittp'].value_counts()
            positive_ratio = target_dist.get(1, 0) / len(self.mega_dataset)
            
            if positive_ratio < 0.1 or positive_ratio > 0.9:
                print(f"‚ö†Ô∏è Suspicious target distribution: {positive_ratio:.1%} positive")
                return False
            
            # Check for sufficient data per asset
            asset_counts = self.mega_dataset['asset'].value_counts()
            min_samples = self.config['data_settings']['min_data_points']
            if isinstance(min_samples, dict):
                min_samples_val = min_samples.get('default', 100)
            else:
                min_samples_val = min_samples
            insufficient_assets = asset_counts[asset_counts < min_samples_val]
            if len(insufficient_assets) > 0:
                print(f"‚ö†Ô∏è Assets with insufficient data: {insufficient_assets.to_dict()}")
            
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Final validation error: {e}")
            return False

    def _print_dataset_summary(self):
        """Print comprehensive dataset summary"""
        print("\nüìä DATASET SUMMARY")
        print("=" * 50)
        print(f"Total Records: {len(self.mega_dataset):,}")
        
        if 'setuptime' in self.mega_dataset.columns:
            print(f"Date Range: {self.mega_dataset['setuptime'].min()} to {self.mega_dataset['setuptime'].max()}")
        
        print(f"Assets: {', '.join(self.available_assets)}")
        print(f"Timeframes: {', '.join(self.available_timeframes)}")
        
        # Target distribution
        target_dist = self.mega_dataset['hittp'].value_counts()
        print(f"Target Distribution: {target_dist.to_dict()}")
        
        # Asset distribution
        asset_dist = self.mega_dataset['asset'].value_counts()
        print(f"Asset Distribution:")
        for asset, count in asset_dist.items():
            print(f"  {asset}: {count:,} ({count/len(self.mega_dataset):.1%})")

    def engineer_leak_free_features(self):
        """Enhanced feature engineering with strict leak prevention"""
        print("üîß Engineering leak-free features...")
        
        try:
            features_df = self.mega_dataset.copy()
            
            # Validate no leakage columns exist
            prohibited_patterns = ['hit', 'exit', 'final', 'outcome', 'result', 'future']
            for col in features_df.columns:
                if col.lower() != 'hittp' and any(pattern in col.lower() for pattern in prohibited_patterns):
                    raise ValueError(f"Potential data leakage detected in column: {col}")
            
            # Basic technical features
            features_df = self._add_technical_features(features_df)
            
            # Time-based features
            features_df = self._add_temporal_features(features_df)
            
            # Asset and timeframe features
            features_df = self._add_categorical_features(features_df)
            
            # Market regime features
            features_df = self._add_market_regime_features(features_df)
            
            # Interaction features
            features_df = self._add_interaction_features(features_df)
            
            # Enhanced timeframe-specific features
            features_df = self._add_enhanced_timeframe_features(features_df)
            
            # Select feature columns (exclude target and metadata)
            exclude_cols = ['hittp', 'setuptime', 'asset', 'timeframe'] + [col for col in features_df.columns if 'unnamed' in col.lower()]
            self.feature_columns = [col for col in features_df.columns if col not in exclude_cols]
            
            # Extract features and target
            X = features_df[self.feature_columns]
            y = features_df['hittp'].fillna(0).astype(int)
            
            print(f"‚úÖ Features engineered: {len(self.feature_columns)} features")
            print(f"üìä Feature matrix shape: {X.shape}")
            print(f"üéØ Target shape: {y.shape}")
            
            # Final validation
            self._validate_features(X)
            
            return X, self.feature_columns, y
            
        except Exception as e:
            print(f"‚ùå Feature engineering failed: {e}")
            raise

    def _add_technical_features(self, df):
        """Add technical indicator features"""
        # Ensure technical columns exist with defaults
        technical_defaults = {
            'atr': 0.001,
            'rsi': 50.0,
            'volumeratio': 1.0,
            'leveltouches': 2
        }
        
        for col, default_val in technical_defaults.items():
            if col not in df.columns:
                df[col] = default_val
            else:
                df[col] = df[col].fillna(default_val)
        
        # Add derived technical features
        df['atr_normalized'] = df['atr'] / df.groupby('asset')['atr'].transform('median')
        df['rsi_extreme'] = ((df['rsi'] < 30) | (df['rsi'] > 70)).astype(int)
        df['volume_extreme'] = (df['volumeratio'] > 2.0).astype(int)
        
        return df

    def _add_temporal_features(self, df):
        """Add time-based features"""
        if 'setuptime' in df.columns:
            df['hour'] = df['setuptime'].dt.hour
            df['day_of_week'] = df['setuptime'].dt.dayofweek
            df['month'] = df['setuptime'].dt.month
            
            # Cyclical encoding
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
            df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
            
            # Trading sessions
            df['london_session'] = ((df['hour'] >= 8) & (df['hour'] <= 16)).astype(int)
            df['newyork_session'] = ((df['hour'] >= 13) & (df['hour'] <= 21)).astype(int)
            df['asian_session'] = ((df['hour'] >= 0) & (df['hour'] <= 8)).astype(int)
            df['overlap_session'] = ((df['hour'] >= 13) & (df['hour'] <= 16)).astype(int)
        else:
            # Default time features if no timestamp
            for feature in ['hour', 'day_of_week', 'month', 'hour_sin', 'hour_cos', 
                          'day_sin', 'day_cos', 'month_sin', 'month_cos',
                          'london_session', 'newyork_session', 'asian_session', 'overlap_session']:
                df[feature] = 0
        
        return df

    def _add_categorical_features(self, df):
        """Add asset and timeframe features"""
        # Asset features
        for asset in ['EURUSDm', 'XAUUSDm', 'GBPUSDm', 'BTCUSDm']:
            df[f'is_{asset.lower()}'] = (df['asset'] == asset).astype(int)
        
        # Timeframe features
        for tf in ['M5', 'M15', 'M30', 'H1']:
            df[f'tf_{tf.lower()}'] = (df['timeframe'] == tf).astype(int)
        
        return df

    def _add_market_regime_features(self, df):
        """Add market regime features"""
        # Volatility regimes
        atr_75 = df['atr'].quantile(0.75)
        atr_25 = df['atr'].quantile(0.25)
        
        df['high_volatility'] = (df['atr'] > atr_75).astype(int)
        df['low_volatility'] = (df['atr'] < atr_25).astype(int)
        df['medium_volatility'] = ((df['atr'] >= atr_25) & (df['atr'] <= atr_75)).astype(int)
        
        # RSI regimes
        df['rsi_oversold'] = (df['rsi'] < 30).astype(int)
        df['rsi_overbought'] = (df['rsi'] > 70).astype(int)
        df['rsi_neutral'] = ((df['rsi'] >= 40) & (df['rsi'] <= 60)).astype(int)
        
        # Volume regimes
        df['high_volume'] = (df['volumeratio'] > 1.5).astype(int)
        df['low_volume'] = (df['volumeratio'] < 0.8).astype(int)
        
        return df

    def _add_interaction_features(self, df):
        """Add interaction features"""
        df['atr_rsi_interaction'] = df['atr'] * df['rsi']
        df['volume_level_interaction'] = df['volumeratio'] * df['leveltouches']
        df['volatility_session_interaction'] = df['high_volatility'] * df['london_session']
        
        return df
        
    def _add_enhanced_timeframe_features(self, df):
        """Add enhanced timeframe-specific features based on the timeframe context
        
        This creates specialized features that perform well for each timeframe context.
        Different timeframes respond to different patterns and this captures that.
        """
        # Ensure we have timeframe column
        if 'timeframe' not in df.columns:
            return df
            
        # Create timeframe groups for specialized feature engineering
        timeframes = df['timeframe'].unique()
        
        for tf in timeframes:
            # Get mask for this timeframe
            tf_mask = df['timeframe'] == tf
            
            # Skip if no data for this timeframe
            if not tf_mask.any():
                continue
                
            # Create empty columns for timeframe-specific features
            df[f'{tf}_volatility_ratio'] = 0.0
            df[f'{tf}_momentum_strength'] = 0.0
            df[f'{tf}_session_performance'] = 0.0
            
            # Calculate ATR ratio specific to this timeframe
            if 'atr' in df.columns:
                tf_median_atr = df.loc[tf_mask, 'atr'].median()
                if tf_median_atr > 0:
                    df.loc[tf_mask, f'{tf}_volatility_ratio'] = df.loc[tf_mask, 'atr'] / tf_median_atr
                    
            # Calculate RSI momentum strength specific to this timeframe
            if 'rsi' in df.columns:
                # Transform RSI to a momentum strength indicator (distance from neutral 50)
                df.loc[tf_mask, f'{tf}_momentum_strength'] = abs(df.loc[tf_mask, 'rsi'] - 50) / 50.0
                
            # Create session performance feature specific to this timeframe
            if all(col in df.columns for col in ['london_session', 'newyork_session', 'asian_session']):
                # Weight sessions differently based on timeframe performance characteristics
                if tf == 'M5':
                    # M5 often performs better in high volatility sessions
                    df.loc[tf_mask, f'{tf}_session_performance'] = (
                        df.loc[tf_mask, 'london_session'] * 1.5 + 
                        df.loc[tf_mask, 'newyork_session'] * 1.2 + 
                        df.loc[tf_mask, 'asian_session'] * 0.7
                    ) / 3.4  # Normalize by sum of weights
                elif tf == 'M15':
                    # M15 balanced across sessions
                    df.loc[tf_mask, f'{tf}_session_performance'] = (
                        df.loc[tf_mask, 'london_session'] * 1.2 + 
                        df.loc[tf_mask, 'newyork_session'] * 1.3 + 
                        df.loc[tf_mask, 'asian_session'] * 0.8
                    ) / 3.3
                elif tf == 'M30':
                    # M30 performs well in transition periods
                    df.loc[tf_mask, f'{tf}_session_performance'] = (
                        df.loc[tf_mask, 'london_session'] * 1.3 + 
                        df.loc[tf_mask, 'newyork_session'] * 1.4 + 
                        df.loc[tf_mask, 'asian_session'] * 0.6
                    ) / 3.3
                elif tf == 'H1':
                    # H1 often captures major session moves
                    df.loc[tf_mask, f'{tf}_session_performance'] = (
                        df.loc[tf_mask, 'london_session'] * 1.4 + 
                        df.loc[tf_mask, 'newyork_session'] * 1.5 + 
                        df.loc[tf_mask, 'asian_session'] * 0.5
                    ) / 3.4
        
        return df

    def _validate_features(self, X):
        """Simple feature validation"""
        # Check for data leakage
        prohibited_patterns = ['hit', 'exit', 'final', 'outcome', 'result']
        leakage_features = [f for f in X.columns if any(p in f.lower() for p in prohibited_patterns)]
        
        if leakage_features:
            raise ValueError(f"Data leakage detected: {leakage_features}")
        
        # Simple cleaning
        X = X.fillna(0)
        X = X.replace([np.inf, -np.inf], 0)
        
        print("‚úÖ Feature validation passed")
        return X
    def load_production_models(self, model_file='production_models_latest.pkl'):
        """Load trained models for production use"""
        try:
            if not Path(model_file).exists():
                print(f"‚ö†Ô∏è Model file {model_file} not found")
                return False
            
            print(f"üì¶ Loading production models from {model_file}...")
            
            with self.model_lock:
                model_package = joblib.load(model_file)
                
                # Extract components
                self.models = model_package.get('models', {})
                self.feature_scaler = model_package.get('scaler', StandardScaler())
                self.feature_columns = model_package.get('feature_columns', [])
                self.validation_results = model_package.get('validation_results', {})
                
                # Assign online model as adaptive_learner if not present
                if 'adaptive_learner' not in self.models or self.models['adaptive_learner'] is None:
                    self.models['adaptive_learner'] = self.online_model
                    if self.debug_mode:
                        print("üîß Assigned online_model as adaptive_learner")
                        
                    # Initialize adaptive_learner if not already initialized
                    if not self.online_model_initialized and hasattr(self, 'online_model'):
                        try:
                            # Create minimal synthetic data for initialization
                            features_array = np.zeros((2, len(self.feature_columns)))
                            target_array = np.array([0, 1])  # One example of each class
                            self.online_model.partial_fit(features_array, target_array, classes=np.array([0, 1]))
                            self.online_model_initialized = True
                            if self.debug_mode:
                                print("‚úÖ Initialized adaptive_learner with synthetic data")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Could not initialize adaptive_learner: {e}")
                
                # Update metadata
                self.model_metadata.update({
                    'last_loaded': datetime.now(),
                    'training_timestamp': model_package.get('training_timestamp'),
                    'validation_scores': self.validation_results,
                    'model_count': len(self.models)
                })
            
            print(f"‚úÖ Loaded {len(self.models)} models successfully")
            print(f"üîß Feature columns: {len(self.feature_columns)}")
            
            # Validate models
            return self._validate_loaded_models()
            
        except Exception as e:
            print(f"‚ùå Failed to load models: {e}")
            self.logger.error(f"Model loading error: {e}")
            return False

    def _validate_loaded_models(self):
        """Validate loaded models"""
        try:
            valid_models = 0
            
            for model_name, model in self.models.items():
                if model is None:
                    print(f"‚ö†Ô∏è Model {model_name} is None")
                    continue
                
                if not hasattr(model, 'predict'):
                    print(f"‚ö†Ô∏è Model {model_name} missing predict method")
                    continue
                
                # Check validation scores
                if model_name in self.validation_results:
                    accuracy = self.validation_results[model_name].get('accuracy', 0)
                    min_accuracy = self.config['ml_settings']['model_validation_threshold']
                    
                    if accuracy < min_accuracy:
                        print(f"‚ö†Ô∏è Model {model_name} accuracy {accuracy:.3f} below threshold {min_accuracy}")
                        continue
                
                valid_models += 1
                print(f"‚úÖ Model {model_name} validated")
            
            if valid_models == 0:
                print("‚ùå No valid models found")
                return False
            
            print(f"‚úÖ {valid_models}/{len(self.models)} models validated successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Model validation failed: {e}")
            return False

    def connect_to_metatrader(self):
        """Enhanced MetaTrader connection with validation"""
        try:
            if not mt5:
                print("‚ùå MetaTrader5 package not available")
                return False
            
            print("üåê Connecting to MetaTrader 5...")
            
            if not mt5.initialize():
                print("‚ùå MetaTrader 5 initialization failed")
                error = mt5.last_error()
                print(f"Error: {error}")
                return False
            
            # Login with account credentials
            account_info = self.config['account']
            login_result = mt5.login(
                account_info['login'],
                account_info['password'],
                account_info['server']
            )
            
            if login_result:
                account = mt5.account_info()
                print(f"‚úÖ Connected to MetaTrader 5")
                print(f"üìä Account: {account.login}")
                print(f"üí∞ Balance: ${account.balance:.2f}")
                print(f"üíµ Equity: ${account.equity:.2f}")
                print(f"üè¢ Server: {account.server}")
                
                # Validate account conditions
                if account.balance < self.min_account_balance:
                    print(f"‚ö†Ô∏è Balance ${account.balance:.2f} below minimum ${self.min_account_balance:.2f}")
                    print("üîß Consider adjusting position sizing")
                
                # Check trading permissions
                if account.trade_allowed:
                    print("‚úÖ Trading is allowed")
                else:
                    print("‚ö†Ô∏è Trading is not allowed on this account")
                
                return True
            else:
                print("‚ùå MetaTrader 5 login failed")
                error = mt5.last_error()
                print(f"Error: {error}")
                return False
                
        except Exception as e:
            print(f"‚ùå MetaTrader connection error: {e}")
            self.logger.error(f"MT5 connection error: {e}")
            return False

    def get_live_market_data(self, symbol, timeframe, bars=None):
        """Enhanced with timeframe-specific bar counts"""
        try:
            # Timeframe mapping
            tf_map = {
                'M1': mt5.TIMEFRAME_M1,
                'M5': mt5.TIMEFRAME_M5,
                'M15': mt5.TIMEFRAME_M15,
                'M30': mt5.TIMEFRAME_M30,
                'H1': mt5.TIMEFRAME_H1,
                'H4': mt5.TIMEFRAME_H4,
                'D1': mt5.TIMEFRAME_D1
            }
            
            # Optimized bar counts for ULTRA_FORCED strategy
            if bars is None:
                optimal_bars = {
                    'M5': 200,   # 16+ hours for intraday patterns
                    'M15': 120,  # 30+ hours for daily patterns  
                    'M30': 100,  # 50+ hours for multi-day patterns
                    'H1': 80,    # 3+ days for weekly patterns
                    'H4': 60,    # 10+ days for trend context
                    'D1': 50     # 50+ days for long-term context
                }
                bars = optimal_bars.get(timeframe, 120)  # Default to 120
            
            if timeframe not in tf_map:
                print(f"‚ö†Ô∏è Unsupported timeframe: {timeframe}")
                return None
                
            print(f"üìä Requesting {bars} bars for {symbol} {timeframe}")
            
            # Try multiple symbol variants
            symbol_variants = [symbol]
            if 'XAU' in symbol:
                symbol_variants.extend(['XAUUSDm', 'GOLD', 'XAUUSDm.', 'GOLD.'])
            elif 'EUR' in symbol:
                symbol_variants.extend(['EURUSDm', 'EURUSDm.'])
            
            for test_symbol in symbol_variants:
                # Check symbol availability
                symbol_info = mt5.symbol_info(test_symbol)
                if not symbol_info:
                    continue
                
                if not symbol_info.visible:
                    # Try to make symbol visible
                    if not mt5.symbol_select(test_symbol, True):
                        continue
                
                # Get market data
                rates = mt5.copy_rates_from_pos(test_symbol, tf_map[timeframe], 0, bars)
                
                # Enhanced validation
                if rates is None or len(rates) < bars * 0.8:  # Allow 20% tolerance
                    print(f"‚ö†Ô∏è Got {len(rates) if rates else 0} bars, requested {bars}")
                    continue  # Try next symbol variant if available
                
                df = pd.DataFrame(rates)
                df['time'] = pd.to_datetime(df['time'], unit='s')
                
                # Add symbol and timeframe info
                df['symbol'] = test_symbol
                df['timeframe'] = timeframe
                
                if self.debug_mode:
                    print(f"‚úÖ Market data: {test_symbol} {timeframe} - {len(rates)} bars")
                
                return df
            
            if self.debug_mode:
                print(f"‚ùå No market data available for {symbol} {timeframe}")
            
            return None
            
        except Exception as e:
            print(f"‚ùå Market data error: {e}")
            self.logger.error(f"Market data error for {symbol} {timeframe}: {e}")
            return None

    def calculate_live_features(self, symbol, timeframe, market_data):
        """Calculate features for live market data"""
        try:
            # Validate data sufficiency for ULTRA_FORCED strategy
            is_sufficient, reason = self.validate_data_for_ultra_forced(market_data, timeframe)
            if not is_sufficient:
                print(f"‚ùå {symbol} {timeframe}: {reason}")
                return None
            
            if market_data is None or len(market_data) < 10:
                return None
            
            features = {}
            data = market_data.copy()
            
            # Technical indicators
            # ATR calculation
            try:
                data['tr'] = np.maximum(
                    data['high'] - data['low'],
                    np.maximum(
                        abs(data['high'] - data['close'].shift(1)),
                        abs(data['low'] - data['close'].shift(1))
                    )
                )
                atr = data['tr'].rolling(14).mean().iloc[-1]
                features['atr'] = atr if not pd.isna(atr) else 0.001
            except:
                features['atr'] = 0.001
            
            # RSI calculation
            try:
                delta = data['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                rs = gain / loss
                rsi = 100 - (100 / (1 + rs))
                features['rsi'] = rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50.0
            except:
                features['rsi'] = 50.0
            
            # Volume ratio
            if 'tick_volume' in data.columns:
                try:
                    avg_volume = data['tick_volume'].rolling(20).mean().iloc[-1]
                    current_volume = data['tick_volume'].iloc[-1]
                    features['volumeratio'] = current_volume / avg_volume if avg_volume > 0 else 1.0
                except:
                    features['volumeratio'] = 1.0
            else:
                features['volumeratio'] = 1.0
            
            # Level touches (placeholder)
            features['leveltouches'] = 2
            
            # Time features
            current_time = datetime.now()
            features['hour'] = current_time.hour
            features['day_of_week'] = current_time.weekday()
            features['month'] = current_time.month
            
            # Cyclical encoding
            features['hour_sin'] = np.sin(2 * np.pi * current_time.hour / 24)
            features['hour_cos'] = np.cos(2 * np.pi * current_time.hour / 24)
            features['day_sin'] = np.sin(2 * np.pi * current_time.weekday() / 7)
            features['day_cos'] = np.cos(2 * np.pi * current_time.weekday() / 7)
            features['month_sin'] = np.sin(2 * np.pi * current_time.month / 12)
            features['month_cos'] = np.cos(2 * np.pi * current_time.month / 12)
            
            # Session features
            hour = current_time.hour
            features['london_session'] = 1 if 8 <= hour <= 16 else 0
            features['newyork_session'] = 1 if 13 <= hour <= 21 else 0
            features['asian_session'] = 1 if 0 <= hour <= 8 else 0
            features['overlap_session'] = 1 if 13 <= hour <= 16 else 0
            
            # Asset features
            features['is_eurusdm'] = 1 if symbol == 'EURUSDm' else 0
            features['is_xauusdm'] = 1 if symbol == 'XAUUSDm' else 0
            features['is_gbpusdm'] = 1 if symbol == 'GBPUSDm' else 0
            features['is_btcusdm'] = 1 if symbol == 'BTCUSDm' else 0
            
            # Timeframe features
            tf_features = {'tf_m5': 0, 'tf_m15': 0, 'tf_m30': 0, 'tf_h1': 0}
            tf_features[f'tf_{timeframe.lower()}'] = 1
            features.update(tf_features)
            
            # Enhanced timeframe-specific features
            # Store timeframe for calibration later
            features['timeframe'] = timeframe
            
            # Add timeframe-specific volatility ratio
            if 'atr' in features:
                # Use timeframe-specific median ATR values (estimated from historical data)
                tf_median_atr = {
                    'M5': 0.0005 if not 'XAU' in symbol else 0.4,
                    'M15': 0.001 if not 'XAU' in symbol else 0.8,
                    'M30': 0.0015 if not 'XAU' in symbol else 1.2,
                    'H1': 0.002 if not 'XAU' in symbol else 1.6
                }.get(timeframe, 0.001)
                
                # Calculate timeframe-specific volatility ratio
                features[f'{timeframe}_volatility_ratio'] = features['atr'] / tf_median_atr
            else:
                features[f'{timeframe}_volatility_ratio'] = 1.0
                
            # Calculate RSI momentum strength specific to this timeframe
            if 'rsi' in features:
                # Transform RSI to a momentum strength indicator (distance from neutral 50)
                features[f'{timeframe}_momentum_strength'] = abs(features['rsi'] - 50) / 50.0
            else:
                features[f'{timeframe}_momentum_strength'] = 0.0
                
            # Create session performance feature specific to this timeframe
            if all(col in features for col in ['london_session', 'newyork_session', 'asian_session']):
                # Weight sessions differently based on timeframe performance characteristics
                if timeframe == 'M5':
                    # M5 often performs better in high volatility sessions
                    features[f'{timeframe}_session_performance'] = (
                        features['london_session'] * 1.5 + 
                        features['newyork_session'] * 1.2 + 
                        features['asian_session'] * 0.7
                    ) / 3.4  # Normalize by sum of weights
                elif timeframe == 'M15':
                    # M15 balanced across sessions
                    features[f'{timeframe}_session_performance'] = (
                        features['london_session'] * 1.2 + 
                        features['newyork_session'] * 1.3 + 
                        features['asian_session'] * 0.8
                    ) / 3.3
                elif timeframe == 'M30':
                    # M30 performs well in transition periods
                    features[f'{timeframe}_session_performance'] = (
                        features['london_session'] * 1.3 + 
                        features['newyork_session'] * 1.4 + 
                        features['asian_session'] * 0.6
                    ) / 3.3
                elif timeframe == 'H1':
                    # H1 often captures major session moves
                    features[f'{timeframe}_session_performance'] = (
                        features['london_session'] * 1.4 + 
                        features['newyork_session'] * 1.5 + 
                        features['asian_session'] * 0.5
                    ) / 3.4
            else:
                features[f'{timeframe}_session_performance'] = 0.5
            
            # Market regime features - Asset-specific volatility thresholds
            atr_val = features['atr']
            
            # Asset-specific volatility classification
            if 'XAU' in symbol:
                # Gold volatility thresholds (adjusted to prevent false high volatility signals)
                high_vol_threshold = 8.0    # Gold ATR > 8.0 is high volatility (was 5.0)
                low_vol_threshold = 0.8     # Gold ATR < 0.8 is low volatility
            elif 'BTC' in symbol:
                # Bitcoin volatility thresholds
                high_vol_threshold = 200.0  # Bitcoin has very high volatility
                low_vol_threshold = 50.0
            else:
                # Forex volatility thresholds (original values)
                high_vol_threshold = 0.003  # Forex ATR > 0.003 is high volatility (was 0.002)
                low_vol_threshold = 0.001   # Forex ATR < 0.001 is low volatility
            
            features['high_volatility'] = 1 if atr_val > high_vol_threshold else 0
            features['low_volatility'] = 1 if atr_val < low_vol_threshold else 0
            features['medium_volatility'] = 1 if low_vol_threshold <= atr_val <= high_vol_threshold else 0
            
            if self.debug_mode:
                print(f"  üîç Volatility Classification for {symbol}:")
                print(f"    ATR: {atr_val:.3f}")
                print(f"    High Vol Threshold: {high_vol_threshold}")
                print(f"    Classification: {'High' if features['high_volatility'] else 'Medium' if features['medium_volatility'] else 'Low'}")
            
            
            # RSI regime features
            rsi_val = features['rsi']
            features['rsi_oversold'] = 1 if rsi_val < 30 else 0
            features['rsi_overbought'] = 1 if rsi_val > 70 else 0
            features['rsi_neutral'] = 1 if 40 <= rsi_val <= 60 else 0
            
            # Volume features
            vol_ratio = features['volumeratio']
            features['high_volume'] = 1 if vol_ratio > 1.5 else 0
            features['low_volume'] = 1 if vol_ratio < 0.8 else 0
            
            # Derived features
            features['atr_normalized'] = atr_val / 0.001  # Normalize by typical value
            features['rsi_extreme'] = 1 if rsi_val < 30 or rsi_val > 70 else 0
            features['volume_extreme'] = 1 if vol_ratio > 2.0 else 0
            
            # Interaction features
            features['atr_rsi_interaction'] = features['atr'] * features['rsi']
            features['volume_level_interaction'] = features['volumeratio'] * features['leveltouches']
            features['volatility_session_interaction'] = features['high_volatility'] * features['london_session']

            # --- Always calculate entryprice and breakoutlevel for all assets ---
            current_price = data['close'].iloc[-1]
            try:
                # Use recent high/low as breakout levels, window size asset-specific
                if 'XAU' in symbol:
                    window = 10
                elif 'BTC' in symbol:
                    window = 5
                else:
                    window = 20
                recent_high = data['high'].rolling(window).max().iloc[-1]
                recent_low = data['low'].rolling(window).min().iloc[-1]
                price_range = recent_high - recent_low
                if current_price > (recent_low + price_range * 0.7):
                    features['breakoutlevel'] = recent_high
                else:
                    features['breakoutlevel'] = recent_low
            except Exception:
                features['breakoutlevel'] = current_price * 1.001
            try:
                atr_buffer = features['atr'] * 0.5
                if features['breakoutlevel'] > current_price:
                    features['entryprice'] = features['breakoutlevel'] + atr_buffer
                else:
                    features['entryprice'] = features['breakoutlevel'] - atr_buffer
            except Exception:
                features['entryprice'] = current_price

            if self.debug_mode:
                print(f"‚úÖ Live features calculated: ATR={atr_val:.5f}, RSI={rsi_val:.1f}")
                print(f"  üìà Entry Price={features['entryprice']:.5f}, Breakout Level={features['breakoutlevel']:.5f}")

            return features
            
        except Exception as e:
            print(f"‚ùå Feature calculation error: {e}")
            self.logger.error(f"Live feature calculation error: {e}")
            return None

    def get_ensemble_predictions(self, features, symbol):
        """Get predictions from all available models"""
        try:
            if not self.models or not self.feature_columns:
                return {}

            # Handle categorical encoding first
            processed_features = features.copy()

            # Define categorical mappings (same as dual model system)
            categorical_mappings = {
                'detectionmethod': {
                    'TESTED_RESISTANCE': 'detectionmethod_TESTED_RESISTANCE',
                    'TESTED_SUPPORT': 'detectionmethod_TESTED_SUPPORT',
                    'ANY_MOVEMENT': 'detectionmethod_ANY_MOVEMENT'
                },
                'setuptype': {
                    'VOLUME_BREAKOUT_LONG': 'setuptype_VOLUME_BREAKOUT_LONG',
                    'VOLUME_BREAKOUT_SHORT': 'setuptype_VOLUME_BREAKOUT_SHORT',
                    'ULTRA_FORCED_SHORT': 'setuptype_ULTRA_FORCED_SHORT',
                    'ULTRA_FORCED_LONG': 'setuptype_ULTRA_FORCED_LONG'
                },
                'symbol': {
                    'EURUSDm': 'symbol_EURUSDm',
                    'XAUUSDm': 'symbol_XAUUSDm'
                }
            }

            # Create encoded features and remove original categorical ones
            for cat_col, value_mapping in categorical_mappings.items():
                if cat_col in processed_features:
                    original_value = processed_features[cat_col]
                    del processed_features[cat_col]
                    for original_val, encoded_name in value_mapping.items():
                        processed_features[encoded_name] = 1.0 if original_value == original_val else 0.0

            # --- Ensure feature vector matches training: always 67 features ---
            feature_vector = []
            missing_features = []
            for feature_name in self.feature_columns:
                if feature_name in processed_features:
                    try:
                        feature_vector.append(float(processed_features[feature_name]))
                    except (ValueError, TypeError):
                        feature_vector.append(0.0)
                        missing_features.append(f"{feature_name} (conversion error)")
                else:
                    feature_vector.append(0.0)
                    missing_features.append(feature_name)

            if missing_features:
                print(f"‚ö†Ô∏è Missing features: {missing_features}")

            # Always ensure we have exactly 67 features for the scaler and models
            feature_array = np.array(feature_vector).reshape(1, -1)
            if feature_array.shape[1] != 67:
                if self.debug_mode:
                    print(f"üîß Adjusting feature count from {feature_array.shape[1]} to 67 for scaler/model")
                if feature_array.shape[1] < 67:
                    padding_needed = 67 - feature_array.shape[1]
                    padding = np.zeros((feature_array.shape[0], padding_needed))
                    feature_array = np.hstack([feature_array, padding])
                else:
                    feature_array = feature_array[:, :67]

            try:
                feature_array_scaled = self.feature_scaler.transform(feature_array)
                if self.debug_mode:
                    print(f"‚úÖ Feature scaling successful: {feature_array.shape} -> {feature_array_scaled.shape}")
            except Exception as e:
                print(f"‚ö†Ô∏è Feature scaling error: {e}")
                feature_array_scaled = feature_array

            predictions = {}

            # Get predictions from each model
            for model_name, model in self.models.items():
                if model is None or not hasattr(model, 'predict_proba'):
                    continue
                try:
                    if model_name == 'adaptive_learner':
                        prob = self.predict_online(feature_vector)
                    else:
                        model_input = feature_array_scaled
                        # For RandomForestClassifier, ensure input is 67 features
                        if model_input.shape[1] != 67:
                            if model_input.shape[1] < 67:
                                pad = np.zeros((model_input.shape[0], 67 - model_input.shape[1]))
                                model_input = np.hstack([model_input, pad])
                            else:
                                model_input = model_input[:, :67]
                        prob = model.predict_proba(model_input)[0][1]
                        if model_name == 'timeframe_ensemble' and 'timeframe' in features:
                            prob = self._calibrate_timeframe_predictions(prob, features['timeframe'])
                    predictions[model_name] = float(prob)
                    if self.debug_mode:
                        print(f"üîÆ {model_name}: {prob:.3f}")
                except Exception as e:
                    if self.debug_mode:
                        print(f"‚ö†Ô∏è {model_name} prediction error: {e}")
                    continue

            return predictions

        except Exception as e:
            print(f"‚ùå Ensemble prediction error: {e}")
            self.logger.error(f"Ensemble prediction error: {e}")
            return {}

    def make_enhanced_trading_decision(self, predictions, symbol, timeframe, features=None):
        """Make trading decision using robust RSI cross logic"""
        # --- Best-practice RSI signal integration ---
        signal = None
        # If recent price data is available in features, use it for the RSI signal
        if features is not None and isinstance(features, dict) and 'recent_data' in features:
            # 'recent_data' should be a DataFrame with 'close' prices
            rsi_signal = self.simple_profitable_rsi_signal(features['recent_data'])
            signal = rsi_signal
        else:
            # Fallback to classic robust RSI cross logic
            prev_rsi = None
            current_rsi = None
            if features is not None:
                if isinstance(features, dict):
                    prev_rsi = features.get('prev_rsi')
                    current_rsi = features.get('rsi')
                elif hasattr(features, 'iloc') and features.shape[0] >= 2:
                    prev_rsi = features.iloc[-2]['rsi'] if 'rsi' in features.columns else None
                    current_rsi = features.iloc[-1]['rsi'] if 'rsi' in features.columns else None
            if prev_rsi is not None and current_rsi is not None:
                if prev_rsi <= 30 and current_rsi > 30:
                    signal = 'BUY'
                elif prev_rsi >= 70 and current_rsi < 70:
                    signal = 'SELL'
                else:
                    signal = 'HOLD'
        # ...existing code for ML ensemble, other filters, etc...
        # Optionally, combine with ML predictions or use as a filter
        return signal
        """Optimized decision making for ULTRA_FORCED setups with position management"""
        try:
            if not predictions or not symbol:  # Added symbol check
                return {
                    'should_trade': False,
                    'ensemble_confidence': 0.0,
                    'rejection_reason': 'No model predictions available or invalid symbol',
                    'symbol': symbol,
                    'timeframe': timeframe
                }

            # Cache ULTRA_FORCED check results
            if not hasattr(self, '_ultra_forced_cache'):
                self._ultra_forced_cache = {}
            
            cache_key = f"{symbol}_{timeframe}"
            if cache_key not in self._ultra_forced_cache:
                # Ensure we pass the symbol to detect_ultra_forced_setup
                setup_direction, expected_win_rate = self.detect_ultra_forced_setup(
                    features=features,
                    timeframe=timeframe,
                    symbol=symbol  # Explicitly pass symbol
                )
                self._ultra_forced_cache[cache_key] = {
                    'direction': setup_direction,
                    'win_rate': expected_win_rate,
                    'timestamp': datetime.now()
                }
            else:
                # Use cached result if less than 1 minute old
                cache_age = datetime.now() - self._ultra_forced_cache[cache_key]['timestamp']
                if cache_age.total_seconds() > 60:  # Refresh cache if older than 1 minute
                    setup_direction, expected_win_rate = self.detect_ultra_forced_setup(features, timeframe)
                    self._ultra_forced_cache[cache_key] = {
                        'direction': setup_direction,
                        'win_rate': expected_win_rate,
                        'timestamp': datetime.now()
                    }
                else:
                    setup_direction = self._ultra_forced_cache[cache_key]['direction']
                    expected_win_rate = self._ultra_forced_cache[cache_key]['win_rate']
            
            # Store current ULTRA_FORCED context for portfolio constraints
            self.last_ultra_forced_check = {
                'is_ultra_forced': setup_direction is not None,
                'direction': setup_direction,
                'win_rate': expected_win_rate
            }

            # Calculate ensemble confidence
            ensemble_confidence = self._calculate_ensemble_confidence(predictions)

            # Get optimized threshold (lower for ULTRA_FORCED)
            threshold = self._get_dynamic_threshold(symbol, timeframe)
            if setup_direction is not None:
                threshold *= 0.8  # 20% lower threshold for ULTRA_FORCED setups

            # Apply optimized filters
            filters_passed, filter_reasons = self._apply_trading_filters(
                features, symbol, timeframe, setup_direction, expected_win_rate
            )

            # Check portfolio constraints (now ULTRA_FORCED aware)
            portfolio_ok = self._check_portfolio_constraints(symbol)

            # Final decision
            should_trade = (
                ensemble_confidence > threshold and 
                filters_passed and
                portfolio_ok and
                (setup_direction is not None or ensemble_confidence > threshold * 1.2) and
                (expected_win_rate > 0.35 if setup_direction is not None else True)
            )

            # Determine rejection reason
            rejection_reason = ""
            if not portfolio_ok:
                rejection_reason = "Portfolio constraints"
            elif ensemble_confidence <= threshold:
                rejection_reason = f"Confidence {ensemble_confidence:.1%} ‚â§ {threshold:.1%}"
            elif not filters_passed:
                rejection_reason = f"Filters: {', '.join(filter_reasons)}"
            elif expected_win_rate <= 0.35 and setup_direction is not None:
                rejection_reason = f"Win rate {expected_win_rate:.1%} too low"

            return {
                'should_trade': should_trade,
                'ensemble_confidence': ensemble_confidence,
                'trade_direction': setup_direction or self._determine_trade_direction(features),
                'setup_type': 'ULTRA_FORCED' if setup_direction else 'NORMAL',
                'expected_win_rate': expected_win_rate if setup_direction else None,
                'rejection_reason': rejection_reason,
                'symbol': symbol,
                'timeframe': timeframe
            }

        except Exception as e:
            return {
                'should_trade': False,
                'ensemble_confidence': 0.0,
                'rejection_reason': f'Error: {str(e)}',
                'symbol': symbol,
                'timeframe': timeframe
            }

    def _calculate_ensemble_confidence(self, predictions):
        """Calculate weighted ensemble confidence with adaptive_learner validation"""
        if not predictions:
            return 0.0
        
        # Create a copy to avoid modifying original predictions
        calibrated_predictions = predictions.copy()
        
        # Handle extreme adaptive_learner values that skew ensemble
        if 'adaptive_learner' in calibrated_predictions:
            adaptive_pred = calibrated_predictions['adaptive_learner']
            original_pred = adaptive_pred
            
            # Cap extreme values to prevent ensemble skewing
            calibrated_predictions['adaptive_learner'] = max(0.1, min(0.9, adaptive_pred))
            
            if self.debug_mode and abs(original_pred - calibrated_predictions['adaptive_learner']) > 0.001:
                print(f"  üîß Capped adaptive_learner: {original_pred:.3f} ‚Üí {calibrated_predictions['adaptive_learner']:.3f}")
            
        # Log individual model predictions for investigation
        if self.debug_mode:
            print("\nüîç Model Confidence Breakdown:")
            for model_name, confidence in calibrated_predictions.items():
                print(f"  ‚Ä¢ {model_name}: {confidence:.3f}")
                if model_name == 'adaptive_learner' and (confidence == 1.0 or confidence == 0.0):
                    print("  ‚ö†Ô∏è Extreme adaptive_learner value detected and capped")
                    
        # Dynamic weights based on model performance and trade count
        total_trades = len(self.trade_history)
        
        if total_trades < 50:
            # Early stage - prioritize gold specialist
            weights = {
                'timeframe_ensemble': 0.30,
                'EURUSDm_specialist': 0.05,
                'gold_specialist': 0.45,
                'adaptive_learner': 0.20
            }
        elif total_trades < 200:
            # Growth stage - increase adaptive learning
            weights = {
                'timeframe_ensemble': 0.25,
                'EURUSDm_specialist': 0.05,
                'gold_specialist': 0.40,
                'adaptive_learner': 0.30
            }
        else:
            # Mature stage - balanced gold/adaptive
            weights = {
                'timeframe_ensemble': 0.20,
                'EURUSDm_specialist': 0.05,
                'gold_specialist': 0.35,
                'adaptive_learner': 0.40
            }
        
        # Calculate weighted average using calibrated predictions
        weighted_sum = 0.0
        weight_sum = 0.0
        
        for model_name, confidence in calibrated_predictions.items():
            if model_name in weights:
                weighted_sum += confidence * weights[model_name]
                weight_sum += weights[model_name]
        
        return weighted_sum / weight_sum if weight_sum > 0 else np.mean(list(calibrated_predictions.values()))

    def _get_dynamic_threshold(self, symbol, timeframe):
        """Optimized thresholds based on asset performance and scalping requirements"""
        
        # Use asset-specific thresholds if available
        if hasattr(self, 'asset_thresholds'):
            base_threshold = self.asset_thresholds.get(symbol, 0.50)
        else:
            base_threshold = self.confidence_threshold
            
        # Apply timeframe-specific modifier for scalping
        if hasattr(self, 'timeframe_threshold_modifiers'):
            tf_modifier = self.timeframe_threshold_modifiers.get(timeframe, 1.0)
            base_threshold *= tf_modifier
        
        # Adaptive threshold based on recent performance
        if len(self.trade_history) >= 10:
            recent_trades = self.trade_history[-20:]
            recent_accuracy = sum(1 for t in recent_trades if t.get('actual_outcome') == 1) / len(recent_trades)
            if recent_accuracy > 0.6:
                base_threshold -= 0.05  # More aggressive reduction for scalping
            elif recent_accuracy < 0.3:
                base_threshold += 0.05  # Smaller increase for scalping
        
        # Asset-specific adjustments
        if 'XAU' in symbol:
            base_threshold -= 0.02  # More aggressive on gold
        elif 'EUR' in symbol:
            base_threshold += 0.15  # Much more conservative on EURUSD

        return max(0.20, min(0.45, base_threshold))

    def _apply_trading_filters(self, features, symbol, timeframe, setup_direction=None, expected_win_rate=None):
        """Optimized filters to block breakouts, allow ULTRA_FORCED setups on both Gold and EURUSD."""

        filters_passed = True
        filter_reasons = []

        if features is None:
            return False, ['No features available']

        # Debug logging for RSI=80.0 issue
        rsi = features.get('rsi', 50)
        if self.debug_mode and rsi >= 80:
            print(f"üîç DEBUGGING RSI={rsi:.1f} on {symbol}:")

        # 1. BLOCK VOLUME BREAKOUTS (20% win rate)
        vol_ratio = features.get('volumeratio', 1.0)
        if vol_ratio > 2.5:
            filters_passed = False
            filter_reasons.append('Volume breakout blocked (20% historical win rate)')

        # 2. REQUIRE ULTRA_FORCED SETUP (use cached result if provided)
        if setup_direction is None or expected_win_rate is None:
            setup_direction, expected_win_rate = self.detect_ultra_forced_setup(features, timeframe, symbol)
            if self.debug_mode and rsi >= 80:
                print(f"  ULTRA_FORCED Detection: {setup_direction}, Win Rate: {expected_win_rate}")
        
        if setup_direction is None:
            filters_passed = False
            filter_reasons.append('Not ULTRA_FORCED mean reversion setup')
            if self.debug_mode and rsi >= 80:
                print(f"  ‚ùå ULTRA_FORCED detection failed for RSI={rsi:.1f}")

        # 3. ALLOW BOTH GOLD AND EURUSD FOR ULTRA_FORCED SETUPS
        # Remove blanket EURUSD restriction - only block if not ULTRA_FORCED
        if 'EUR' in symbol and setup_direction is None:
            filters_passed = False
            filter_reasons.append('EURUSD only allowed for ULTRA_FORCED setups')

        # 4 & 5. Enhanced logic for extreme setups
        super_extreme = rsi < 20 or rsi > 85
        ultra_forced_extreme = rsi < 25 or rsi > 75  # ULTRA_FORCED thresholds
        
        if self.debug_mode and rsi >= 80:
            print(f"  Super Extreme (RSI<20 or >85): {super_extreme}")
            print(f"  ULTRA_FORCED Extreme (RSI<25 or >75): {ultra_forced_extreme}")
            print(f"  Setup Direction: {setup_direction}")

        # For confirmed ULTRA_FORCED setups with extreme RSI, bypass most filters
        if setup_direction is not None and ultra_forced_extreme:
            if self.debug_mode:
                print(f"‚úÖ ULTRA_FORCED bypass activated for {symbol} RSI={rsi:.1f}")
            # Only check for extreme volatility and volume breakouts
            if features.get('high_volatility', 0) and features.get('atr', 0) > 10.0:  # Very high threshold
                filters_passed = False
                filter_reasons.append('Extreme volatility')
        else:
            # Standard filters for non-ULTRA_FORCED setups
            if features.get('high_volatility', 0):
                filters_passed = False
                filter_reasons.append('High volatility blocks mean reversion')
            if rsi > 85 or rsi < 15:
                filters_passed = False
                filter_reasons.append('RSI too extreme for non-ULTRA_FORCED')

        if self.debug_mode and rsi >= 80:
            print(f"  Final Result: Passed={filters_passed}, Reasons={filter_reasons}")

        return filters_passed, filter_reasons

    def _check_portfolio_constraints(self, symbol):
        """Enhanced portfolio constraints with ULTRA_FORCED priority"""
        try:
            # Get current position info
            symbol_trades = [t for t in self.active_trades.values() if t.get('symbol') == symbol]
            max_per_symbol = self.config['trading'].get('max_trades_per_symbol', 2)
            
            # Check for ULTRA_FORCED setup
            is_ultra_forced = (hasattr(self, 'last_ultra_forced_check') and 
                             self.last_ultra_forced_check.get('is_ultra_forced', False))
            
            if is_ultra_forced:
                ultra_forced_direction = self.last_ultra_forced_check.get('direction')
                win_rate = self.last_ultra_forced_check.get('win_rate', 0.0)
                
                # Handle existing positions
                for trade in symbol_trades:
                    current_type = trade.get('type')
                    # Close opposite direction trades for strong setups
                    if current_type != ultra_forced_direction and win_rate >= 0.40:
                        if self.debug_mode:
                            print(f"ÔøΩ Closing {current_type} trade for {ultra_forced_direction} ULTRA_FORCED setup ({win_rate:.1%} win rate)")
                        ticket_id = trade.get('ticket') or trade.get('order_id')
                        if ticket_id:
                            self._close_trade(ticket_id, 
                                f"Closed {current_type} for {ultra_forced_direction} ULTRA_FORCED setup ({win_rate:.1%} win rate)")
                        else:
                            if self.debug_mode:
                                print(f"‚ö†Ô∏è Cannot close trade - no valid ticket ID found in trade: {trade}")
                
                # Always allow strong ULTRA_FORCED setups
                if win_rate >= 0.40:
                    if self.debug_mode:
                        print(f"‚úÖ Allowing ULTRA_FORCED {ultra_forced_direction} ({win_rate:.1%} win rate)")
                    return True
                    
                # For weaker setups, respect position limits
                return len(symbol_trades) < max_per_symbol
            
            # Standard position limits for normal trades
            max_total = self.config['trading'].get('max_simultaneous_trades', 3)
            if len(symbol_trades) >= max_per_symbol:
                return False
            if len(self.active_trades) >= max_total:
                return False
                
            return True
            
        except Exception as e:
            self.logger.error(f"Portfolio constraint check error: {e}")
            return False

            # Standard position limits for non-ULTRA_FORCED trades
            if len(symbol_trades) >= max_per_symbol:
                return False

            # Maximum total trades check (exempt ULTRA_FORCED)
            max_total = self.config['trading'].get('max_simultaneous_trades', 3)
            if len(self.active_trades) >= max_total and not is_ultra_forced:
                return False

            return True

        except Exception as e:
            self.logger.error(f"Portfolio constraint check error: {e}")
            return False
        
        # Maximum total trades
        max_total = self.config['trading'].get('max_simultaneous_trades', 3)
        if len(self.active_trades) >= max_total:
            return False
        
        # Check correlation limits (simplified)
        # In a full implementation, you would check correlation between assets
        
        return True

    def _get_confidence_tier(self, confidence):
        """Classify confidence into tiers"""
        if confidence >= 0.75:
            return 'VERY_HIGH'
        elif confidence >= 0.60:
            return 'HIGH'
        elif confidence >= 0.45:
            return 'MEDIUM'
        else:
            return 'LOW'

    def apply_quick_optimizations(self):
        """Apply optimizations based on 25K dataset analysis"""
        print("üéØ APPLYING OPTIMIZATIONS BASED ON 25K DATASET...")
        
        # Configure for scalping on gold and major pairs
        self.config['trading']['symbols'] = ['XAUUSDm', 'EURUSDm']  # Allow both Gold and EURUSD for ULTRA_FORCED
        self.config['trading']['timeframes'] = ['M1', 'M5', 'M15']  # Scalping-optimized timeframes
        
        # Asset-specific thresholds - Optimized for scalping opportunities
        self.asset_thresholds = {
            'XAUUSDm': 0.15,    # Very aggressive for gold scalping (35% win rate acceptable)
            'EURUSDm': 0.20,    # More aggressive for EURUSD scalping
            'GBPUSDm': 1.0,     # Disabled
            'BTCUSDm': 1.0      # Disabled
        }
        
        # Timeframe-specific threshold adjustments
        self.timeframe_threshold_modifiers = {
            'M1': 0.45,  # 15% lower threshold for M1
            'M5': 0.55,  # 10% lower threshold for M5
            'M15': 0.60  # 5% lower threshold for M15
        }
        
        # Add enhanced data requirements - Optimized for scalping
        self.bar_requirements = {
            'M1': 300,   # Added M1 for scalping
            'M5': 150,   # Reduced for faster processing
            'M15': 100,  # Reduced for faster processing
            'M30': 80,   # Kept for trend context
            'H1': 60     # Kept for trend context
        }
        
        # Update data validation settings
        self.min_data_quality_threshold = 0.9  # Stricter data quality for scalping
        
        self.strategy_mode = 'ULTRA_FORCED_MEAN_REVERSION'
        print("‚úÖ Optimizations applied - Gold prioritized, breakouts disabled")
        print("üìä Enhanced data requirements applied for ULTRA_FORCED strategy")

    def detect_ultra_forced_setup(self, features, timeframe='M15', symbol=None):
        """Detect 40%+ win rate ULTRA_FORCED setups with adaptive RSI thresholds"""
        if features is None or symbol is None:
            return None, 0.0
        
        try:
            rsi = features.get('rsi', 50)
            atr = features.get('atr', 0.001)
            vol_ratio = features.get('volumeratio', 1.0)
            
            # Get optimized RSI thresholds from learning system
            thresholds = self.get_optimized_ultra_forced_thresholds()
            
            # Use learned thresholds or defaults
            oversold_min = thresholds.get('oversold_min', 20)
            oversold_max = thresholds.get('oversold_max', 30)
            overbought_min = thresholds.get('overbought_min', 70)
            overbought_max = thresholds.get('overbought_max', 80)
            
            # Adaptive conditions based on learned patterns
            extreme_oversold = rsi < oversold_max  # Learned threshold
            extreme_overbought = rsi > overbought_min  # Learned threshold
            super_extreme_oversold = rsi < oversold_min  # Best performing range
            super_extreme_overbought = rsi > overbought_max  # Best performing range
            
            if self.debug_mode and hasattr(self, 'dynamic_rsi_thresholds'):
                print(f"üß† Using optimized RSI thresholds: Oversold {oversold_min}-{oversold_max}, Overbought {overbought_min}-{overbought_max}")
            
            # Adjust volatility threshold based on timeframe and asset
            base_volatility_thresholds = {
                'M5': 1.0,    # Base thresholds for forex pairs
                'M15': 2.5,
                'M30': 5.0,
                'H1': 7.0
            }
            
            # Gold-specific volatility thresholds (2.5x higher to account for natural volatility)
            gold_volatility_thresholds = {
                'M5': 2.5,    # Adjusted for Gold's natural volatility
                'M15': 6.25,
                'M30': 12.5,
                'H1': 17.5
            }
            
            # Select appropriate threshold based on asset
            if 'XAU' in symbol:
                volatility_thresholds = gold_volatility_thresholds
            else:
                volatility_thresholds = base_volatility_thresholds
            max_atr = volatility_thresholds.get(timeframe, 2.0)
            low_volatility = atr < max_atr  # Timeframe-adjusted threshold
            
            # Volume classification based on ratio - Optimized for scalping
            very_low_volume = vol_ratio < 0.1  # Increased minimum for scalping safety
            low_volume = 0.1 <= vol_ratio < 0.3  # Good for quick reversals
            normal_volume = 0.3 <= vol_ratio <= 0.8  # Optimal range for scalping (35%+ win rate)
            moderate_volume = 0.8 < vol_ratio <= 3.0  # Extended range for momentum scalping
            high_volume = vol_ratio > 3.0  # Consider for breakout scalping
            
            # Volume acceptance based on tested win rates
            volume_ok = 0.05 <= vol_ratio <= 2.5  # Accept full mean reversion range
            optimal_volume = 0.2 <= vol_ratio <= 0.38  # Best win rate range
            
            if super_extreme_oversold or super_extreme_overbought:
                # In super extreme conditions (RSI < min or > max), same volume rule applies
                win_rate_modifier = 1.2 if normal_volume else 1.1  # Best win rate in 0.2-0.5 range
            elif extreme_oversold or extreme_overbought:
                # In normal extreme conditions, accept low through high volume
                volume_ok = not very_low_volume  # Also permissive here
            
            if self.debug_mode:
                print(f"ULTRA_FORCED Check [{timeframe}] - Using Optimized Thresholds:")
                print(f"  RSI: {rsi:.1f} (Oversold: <{oversold_max}, Super: <{oversold_min} | Overbought: >{overbought_min}, Super: >{overbought_max})")
                print(f"    Super Extreme: {'YES' if super_extreme_oversold or super_extreme_overbought else 'NO'}")
                print(f"    Normal Extreme: {'YES' if extreme_oversold or extreme_overbought else 'NO'}")
                print(f"  ATR: {atr:.3f} (Low Vol: {low_volatility}, Max: {max_atr:.3f})")
                print(f"  Volume:")
                print(f"    Ratio: {vol_ratio:.2f}")
                print(f"    Classification: {'Very Low' if very_low_volume else 'Low' if low_volume else 'Normal' if normal_volume else 'High'}")
                print(f"    Volume OK: {volume_ok}")

            win_rate_modifier = 1.0  # Default modifier
            if super_extreme_oversold or super_extreme_overbought:
                win_rate_modifier = 1.2 if normal_volume else 1.1  # Higher boost for normal volume
            
            # Apply learned pattern bonus
            if hasattr(self, 'ultra_forced_patterns'):
                patterns = self.ultra_forced_patterns
                
                # Check if this RSI bucket has good historical performance
                rsi_bucket = self._get_rsi_bucket(rsi)
                if rsi_bucket in patterns.get('rsi_buckets', {}):
                    bucket_data = patterns['rsi_buckets'][rsi_bucket]
                    if bucket_data['trades'] >= 5 and bucket_data['win_rate'] > 0.5:
                        win_rate_modifier *= 1.1  # Boost for historically good RSI ranges
                        if self.debug_mode:
                            print(f"  üß† RSI bucket {rsi_bucket} bonus applied (WR: {bucket_data['win_rate']:.1%})")
                
                # Check symbol-specific performance
                if symbol in patterns.get('symbol_performance', {}):
                    symbol_data = patterns['symbol_performance'][symbol]
                    if symbol_data['trades'] >= 5 and symbol_data['win_rate'] > 0.5:
                        win_rate_modifier *= 1.05  # Small symbol-specific boost
                        if self.debug_mode:
                            print(f"  üß† Symbol {symbol} bonus applied (WR: {symbol_data['win_rate']:.1%})")
            
            if extreme_oversold and low_volatility and volume_ok:
                expected_win_rate = 0.409 * win_rate_modifier
                if self.debug_mode:
                    print(f"‚úÖ BUY Signal: {'SUPER ' if super_extreme_oversold else ''}Oversold ULTRA_FORCED setup detected (Expected WR: {expected_win_rate:.1%})")
                return 'BUY', expected_win_rate
            elif extreme_overbought and low_volatility and volume_ok:
                expected_win_rate = 0.413 * win_rate_modifier
                if self.debug_mode:
                    print(f"‚úÖ SELL Signal: {'SUPER ' if super_extreme_overbought else ''}Overbought ULTRA_FORCED setup detected (Expected WR: {expected_win_rate:.1%})")
                return 'SELL', expected_win_rate
            else:
                if self.debug_mode:
                    print(f"‚ùå No ULTRA_FORCED setup detected")
                return None, 0.0
            
        except Exception as e:
            self.logger.error(f"ULTRA_FORCED setup detection error: {e}")
            return None, 0.0

    def consolidate_ultra_forced_signals(self, all_analyses):
        """Prioritize higher timeframe ULTRA_FORCED signals"""
        ultra_forced_signals = [a for a in all_analyses if a.get('setup_type') == 'ULTRA_FORCED']
        if not ultra_forced_signals:
            return None
        # Prioritize by timeframe (higher = more reliable)
        timeframe_priority = {'H1': 4, 'M30': 3, 'M15': 2, 'M5': 1}
        best_signal = max(ultra_forced_signals, key=lambda x: timeframe_priority.get(x['timeframe'], 0))
        return best_signal

    def calculate_ultra_forced_strength(self, rsi, atr, confidence):
        """Score the strength of ULTRA_FORCED setup"""
        # RSI extremeness (85.8 is very extreme)
        rsi_score = max(0, min(100, abs(rsi - 50) - 35)) / 15  # 0-1 scale
        # ML confidence bonus
        conf_score = min(1.0, confidence / 0.6)  # 0-1 scale
        # Combined strength
        strength = (rsi_score * 0.7) + (conf_score * 0.3)
        return strength  # 0-1, where 1 is strongest

    def _determine_trade_direction(self, features):
        """Determine trade direction based on features"""
        if features is None:
            return 'BUY'  # Default
        
        # First check for ULTRA_FORCED setup
        direction, win_rate = self.detect_ultra_forced_setup(features, timeframe='M15', symbol=None)  # Default to M15
        if direction:
            return direction
            
        rsi = features.get('rsi', 50)
        
        # Simple RSI-based direction
        if rsi > 60:
            return 'SELL'
        elif rsi < 40:
            return 'BUY'
        else:
            # Use other indicators or default to BUY
            return 'BUY'

    def start_enhanced_system(self):
        """Start the enhanced adaptive trading system"""
        print("\nüöÄ STARTING ENHANCED ADAPTIVE ML TRADING SYSTEM")
        print("=" * 60)
        
        # Step 1: Load models
        print("üì¶ Step 1: Loading production models...")
        if not self.load_production_models():
            print("‚ùå No trained models found. Please run training first.")
            return False
        
        # Step 2: Connect to MetaTrader
        print("\nüåê Step 2: Connecting to MetaTrader...")
        if not self.connect_to_metatrader():
            print("‚ùå MetaTrader connection failed. Running in analysis mode.")
            return False
        
        # Step 3: Start monitoring and trading
        print("\nüîÑ Step 3: Starting intelligent monitoring...")
        self.start_intelligent_monitoring()
        
        return True

    def start_intelligent_monitoring(self):
        """Start intelligent market monitoring and trading"""
        print("üîç Enhanced monitoring system started...")
        
        # Ensure enhanced learning system is properly initialized
        if not hasattr(self, 'last_learning_update'):
            print("üß† Initializing enhanced learning components...")
            self._init_enhanced_learning_components()
        
        scan_count = 0
        
        while True:
            try:
                scan_count += 1
                
                # Check if market is open and trading conditions are met
                if not self._check_enhanced_trading_conditions():
                    if self.debug_mode:
                        print(f"üö´ Trading conditions not met (Scan #{scan_count})")
                    time.sleep(300)  # Wait 5 minutes
                    continue
                
                print(f"\nüîç ENHANCED MARKET SCAN #{scan_count} - {datetime.now().strftime('%H:%M:%S')}")
                print("=" * 60)
                
                # Scan all symbol/timeframe combinations
                opportunities_found = 0
                
                for symbol in self.config['trading']['symbols']:
                    for timeframe in self.config['trading']['timeframes']:
                        try:
                            # Analyze market opportunity
                            analysis = self._analyze_enhanced_opportunity(symbol, timeframe)
                            
                            if analysis and analysis['should_trade']:
                                opportunities_found += 1
                                print(f"üéØ OPPORTUNITY: {symbol} {timeframe} - Confidence: {analysis['ensemble_confidence']:.1%}")
                                
                                # Execute trade
                                success = self._execute_enhanced_trade(analysis)
                                if success:
                                    print(f"‚úÖ Trade executed successfully")
                                else:
                                    print(f"‚ùå Trade execution failed")
                            
                            elif self.debug_mode and analysis:
                                conf = analysis['ensemble_confidence']
                                reason = analysis['rejection_reason']
                                print(f"‚ùå {symbol} {timeframe}: {conf:.1%} - {reason}")
                        
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error analyzing {symbol} {timeframe}: {e}")
                            continue
                
                print(f"üìä Scan complete: {opportunities_found} opportunities found")
                
                # Update enhanced learning system periodically
                self.update_enhanced_learning_system()
                
                # Wait before next scan
                time.sleep(120)  # 2 minutes between scans
                
            except KeyboardInterrupt:
                print("\nüõë System stopped by user")
                # Save enhanced learning state before shutdown
                self.save_enhanced_learning_state()
                break
            except Exception as e:
                print(f"‚ùå System error: {e}")
                self.logger.error(f"System error: {e}")
                time.sleep(300)  # Wait 5 minutes before retrying

    def _check_enhanced_trading_conditions(self):
        """Enhanced trading conditions check"""
        try:
            # Market hours check
            current_time = datetime.now()
            if current_time.weekday() >= 5:  # Weekend
                return False
            
            # Daily trade limits
            today = current_time.date()
            today_trades = [t for t in self.trade_history if t.get('timestamp', datetime.min).date() == today]
            
            if len(today_trades) >= self.max_daily_trades:
                return False
            
            # Account balance check
            if mt5:
                account_info = mt5.account_info()
                if account_info:
                    if account_info.balance < self.min_account_balance:
                        return False
                    
                    # Check daily loss limit
                    daily_loss_limit = account_info.equity * self.config['risk_management']['daily_loss_limit']
                    today_pnl = sum(t.get('pnl', 0) for t in today_trades)
                    
                    if today_pnl < -daily_loss_limit:
                        return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Trading conditions check error: {e}")
            return False

    def _analyze_enhanced_opportunity(self, symbol, timeframe):
        """Analyze market opportunity with enhanced logic"""
        try:
            # Get live market data
            market_data = self.get_live_market_data(symbol, timeframe)
            if market_data is None:
                self.logger.debug(f"No market data for {symbol} {timeframe}")
                return None

            # Calculate features
            features = self.calculate_live_features(symbol, timeframe, market_data)
            if features is None:
                self.logger.debug(f"No features for {symbol} {timeframe}")
                return None

            # Get ensemble predictions
            predictions = self.get_ensemble_predictions(features, symbol)
            if not predictions:
                self.logger.debug(f"No predictions for {symbol} {timeframe}")
                return None

            # Make trading decision
            analysis = self.make_enhanced_trading_decision(predictions, symbol, timeframe, features)
            if analysis is None:
                self.logger.debug(f"make_enhanced_trading_decision returned None for {symbol} {timeframe}")
            # Log analysis for learning only if analysis is a dict
            if analysis is not None and isinstance(analysis, dict):
                self._log_analysis(analysis)
            else:
                self.logger.warning(f"Analysis is None or not a dict, skipping log: {analysis}")
            return analysis

        except Exception as e:
            self.logger.error(f"Enhanced opportunity analysis error: {e}")
            return None

    def _execute_enhanced_trade(self, analysis):
        """Execute trade with enhanced risk management, blocking simultaneous opposite trades for same symbol/timeframe"""

        try:
            # Ensure confidence_tier is set in analysis
            if 'confidence_tier' not in analysis:
                analysis['confidence_tier'] = self._get_confidence_tier(analysis.get('ensemble_confidence', 0.0))

            symbol = analysis['symbol']
            timeframe = analysis['timeframe']
            confidence = analysis['ensemble_confidence']
            direction = analysis.get('trade_direction', 'BUY')

            # === ENHANCED LEARNING INTEGRATION ===
            if hasattr(self, 'advanced_learning_engine') and self.advanced_learning_engine is not None:
                try:
                    # Ensure advanced learning engine has required methods
                    if (hasattr(self.advanced_learning_engine, 'calibrate_confidence') and 
                        hasattr(self.advanced_learning_engine, 'get_trading_insights')):
                        
                        # Get enhanced insights from learning engine
                        features = analysis.get('features', {})
                        market_context = analysis.get('market_context', {})
                        
                        # Apply confidence calibration
                        calibrated_confidence = self.advanced_learning_engine.calibrate_confidence(
                            confidence, features, market_context
                        )
                        
                        # Check if we should adjust the trade based on learning insights
                        learning_insights = self.advanced_learning_engine.get_trading_insights()
                        
                        # Update confidence with calibrated value
                        if calibrated_confidence != confidence:
                            print(f"üß† Confidence calibrated: {confidence:.1%} ‚Üí {calibrated_confidence:.1%}")
                            confidence = calibrated_confidence
                            analysis['ensemble_confidence'] = confidence
                            analysis['confidence_tier'] = self._get_confidence_tier(confidence)
                        
                        # Apply dynamic threshold adaptation
                        symbol_thresholds = self._safe_get(self.threshold_adaptation, symbol, {})
                        current_threshold = self._safe_get(symbol_thresholds, timeframe, 0.25) if isinstance(symbol_thresholds, dict) else 0.45
                        if confidence < current_threshold:
                            print(f"üõ°Ô∏è Trade blocked by adaptive threshold: {confidence:.1%} < {current_threshold:.1%}")
                            return False
                    else:
                        print("‚ö†Ô∏è Enhanced learning methods not available, using standard processing")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Enhanced learning error: {e}, continuing with standard processing")
                    # Continue with standard processing
            else:
                print("‚ö†Ô∏è Advanced learning engine not available, using standard processing")
            
            # Get risk adjustment from enhanced risk manager
            if hasattr(self, 'enhanced_risk_manager') and self.enhanced_risk_manager is not None:
                try:
                    market_context = analysis.get('market_context', {})
                    risk_multiplier = self.enhanced_risk_manager.get_risk_multiplier(symbol, market_context)
                    if risk_multiplier < 0.5:
                        print(f"üõ°Ô∏è Trade blocked by risk manager: risk multiplier too low ({risk_multiplier:.2f})")
                        return False
                    # Store risk multiplier for position sizing
                    analysis['risk_multiplier'] = risk_multiplier
                    
                    if self.debug_mode:
                        print(f"üß† Enhanced risk manager applied:")
                        print(f"   Risk multiplier: {risk_multiplier:.2f}")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Enhanced risk manager error: {e}, using default risk settings")
                    analysis['risk_multiplier'] = 1.0

            # For ULTRA_FORCED setups, be more flexible with position management
            if analysis.get('setup_type') == 'ULTRA_FORCED':
                open_positions = mt5.positions_get(symbol=symbol)
                if open_positions:
                    for pos in open_positions:
                        # If we have opposite direction, consider closing it
                        if (direction == 'SELL' and pos.type == mt5.POSITION_TYPE_BUY) or \
                           (direction == 'BUY' and pos.type == mt5.POSITION_TYPE_SELL):
                            print(f"üîÑ ULTRA_FORCED signal opposes existing position - consider closing")
                            # Option: Close the opposite position here

            # --- Early exit for disabled BTC trading ---
            if symbol.upper() in ['BTCUSD', 'BTCUSDM']:
                print(f"‚ö†Ô∏è BTC trading disabled for {symbol} - skipping trade execution")
                return False

            # --- Prevent simultaneous opposite trades for same symbol/timeframe ---
            open_positions = mt5.positions_get(symbol=symbol)
            if open_positions:
                for pos in open_positions:
                    # Check if position is for this timeframe (if stored in comment or magic)
                    pos_comment = getattr(pos, 'comment', '')
                    pos_timeframe = None
                    if pos_comment and '_' in pos_comment:
                        parts = pos_comment.split('_')
                        if len(parts) >= 3:
                            pos_timeframe = parts[2]
                    # If timeframe matches (or if timeframe not encoded, block by symbol)
                    if (pos_timeframe == timeframe) or (pos_timeframe is None):
                        print(f"‚ùå Trade blocked: Open position exists for {symbol} {timeframe}")
                        return False

            if self.debug_mode:
                print(f"üéØ Executing {direction} trade: {symbol} {timeframe}")
                print(f"   Confidence: {confidence:.1%}")
                print(f"   Tier: {analysis['confidence_tier']}")

            # Get account info
            account_info = mt5.account_info()
            if not account_info:
                print("‚ùå Cannot get account info")
                return False

            # Calculate position size with enhanced risk management
            position_size = self._calculate_enhanced_position_size(confidence, account_info, symbol, analysis)

            # Get current price
            tick = mt5.symbol_info_tick(symbol)
            if not tick:
                print(f"‚ùå Cannot get tick data for {symbol}")
                return False

            price = tick.ask if direction == 'BUY' else tick.bid

            if self.debug_mode:
                print(f"   üîç Debug: Entry Price = {price}")
                print(f"   üîç Debug: Direction = {direction}")

            # --- Asset-specific stop logic: Trailing SL for BTCUSDm, fixed for others ---
            # Get symbol info for proper point calculation
            symbol_info = mt5.symbol_info(symbol)
            if symbol_info is None:
                print(f"‚ùå Cannot get symbol info for {symbol}")
                return False
            
            # Small account optimization: Use smaller, account-appropriate stops
            account_balance = account_info.balance
            
            if symbol.upper() in ['BTCUSD', 'BTCUSDm']:
                # BTC trading temporarily disabled due to stop calculation issues
                print(f"‚ö†Ô∏è BTC trading disabled for {symbol} - skipping trade execution")
                return False
            else:
                # All others: account-size adjusted stops
                if 'XAU' in symbol.upper():
                    # Gold: smaller stops for small accounts
                    if account_balance <= 100:
                        sl_dollars = 2.0   # $2 SL for small accounts
                        tp_dollars = 6.0   # $6 TP for small accounts
                    else:
                        sl_dollars = 5.0   # $5 SL for larger accounts
                        tp_dollars = 15.0  # $15 TP for larger accounts
                    
                    if direction == 'BUY':
                        stop_loss = price - sl_dollars
                        take_profit = price + tp_dollars
                    else:
                        stop_loss = price + sl_dollars
                        take_profit = price - tp_dollars
                else:
                    # Forex: smaller pip-based stops for small accounts
                    pip = 0.0001 if symbol_info.digits >= 4 else 0.01
                    if account_balance <= 100:
                        sl_pips = 30   # 30 pips SL for small accounts
                        tp_pips = 90   # 90 pips TP for small accounts
                    else:
                        sl_pips = 50   # 50 pips SL for larger accounts
                        tp_pips = 150  # 150 pips TP for larger accounts
                    
                    if direction == 'BUY':
                        stop_loss = price - sl_pips * pip
                        take_profit = price + tp_pips * pip
                    else:
                        stop_loss = price + sl_pips * pip
                        take_profit = price - tp_pips * pip
                trailing_stop = None

            # Prepare order
            order_type = mt5.ORDER_TYPE_BUY if direction == 'BUY' else mt5.ORDER_TYPE_SELL

            request = {
                "action": mt5.TRADE_ACTION_DEAL,
                "symbol": symbol,
                "volume": position_size,
                "type": order_type,
                "price": price,
                "sl": stop_loss,
                "tp": take_profit,
                "deviation": 20,
                "magic": 234567,
                "comment": f"Enhanced_AI_{timeframe}_{confidence:.0%}",
                "type_time": mt5.ORDER_TIME_GTC,
                "type_filling": mt5.ORDER_FILLING_IOC
            }
            # Add trailing stop for BTCUSDm if supported
            if trailing_stop is not None:
                request["trailing_stop"] = trailing_stop

            if self.debug_mode:
                print(f"   Stop Loss: {stop_loss}")
                print(f"   Take Profit: {take_profit}")

            # Send order
            result = mt5.order_send(request)

            if result.retcode != mt5.TRADE_RETCODE_DONE:
                print(f"‚ùå Trade failed: {result.comment}")
                return False

            # Log successful trade
            trade_data = {
                'timestamp': datetime.now(),
                'symbol': symbol,
                'timeframe': timeframe,
                'direction': direction,
                'entry_price': price,
                'volume': position_size,
                'confidence': confidence,
                'analysis': analysis,
                'order_id': result.order,
                'magic': 234567
            }

            self.active_trades[result.order] = trade_data
            self._log_trade_to_database(trade_data)

            print(f"‚úÖ Trade executed: {symbol} {direction} at {price}")
            return True

        except Exception as e:
            print(f"‚ùå Trade execution error: {e}")
            self.logger.error(f"Enhanced trade execution error: {e}")
            return False

    def _calculate_enhanced_position_size(self, confidence, account_info, symbol, analysis=None):
        """Calculate position size with enhanced risk management and adaptive sizing"""
        try:
            # Base risk amount
            base_risk = account_info.balance * self.risk_per_trade
            
            # Confidence-based adjustment
            confidence_multiplier = min(1.5, max(0.5, confidence / 0.5))
            adjusted_risk = base_risk * confidence_multiplier
            
            # Enhanced risk management adjustment
            if analysis and hasattr(self, 'enhanced_risk_manager'):
                risk_multiplier = analysis.get('risk_multiplier', 1.0)
                adjusted_risk *= risk_multiplier
                
                if self.debug_mode:
                    print(f"   üß† Risk adjustment applied: {risk_multiplier:.2f}")
            
            # ADAPTIVE RISK MANAGEMENT - Symbol performance-based adjustment
            timeframe = analysis.get('timeframe', 'M15') if analysis else 'M15'
            adaptive_multiplier = self.adaptive_risk_management(symbol, timeframe)
            adjusted_risk *= adaptive_multiplier
            
            if self.debug_mode:
                print(f"   üéØ Adaptive risk multiplier for {symbol}: {adaptive_multiplier:.2f}")
            
            # Get symbol info for lot size calculation
            symbol_info = mt5.symbol_info(symbol)
            if not symbol_info:
                return symbol_info.volume_min if symbol_info else 0.01
            
            # Calculate lot size based on symbol type
            if 'XAU' in symbol:  # Gold
                # For gold, use simpler calculation
                lot_size = adjusted_risk / 1000  # Rough estimate
            else:  # Forex
                # For forex pairs
                lot_size = adjusted_risk / 10000  # Rough estimate for forex
            
            # Ensure within symbol limits
            min_lot = symbol_info.volume_min
            max_lot = symbol_info.volume_max
            step = symbol_info.volume_step
            
            # Round to step size
            lot_size = round(lot_size / step) * step
            
            # Apply limits
            lot_size = max(min_lot, min(max_lot, lot_size))
            
            return lot_size
            
        except Exception as e:
            print(f"‚ùå Position size calculation error: {e}")
            return 0.01  # Default minimum

    def _log_analysis(self, analysis):
        """Log analysis for learning and debugging"""
        try:
            if analysis is None or not isinstance(analysis, dict):
                self.logger.error("Analysis logging error: analysis is None or not a dict")
                return
            self.analysis_log.append({
                'timestamp': datetime.now(),
                'symbol': analysis.get('symbol', ''),
                'timeframe': analysis.get('timeframe', ''),
                'confidence': analysis.get('ensemble_confidence', 0.0),
                'should_trade': analysis.get('should_trade', False),
                'rejection_reason': analysis.get('rejection_reason', ''),
                'tier': analysis.get('confidence_tier', 'UNKNOWN')
            })
            # Keep only recent analyses
            if len(self.analysis_log) > 1000:
                self.analysis_log = self.analysis_log[-500:]
        except Exception as e:
            self.logger.error(f"Analysis logging error: {e}")

    def _log_trade_to_database(self, trade_data):
        """Log trade to enhanced database"""
        try:
            cursor = self.conn.cursor()
            analysis = trade_data.get('analysis', {})
            
            cursor.execute('''
            INSERT INTO enhanced_trades (
                timestamp, symbol, timeframe, direction, entry_price, volume,
                confidence, model_version, features_used, market_conditions,
                risk_amount, session, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                trade_data['timestamp'],
                trade_data['symbol'],
                trade_data['timeframe'],
                trade_data['direction'],
                trade_data['entry_price'],
                trade_data['volume'],
                trade_data['confidence'],
                'enhanced_v3.0',
                json.dumps(list(analysis.get('features', {}).keys())[:10]),  # Top 10 features
                analysis.get('market_conditions', ''),
                trade_data['volume'] * trade_data['entry_price'] * self.risk_per_trade,
                self._get_current_session(),
                datetime.now()
            ))
            
            self.conn.commit()
            
        except Exception as e:
            self.logger.error(f"Trade logging error: {e}")

    def _get_current_session(self):
        """Get current trading session"""
        hour = datetime.now().hour
        if 8 <= hour <= 16:
            return 'London'
        elif 13 <= hour <= 21:
            return 'NewYork'
        elif 0 <= hour <= 8:
            return 'Asian'
        else:
            return 'Overlap'
    
    def print_system_status(self):
        """Print enhanced system status"""
        try:
            print(f"\nüìä ENHANCED SYSTEM STATUS - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 70)
            
            # Account status
            if hasattr(self, 'mt5') and self.mt5:
                account_info = self.mt5.account_info()
                if account_info:
                    print(f"üí∞ Account Balance: ${account_info.balance:.2f}")
                    print(f"üíµ Equity: ${account_info.equity:.2f}")
                    print(f"üìä Free Margin: ${account_info.margin_free:.2f}")
            
            # Model status
            loaded_models = sum(1 for model in self.models.values() if model is not None)
            print(f"ü§ñ Loaded Models: {loaded_models}/{len(self.models)}")
            print(f"üîß Feature Columns: {len(self.feature_columns)}")
            
            # Trading status
            print(f"üìà Active Trades: {len(self.active_trades)}")
            print(f"üìä Total Trade History: {len(self.trade_history)}")
            
            # Recent analysis summary
            if self.analysis_log:
                recent_analyses = self.analysis_log[-20:]
                total_recent = len(recent_analyses)
                trade_signals = sum(1 for a in recent_analyses if a.get('should_trade', False))
                
                # Make sure we're working with numerical confidence values
                confidence_values = []
                for a in recent_analyses:
                    conf = a.get('confidence', 0)
                    if isinstance(conf, (int, float)):
                        confidence_values.append(conf)
                
                avg_confidence = np.mean(confidence_values) if confidence_values else 0.0
                
                print(f"üîç Recent Analysis: {total_recent} scans, {trade_signals} signals")
                print(f"üìà Avg Confidence: {avg_confidence:.1%}")
                
                # Confidence distribution - safely check types
                high_conf = 0
                med_conf = 0
                low_conf = 0
                
                for a in recent_analyses:
                    conf = a.get('confidence', 0)
                    if not isinstance(conf, (int, float)):
                        continue
                        
                    if conf > 0.7:
                        high_conf += 1
                    elif conf > 0.5:
                        med_conf += 1
                    else:
                        low_conf += 1
                
                print(f"üéØ Confidence Distribution: High({high_conf}) Med({med_conf}) Low({low_conf})")
            
            # Performance metrics
            if self.trade_history:
                wins = sum(1 for t in self.trade_history if t.get('actual_outcome') == 1)
                total_trades = len(self.trade_history)
                accuracy = wins / total_trades if total_trades > 0 else 0
                total_pnl = sum(t.get('pnl', 0) for t in self.trade_history)
                
                print(f"üìà Performance: {accuracy:.1%} accuracy ({wins}/{total_trades})")
                print(f"üí∞ Total P&L: ${total_pnl:.2f}")
            
            # Current settings
            if hasattr(self, 'confidence_threshold'):
                threshold = self.confidence_threshold
                if isinstance(threshold, (int, float)):
                    print(f"üéöÔ∏è Confidence Threshold: {threshold:.1%}")
                elif isinstance(threshold, dict):
                    # Handle dictionarys threshold values
                    print(f"üéöÔ∏è Confidence Thresholds: ", end="")
                    for symbol, value in threshold.items():
                        if isinstance(value, (int, float)):
                            print(f"{symbol}: {value:.1%}", end=" ")
                        else:
                            print(f"{symbol}: {value}", end=" ")
                    print()  # Add newline
                else:
                    print(f"üéöÔ∏è Confidence Threshold: {threshold}")
            
            if hasattr(self, 'risk_per_trade'):
                risk = self.risk_per_trade
                if isinstance(risk, (int, float)):
                    print(f"‚ö†Ô∏è Risk per Trade: {risk:.1%}")
                elif isinstance(risk, dict):
                    # Handle dictionary risk values
                    print(f"‚ö†Ô∏è Risk per Trade: ", end="")
                    for symbol, value in risk.items():
                        if isinstance(value, (int, float)):
                            print(f"{symbol}: {value:.1%}", end=" ")
                        else:
                            print(f"{symbol}: {value}", end=" ")
                    print()  # Add newline
                else:
                    print(f"‚ö†Ô∏è Risk per Trade: {risk}")
            
            if hasattr(self, 'max_daily_trades'):
                trades_limit = self.max_daily_trades
                if isinstance(trades_limit, dict):
                    print(f"üî¢ Max Daily Trades: ", end="")
                    for symbol, value in trades_limit.items():
                        print(f"{symbol}: {value}", end=" ")
                    print()  # Add newline
                else:
                    print(f"üî¢ Max Daily Trades: {trades_limit}")
            
            print("=" * 70)
            
        except Exception as e:
            print(f"‚ùå Status display error: {e}")
            if hasattr(self, 'logger'):
                self.logger.error(f"System status error: {e}")

    def _print_config_summary(self):
        """Print enhanced configuration summary"""
        print("\n=== ENHANCED SYSTEM CONFIGURATION ===")
        print(f"üéØ Assets: {', '.join(self.config['trading']['symbols'])}")
        print(f"‚è∞ Timeframes: {', '.join(self.config['trading']['timeframes'])}")
        print(f"üí∞ Risk per Trade: {self.config['account']['risk_per_trade']:.1%}")
        print(f"üî¢ Max Daily Trades: {self.config['account'].get('max_daily_trades', self.config['trading'].get('max_daily_trades', 5))}")
        conf_thresh = self.config['ml_settings']['confidence_threshold']
        if isinstance(conf_thresh, dict):
            print(f"üéöÔ∏è Base Confidence Thresholds: {conf_thresh}")
        else:
            print(f"üéöÔ∏è Base Confidence Threshold: {conf_thresh:.1%}")
        print(f"üîÑ Dynamic Threshold: {'ENABLED' if self.config['ml_settings']['dynamic_threshold_adjustment'] else 'DISABLED'}")
        print(f"üß† Online Learning: {'ENABLED' if self.config['ml_settings']['online_learning_enabled'] else 'DISABLED'}")
        print(f"üõ°Ô∏è Max Simultaneous Trades: {self.config['trading']['max_simultaneous_trades']}")

    def online_partial_fit(self, features, target):
        """Enhanced online learning with validation"""
        try:
            with self.model_lock:
                features_array = np.array(features).reshape(1, -1)
                target_array = np.array([target])
                
                if not self.online_model_initialized:
                    self.online_model.partial_fit(features_array, target_array, classes=np.array([0, 1]))
                    self.online_model_initialized = True
                    print("üß† Online model initialized")
                else:
                    self.online_model.partial_fit(features_array, target_array)
                
                if self.debug_mode:
                    print(f"üîÑ Online model updated with target: {target}")
                
        except Exception as e:
            self.logger.error(f"Online learning error: {e}")

    def predict_online(self, features):
        """Enhanced online prediction with calibration and bounds checking"""
        try:
            features_array = np.array(features).reshape(1, -1)
            
            # Check if model is initialized
            if not self.online_model_initialized:
                # Try to initialize it with these features if possible
                if len(self.trade_history) >= 2:
                    try:
                        # Get some recent trades for initialization
                        sample_features = []
                        sample_targets = []
                        for i, trade in enumerate(self.trade_history[-10:]):
                            if 'features_array' in trade and 'actual_outcome' in trade:
                                sample_features.append(trade['features_array'])
                                sample_targets.append(trade['actual_outcome'])
                                if len(sample_features) >= 2 and 1 in sample_targets and 0 in sample_targets:
                                    break
                        
                        if len(sample_features) >= 2:
                            self.online_model.partial_fit(
                                np.array(sample_features), 
                                np.array(sample_targets), 
                                classes=np.array([0, 1])
                            )
                            self.online_model_initialized = True
                            if self.debug_mode:
                                print("‚úÖ Initialized adaptive_learner with historical trade data")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Failed to initialize adaptive_learner: {e}")
                
                # If still not initialized, use feature-based logic instead of default 0.5
                if not self.online_model_initialized:
                    # Look at current market features for a more intelligent default
                    try:
                        rsi = features[self.feature_columns.index('rsi')] if 'rsi' in self.feature_columns else 50
                        if rsi < 30:  # Oversold
                            return 0.65  # Slightly bullish bias
                        elif rsi > 70:  # Overbought
                            return 0.35  # Slightly bearish bias
                        return 0.5  # Neutral otherwise
                    except:
                        return 0.5  # Neutral if features can't be parsed
            
            # Get raw probability
            prob = self.online_model.predict_proba(features_array)[0, 1]
            
            # Apply calibration to prevent extreme values
            # SGD classifiers can be overconfident, especially with limited data
            prob = max(0.05, min(0.95, prob))  # Prevent extreme 0.0 or 1.0 values
            
            # Additional smoothing for very extreme values
            if prob < 0.1:
                prob = 0.1 + (prob - 0.05) * 0.5  # Soften very low probabilities
            elif prob > 0.9:
                prob = 0.9 - (0.95 - prob) * 0.5  # Soften very high probabilities
            
            if self.debug_mode and (prob > 0.45 or prob < 0.15):
                print(f"  üîß Calibrated adaptive_learner: {prob:.3f} (was extreme)")
            
            return float(prob)
            
        except Exception as e:
            self.logger.error(f"Online prediction error: {e}")
            return 0.5

    def save_system_state(self):
        """Save current system state for recovery"""
        try:
            state = {
                'models': self.models,
                'feature_scaler': self.feature_scaler,
                'feature_columns': self.feature_columns,
                'online_model': self.online_model,
                'online_model_initialized': self.online_model_initialized,
                'confidence_threshold': self.confidence_threshold,
                'performance_metrics': self.performance_metrics,
                'model_metadata': self.model_metadata,
                'analysis_log': self.analysis_log[-100:],  # Keep last 100
                'timestamp': datetime.now()
            }
            
            # Add adaptive risk management data if available
            if hasattr(self, 'symbol_performance_history'):
                state['symbol_performance_history'] = self.symbol_performance_history
            if hasattr(self, 'asset_risk_multipliers'):
                state['asset_risk_multipliers'] = self.asset_risk_multipliers
            if hasattr(self, 'adaptive_risk_config'):
                state['adaptive_risk_config'] = self.adaptive_risk_config
            
            # Add ULTRA_FORCED pattern learning data if available
            if hasattr(self, 'ultra_forced_patterns'):
                state['ultra_forced_patterns'] = self.ultra_forced_patterns
            if hasattr(self, 'dynamic_rsi_thresholds'):
                state['dynamic_rsi_thresholds'] = self.dynamic_rsi_thresholds
            
            filename = f"system_state_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(state, filename)
            
            # Also save as latest
            joblib.dump(state, 'system_state_latest.pkl')
            
            print(f"üíæ System state saved: {filename}")
            
        except Exception as e:
            self.logger.error(f"State saving error: {e}")

    def load_system_state(self, state_file='system_state_latest.pkl'):
        """Load system state for recovery"""
        try:
            if not Path(state_file).exists():
                print(f"‚ö†Ô∏è State file {state_file} not found")
                return False
            
            state = joblib.load(state_file)
            
            with self.model_lock:
                self.models = state.get('models', {})
                self.feature_scaler = state.get('feature_scaler', StandardScaler())
                self.feature_columns = state.get('feature_columns', [])
                self.online_model = state.get('online_model', None)
                self.online_model_initialized = state.get('online_model_initialized', False)
                self.confidence_threshold = state.get('confidence_threshold', 0.35)
                self.performance_metrics = state.get('performance_metrics', {})
                self.model_metadata = state.get('model_metadata', {})
                self.analysis_log = state.get('analysis_log', [])
            
            # Restore adaptive risk management data if available
            if 'symbol_performance_history' in state:
                self.symbol_performance_history = state['symbol_performance_history']
            if 'asset_risk_multipliers' in state:
                self.asset_risk_multipliers = state['asset_risk_multipliers']
            if 'adaptive_risk_config' in state:
                self.adaptive_risk_config = state['adaptive_risk_config']
            
            # Restore ULTRA_FORCED pattern learning data if available
            if 'ultra_forced_patterns' in state:
                self.ultra_forced_patterns = state['ultra_forced_patterns']
            if 'dynamic_rsi_thresholds' in state:
                self.dynamic_rsi_thresholds = state['dynamic_rsi_thresholds']
            
            print(f"üì¶ System state loaded from {state_file}")
            return True
            
        except Exception as e:
            print(f"‚ùå State loading error: {e}")
            return False

    def emergency_shutdown(self):
        """Emergency shutdown procedure"""
        try:
            print("\nüö® EMERGENCY SHUTDOWN INITIATED")
            
            # Stop learning
            self.learning_active = False
            
            # Close all positions (if configured to do so)
            if self.config.get('emergency', {}).get('close_all_positions', False):
                self.close_all_positions()
            
            # Save current state
            self.save_system_state()
            
            # Save enhanced learning state
            self.save_enhanced_learning_state()
            
            # Close database connection
            if hasattr(self, 'conn'):
                self.conn.close()
            
            # Shutdown MT5
            if mt5:
                mt5.shutdown()
            
            print("‚úÖ Emergency shutdown completed")
            
        except Exception as e:
            print(f"‚ùå Emergency shutdown error: {e}")

    def _close_trade(self, position_ticket, reason="Manual close"):
        """Close a specific trade by position ticket"""
        try:
            # Get position info
            position = None
            positions = mt5.positions_get()
            for pos in positions:
                if pos.ticket == position_ticket:
                    position = pos
                    break
            
            if not position:
                print(f"‚ùå Position {position_ticket} not found")
                return False
            
            symbol = position.symbol
            volume = position.volume
            position_type = position.type
            
            # Reverse order type for closing
            if position_type == mt5.POSITION_TYPE_BUY:
                order_type = mt5.ORDER_TYPE_SELL
                price = mt5.symbol_info_tick(symbol).bid
            else:
                order_type = mt5.ORDER_TYPE_BUY
                price = mt5.symbol_info_tick(symbol).ask
            
            request = {
                "action": mt5.TRADE_ACTION_DEAL,
                "symbol": symbol,
                "volume": volume,
                "type": order_type,
                "position": position_ticket,
                "price": price,
                "deviation": 20,
                "magic": 234567,
                "comment": reason,
                "type_time": mt5.ORDER_TIME_GTC,
                "type_filling": mt5.ORDER_FILLING_IOC
            }
            
            result = mt5.order_send(request)
            
            # Check if result is None (MT5 request failed)
            if result is None:
                error = mt5.last_error()
                print(f"‚ùå MT5 order_send returned None for position {position_ticket}: {error}")
                self.logger.error(f"MT5 order_send failed: {error}")
                return False
            
            # Check if result has retcode attribute and if closure was successful
            if hasattr(result, 'retcode') and result.retcode == mt5.TRADE_RETCODE_DONE:
                print(f"‚úÖ Closed position {position_ticket}: {symbol} ({reason})")
                # Remove from active trades
                if position_ticket in self.active_trades:
                    del self.active_trades[position_ticket]
                return True
            else:
                # Handle different error cases
                if hasattr(result, 'retcode') and hasattr(result, 'comment'):
                    print(f"‚ùå Failed to close position {position_ticket}: Code {result.retcode} - {result.comment}")
                elif hasattr(result, 'retcode'):
                    print(f"‚ùå Failed to close position {position_ticket}: Code {result.retcode}")
                else:
                    print(f"‚ùå Failed to close position {position_ticket}: Invalid result format")
                return False
                
        except Exception as e:
            print(f"‚ùå Close trade error: {e}")
            self.logger.error(f"Close trade error: {e}")
            return False

    def close_all_positions(self):
        """Close all open positions"""
        try:
            positions = mt5.positions_get()
            if positions:
                for position in positions:
                    # Close position logic here
                    symbol = position.symbol
                    volume = position.volume
                    position_type = position.type
                    
                    # Reverse order type
                    if position_type == mt5.POSITION_TYPE_BUY:
                        order_type = mt5.ORDER_TYPE_SELL
                        price = mt5.symbol_info_tick(symbol).bid
                    else:
                        order_type = mt5.ORDER_TYPE_BUY
                        price = mt5.symbol_info_tick(symbol).ask
                    
                    request = {
                        "action": mt5.TRADE_ACTION_DEAL,
                        "symbol": symbol,
                        "volume": volume,
                        "type": order_type,
                        "position": position.ticket,
                        "price": price,
                        "deviation": 20,
                        "magic": 234567,
                        "comment": "Emergency close",
                        "type_time": mt5.ORDER_TIME_GTC,
                        "type_filling": mt5.ORDER_FILLING_IOC
                    }
                    
                    result = mt5.order_send(request)
                    if result.retcode == mt5.TRADE_RETCODE_DONE:
                        print(f"‚úÖ Closed position: {symbol}")
                    else:
                        print(f"‚ùå Failed to close position: {symbol}")
                        
        except Exception as e:
            print(f"‚ùå Position closing error: {e}")

# ====== MAIN EXECUTION FUNCTIONS ======

def run_enhanced_training_pipeline(system):
    """Run enhanced training pipeline"""
    try:
        print("üöÄ Starting Enhanced Training Pipeline...")
        
        # Load data
        if not system.load_mega_dataset():
            print("‚ùå Failed to load dataset")
            return False
        
        # Engineer features
        X, feature_columns, y = system.engineer_leak_free_features()
        
        # Save for production use
        system.feature_columns = feature_columns
        
        print("‚úÖ Enhanced training pipeline completed")
        return True
        
    except Exception as e:
        print(f"‚ùå Training pipeline failed: {e}")
        return False

def main():
    """Main with optimizations"""
    print("üöÄ ULTRA_FORCED OPTIMIZED SYSTEM")
    
    try:
        # Create system
        system = EnhancedAdaptiveMLTradingSystem()
        
        # Apply optimizations immediately
        system.apply_quick_optimizations()
        
        # Load models
        if not system.load_production_models():
            print("‚ö†Ô∏è No models found, but continuing with optimized filters")
        
        # Start system
        success = system.start_enhanced_system()
        
        if success:
            print("‚úÖ ULTRA_FORCED system active!")
            
            # Keep running until interrupted
            try:
                while True:
                    time.sleep(60)  # Check every minute
                    
                    # Print status every 10 minutes
                    if datetime.now().minute % 10 == 0:
                        system.print_system_status()
                        
            except KeyboardInterrupt:
                print("\nüõë System stopped by user")
        else:
            print("‚ùå System failed to start")
    
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    finally:
        # Cleanup
        try:
            if 'system' in locals():
                system.emergency_shutdown()
        except:
            pass

# Calibration method for timeframe predictions
def _calibrate_timeframe_predictions(self, prediction, timeframe):
    """Calibrate timeframe_ensemble predictions based on known bias patterns
    
    Different timeframes often have different prediction characteristics:
    - Some timeframes produce overconfident predictions
    - Some have systematic bias toward bullish/bearish predictions
    - Some need rescaling to match their real-world performance
    
    This calibration improves the reliability of timeframe_ensemble predictions
    """
    # Skip calibration if no timeframe is provided
    if not timeframe:
        return prediction
        
    # Initialize calibration parameters if not present
    if not hasattr(self, 'timeframe_calibration'):
        # Default calibration parameters based on backtest analysis
        # Format: [center_shift, scaling_factor]
        self.timeframe_calibration = {
            'M5':  [-0.05, 1.2],  # M5 tends to be slightly bearish biased, needs amplification
            'M15': [0.00, 1.1],   # M15 is balanced but slightly underconfident
            'M30': [0.02, 0.9],   # M30 tends to be slightly bullish biased, needs dampening
            'H1':  [0.03, 0.8]    # H1 tends to be more bullish biased, needs stronger dampening
        }
    
    # Apply calibration if timeframe parameters exist
    if timeframe in self.timeframe_calibration:
        center_shift, scaling_factor = self.timeframe_calibration[timeframe]
        
        # First adjust the prediction by centering bias (0.5 is neutral)
        centered_prediction = prediction - center_shift
        
        # Then rescale while keeping within [0,1] bounds
        # Apply scaling around the neutral point (0.5)
        if centered_prediction > 0.5:
            # Scale confidence above 0.5
            calibrated = 0.5 + (centered_prediction - 0.5) * scaling_factor
        else:
            # Scale confidence below 0.5
            calibrated = 0.5 - (0.5 - centered_prediction) * scaling_factor
            
        # Ensure prediction stays within [0,1] bounds
        calibrated = max(0.0, min(1.0, calibrated))
        
        if self.debug_mode:
            print(f"üîß Calibrated {timeframe} prediction: {prediction:.3f} ‚Üí {calibrated:.3f}")
            
        return calibrated
    
    # Return original prediction if no calibration for this timeframe
    return prediction

# Add the calibration method to the EnhancedAdaptiveMLTradingSystem class
EnhancedAdaptiveMLTradingSystem._calibrate_timeframe_predictions = _calibrate_timeframe_predictions

if __name__ == "__main__":
    main()
